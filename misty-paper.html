<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Responsive Robotics to Increase Trust in Autonomous Human–Robot Interaction</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="misty-paper_files/libs/clipboard/clipboard.min.js"></script>
<script src="misty-paper_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="misty-paper_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="misty-paper_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="misty-paper_files/libs/quarto-html/popper.min.js"></script>
<script src="misty-paper_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="misty-paper_files/libs/quarto-html/anchor.min.js"></script>
<link href="misty-paper_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="misty-paper_files/libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="misty-paper_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="misty-paper_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="misty-paper_files/libs/bootstrap/bootstrap-9e3ffae467580fdb927a41352e75a2e0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods"><span class="header-section-number">2</span> Methods</a>
  <ul>
  <li><a href="#interaction-policies" id="toc-interaction-policies" class="nav-link" data-scroll-target="#interaction-policies"><span class="header-section-number">2.1</span> Interaction Policies</a></li>
  <li><a href="#collaborative-task-design" id="toc-collaborative-task-design" class="nav-link" data-scroll-target="#collaborative-task-design"><span class="header-section-number">2.2</span> Collaborative Task Design</a></li>
  <li><a href="#task-1-robot-dependent-collaborative-reasoning" id="toc-task-1-robot-dependent-collaborative-reasoning" class="nav-link" data-scroll-target="#task-1-robot-dependent-collaborative-reasoning">Task 1: Robot-Dependent Collaborative Reasoning</a></li>
  <li><a href="#task-2-open-ended-collaborative-problem-solving" id="toc-task-2-open-ended-collaborative-problem-solving" class="nav-link" data-scroll-target="#task-2-open-ended-collaborative-problem-solving">Task 2: Open-Ended Collaborative Problem Solving</a></li>
  <li><a href="#study-protocol" id="toc-study-protocol" class="nav-link" data-scroll-target="#study-protocol"><span class="header-section-number">2.3</span> Study Protocol</a></li>
  <li><a href="#measures" id="toc-measures" class="nav-link" data-scroll-target="#measures"><span class="header-section-number">2.4</span> Measures</a>
  <ul class="collapse">
  <li><a href="#self-report-measures" id="toc-self-report-measures" class="nav-link" data-scroll-target="#self-report-measures"><span class="header-section-number">2.4.1</span> Self-Report Measures</a></li>
  <li><a href="#objective-measures" id="toc-objective-measures" class="nav-link" data-scroll-target="#objective-measures"><span class="header-section-number">2.4.2</span> Objective Measures</a></li>
  </ul></li>
  <li><a href="#participants" id="toc-participants" class="nav-link" data-scroll-target="#participants"><span class="header-section-number">2.5</span> Participants</a>
  <ul class="collapse">
  <li><a href="#randomization-check" id="toc-randomization-check" class="nav-link" data-scroll-target="#randomization-check"><span class="header-section-number">2.5.1</span> Randomization Check</a></li>
  </ul></li>
  <li><a href="#analytic-strategy" id="toc-analytic-strategy" class="nav-link" data-scroll-target="#analytic-strategy"><span class="header-section-number">2.6</span> Analytic Strategy</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">3</span> Results</a>
  <ul>
  <li><a href="#primary-analysis-eligible-sample" id="toc-primary-analysis-eligible-sample" class="nav-link" data-scroll-target="#primary-analysis-eligible-sample"><span class="header-section-number">3.1</span> Primary Analysis: Eligible Sample</a></li>
  <li><a href="#hierarchical-models" id="toc-hierarchical-models" class="nav-link" data-scroll-target="#hierarchical-models"><span class="header-section-number">3.2</span> Hierarchical Models</a>
  <ul class="collapse">
  <li><a href="#trust-in-industrial-humanrobot-collaboration" id="toc-trust-in-industrial-humanrobot-collaboration" class="nav-link" data-scroll-target="#trust-in-industrial-humanrobot-collaboration"><span class="header-section-number">3.2.1</span> Trust in Industrial Human–Robot Collaboration</a></li>
  <li><a href="#trust-perception-scalehri" id="toc-trust-perception-scalehri" class="nav-link" data-scroll-target="#trust-perception-scalehri">Trust Perception Scale–HRI</a></li>
  </ul></li>
  <li><a href="#bayesian-analysis" id="toc-bayesian-analysis" class="nav-link" data-scroll-target="#bayesian-analysis"><span class="header-section-number">3.3</span> Bayesian analysis</a>
  <ul class="collapse">
  <li><a href="#primary-analyses-eligible-sample" id="toc-primary-analyses-eligible-sample" class="nav-link" data-scroll-target="#primary-analyses-eligible-sample"><span class="header-section-number">3.3.1</span> Primary Analyses: Eligible Sample</a></li>
  <li><a href="#experienced-trust-ti-hrc" id="toc-experienced-trust-ti-hrc" class="nav-link" data-scroll-target="#experienced-trust-ti-hrc">Experienced Trust (TI-HRC)</a></li>
  </ul></li>
  <li><a href="#sensitivity-analyses-full-sample" id="toc-sensitivity-analyses-full-sample" class="nav-link" data-scroll-target="#sensitivity-analyses-full-sample"><span class="header-section-number">3.4</span> Sensitivity Analyses: Full Sample</a>
  <ul class="collapse">
  <li><a href="#task-oriented-trust-tps-hri-1" id="toc-task-oriented-trust-tps-hri-1" class="nav-link" data-scroll-target="#task-oriented-trust-tps-hri-1">Task-Oriented Trust (TPS-HRI)</a></li>
  <li><a href="#experienced-trust-ti-hrc-1" id="toc-experienced-trust-ti-hrc-1" class="nav-link" data-scroll-target="#experienced-trust-ti-hrc-1">Experienced Trust (TI-HRC)</a></li>
  </ul></li>
  <li><a href="#mechanism-analyses-communication-breakdown" id="toc-mechanism-analyses-communication-breakdown" class="nav-link" data-scroll-target="#mechanism-analyses-communication-breakdown"><span class="header-section-number">3.5</span> Mechanism Analyses: Communication Breakdown</a>
  <ul class="collapse">
  <li><a href="#task-oriented-trust-tps-hri-2" id="toc-task-oriented-trust-tps-hri-2" class="nav-link" data-scroll-target="#task-oriented-trust-tps-hri-2">Task-Oriented Trust (TPS-HRI)</a></li>
  <li><a href="#experienced-trust-ti-hrc-2" id="toc-experienced-trust-ti-hrc-2" class="nav-link" data-scroll-target="#experienced-trust-ti-hrc-2">Experienced Trust (TI-HRC)</a></li>
  </ul></li>
  <li><a href="#task-performance" id="toc-task-performance" class="nav-link" data-scroll-target="#task-performance"><span class="header-section-number">3.6</span> Task performance</a></li>
  <li><a href="#individual-differences-and-correlational-patterns" id="toc-individual-differences-and-correlational-patterns" class="nav-link" data-scroll-target="#individual-differences-and-correlational-patterns"><span class="header-section-number">3.7</span> Individual differences and correlational patterns</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="header-section-number">4</span> Discussion</a></li>
  <li><a href="#technical-challenges" id="toc-technical-challenges" class="nav-link" data-scroll-target="#technical-challenges"><span class="header-section-number">5</span> Technical challenges</a></li>
  <li><a href="#conclusion-and-future-work" id="toc-conclusion-and-future-work" class="nav-link" data-scroll-target="#conclusion-and-future-work"><span class="header-section-number">6</span> Conclusion and Future Work</a></li>
  
  
  
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="misty-paper.pdf"><i class="bi bi-file-pdf"></i>Typst (ieee)</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Responsive Robotics to Increase Trust in Autonomous Human–Robot Interaction</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">M.C. Lau <a href="mailto:mclau@laurentian.ca" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Shauna Heron <a href="mailto:sheron@laurentian.ca" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>This study implements a multi-stage collaborative task system where participants collaborate with the Misty-II social robot to solve a who-dunnit type task. The system utilizes an autonomous, mixed-initiative dialogue architecture with affect-responsive capabilities.</p>
  </div>
</div>


</header>


<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>As automation expands across safety-critical domains such as manufacturing, mining, and healthcare, robotic systems are increasingly expected to operate alongside humans rather than in isolation <span class="citation" data-cites="fu2021 ciuffreda2025 diab2025 spitale2023"><a href="#ref-fu2021" role="doc-biblioref">[1]</a>, <a href="#ref-ciuffreda2025" role="doc-biblioref">[2]</a>, <a href="#ref-diab2025" role="doc-biblioref">[3]</a>, <a href="#ref-spitale2023" role="doc-biblioref">[4]</a></span>. In these collaborative settings, successful deployment depends not only on technical performance and safety guarantees, but on whether human users are willing to rely on, communicate with, and coordinate their actions around systems driven by artificial intelligence (AI) <span class="citation" data-cites="campagna2025 emaminejad2022"><a href="#ref-campagna2025" role="doc-biblioref">[5]</a>, <a href="#ref-emaminejad2022" role="doc-biblioref">[6]</a></span>. Trust has therefore emerged as a central determinant of adoption and effective use in human–robot collaboration (HRC) <span class="citation" data-cites="wischnewski2023 campagna2025"><a href="#ref-campagna2025" role="doc-biblioref">[5]</a>, <a href="#ref-wischnewski2023" role="doc-biblioref">[7]</a></span>. Insufficient trust can lead to disuse or rejection of automation, while excessive trust risks overreliance—particularly in environments characterized by uncertainty or incomplete information <span class="citation" data-cites="devisser2020"><a href="#ref-devisser2020" role="doc-biblioref">[8]</a></span>.</p>
<p>A substantial body of human–robot interaction (HRI) research has examined how robot behaviour shapes user trust, perceived reliability, and cooperation across industrial and social contexts <span class="citation" data-cites="shayganfar2019 fartook2025"><a href="#ref-shayganfar2019" role="doc-biblioref">[9]</a>, <a href="#ref-fartook2025" role="doc-biblioref">[10]</a></span>. Trust is commonly conceptualized as a multidimensional construct encompassing cognitive evaluations of competence and reliability, affective responses to the interaction partner, and behavioural willingness to rely on the system under conditions of risk or uncertainty <span class="citation" data-cites="muir1994 hancock2011 devisser2020"><a href="#ref-devisser2020" role="doc-biblioref">[8]</a>, <a href="#ref-muir1994" role="doc-biblioref">[11]</a>, <a href="#ref-hancock2011" role="doc-biblioref">[12]</a></span>. Despite this multidimensional framing, empirical studies have predominantly operationalized trust using post-interaction self-report questionnaires, often collected following short, highly controlled interactions.</p>
<p>Importantly, much of the existing HRI trust literature relies on scripted behaviours, simulated environments, or Wizard-of-Oz paradigms in which a human operator covertly manages the robot’s behaviour. While these approaches are valuable for isolating specific design factors, they obscure the interaction breakdowns and system imperfections that characterize real-world autonomous robots <span class="citation" data-cites="campagna2025"><a href="#ref-campagna2025" role="doc-biblioref">[5]</a></span>. In deployed systems, limitations such as speech recognition errors, delayed responses, misinterpretations of user intent, and incomplete affect sensing are not peripheral issues but defining features of interaction. These failures are likely to play a decisive role in shaping trust and collaboration, yet remain underrepresented in empirical evaluations.</p>
<p>One proposed mechanism for supporting trust in HRI is responsiveness: the extent to which a robot adapts its behaviour based on user state and interaction context <span class="citation" data-cites="shayganfar2019 fartook2025"><a href="#ref-shayganfar2019" role="doc-biblioref">[9]</a>, <a href="#ref-fartook2025" role="doc-biblioref">[10]</a></span>. Responsive robots may adjust dialogue, timing, or support strategies in response to inferred cues such as confusion, frustration, or disengagement, and prior work suggests that such adaptive behaviour can enhance perceived social intelligence and trustworthiness in dialogue-driven tasks <span class="citation" data-cites="birnbaum2016"><a href="#ref-birnbaum2016" role="doc-biblioref">[13]</a></span>. However, most evidence for these effects comes from simulated or semi-autonomous systems, leaving open questions about how responsiveness operates when implemented in fully autonomous, in-person interactions subject to real-time constraints and failure <span class="citation" data-cites="campagna2025"><a href="#ref-campagna2025" role="doc-biblioref">[5]</a></span>.</p>
<p>From an engineering perspective, responsiveness represents an interaction policy rather than a superficial social cue <span class="citation" data-cites="shayganfar2019"><a href="#ref-shayganfar2019" role="doc-biblioref">[9]</a></span>. Proactive assistance based on interaction context differs fundamentally from reactive, request-based behaviour, particularly in fully autonomous systems—for example, offering clarification or encouragement when confusion or hesitation is inferred, rather than waiting for an explicit request for help <span class="citation" data-cites="birnbaum2016"><a href="#ref-birnbaum2016" role="doc-biblioref">[13]</a></span>. Implementing such policies requires robots to manage spoken-language dialogue, track interaction state over time, and coordinate verbal and nonverbal responses in real time, all while operating under noise, latency, and sensing uncertainty <span class="citation" data-cites="campagna2025"><a href="#ref-campagna2025" role="doc-biblioref">[5]</a></span>.</p>
<p>The present work addresses these gaps through a pilot study examining trust and collaboration during in-person interaction with a fully autonomous social robot. Participants collaborated with one of two versions of the same robot platform during a dialogue-driven puzzle task requiring shared problem solving. In both conditions, all interaction management—including speech recognition, dialogue state tracking, task progression, and response generation—was handled and logged autonomously by the robot without human intervention. In the responsive condition, the robot employed a proactive interaction policy, adapting its assistance based on conversational cues and inferred user affect. In the neutral condition, the robot followed a reactive policy, providing general guidance but assistance only when explicitly requested.</p>
<p>This pilot study had three primary objectives: (1) to design and evaluate the feasibility of an autonomous spoken-language interaction system with affect-responsive behaviour on a mobile robot platform; (2) to assess whether interaction policy influences post-interaction trust and collaborative experience under realistic autonomous conditions; and (3) to explore how behavioural and interaction-level indicators align with subjective trust evaluations. Rather than optimizing for flawless interaction, the system was intentionally designed to reflect the capabilities and limitations of contemporary social robots, allowing interaction breakdowns to surface naturally.</p>
<p>By combining post-interaction trust measures with task-level and behavioural observations, this study aims to contribute empirical evidence on how trust in human–robot collaboration emerges and is enacted during fully autonomous interaction. The findings are intended to inform the design of a larger subsequent study by evaluating feasibility and identifying technical, interactional, and methodological challenges that must be addressed when evaluating affect-responsive robots in real-world contexts.</p>
</section>
<section id="methods" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="methods"><span class="header-section-number">2</span> Methods</h2>
<p>This study employed a between-subjects experimental design to examine how robot interaction policy influences trust and collaboration during fully autonomous, in-person human–robot interaction. The sole experimental factor was the robot’s interaction policy, with participants randomly assigned to interact with either a responsive or neutral version of the same robot system.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Throughout this paper, references to “the robot” denote the fully autonomous interactive system comprising the Misty-II hardware platform and its onboard software stack, with all interaction decisions generated without human intervention, including spoken-language processing, dialogue management, and the interaction policy governing verbal and nonverbal behaviour. Additional details of the system architecture are provided in Appendix A.</p>
</div>
</div>
<section id="interaction-policies" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="interaction-policies"><span class="header-section-number">2.1</span> Interaction Policies</h3>
<p>In the responsive policy condition, the robot employed a proactive, affect-adaptive interaction policy. Robot responses were modulated based on inferred participant affect, dialogue context, and task demands, resulting in unsolicited encouragement, clarification, and engagement-oriented behaviours when appropriate. For example, if the participant exhibited signs of confusion or hesitation (e.g., long pauses, requests for repetition or detected irritation), the robot would proactively offer hints or rephrase instructions. Similarly, if the participant demonstrated engagement (e.g., rapid responses, affirmative feedback), the robot would reciprocate with positive reinforcement and increased task involvement.</p>
<p>In the neutral or control policy condition, the robot employed a neutral, reactive interaction policy. General information and guidance were were provided to move the participant through the tasks, but additional help was only provided when explicitly requested by the participant and without affect-based adaptation or proactive support beyond a check-in when participant was silent for more than 1 minute. For example, if the participant appeared confused but did not request assistance, the robot would not intervene. The robot’s verbal and nonverbal behaviours were designed to be neutral and non-engaging, avoiding unsolicited encouragement or affective responses.</p>
<p>Both conditions used identical hardware, software infrastructure, sensing capabilities, and task logic.</p>
</section>
<section id="collaborative-task-design" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="collaborative-task-design"><span class="header-section-number">2.2</span> Collaborative Task Design</h3>
<p>The task structure was designed to elicit collaboration under two distinct dependency conditions: (1) enforced collaboration, where the robot was required to complete the task, and (2) optional collaboration, where participants could choose whether to engage the robot. To this end, participants completed an immersive, narrative-driven puzzle game consisting of five sequential stages and two timed reasoning tasks. The game context positioned participants as investigators searching for a missing robot colleague, with the robot serving as a diegetic guide and collaborative partner. The overall interaction lasted approximately 25 minutes.</p>
<p>The interactions with the Misty-II social robot took place in a shared physical workspace that included a participant-facing computer interface <span class="citation" data-cites="mistyrobotics"><a href="#ref-mistyrobotics" role="doc-biblioref">[14]</a></span>. The interface was used to display task materials, collect participant inputs, and manage task progression (see <a href="#fig-task1" class="quarto-xref">Figure&nbsp;2</a>). Importantly, the interface did <em>not</em> function as a control mechanism for the robot. Instead, the robot could autonomously monitor task progression and participant inputs via the interface and managed dialogue and behaviour accordingly.</p>
<div id="fig-setup" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-setup-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/misty-pullback.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="482">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-setup-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Experimental setup showing the autonomous robot and participant-facing task interface used during in-person sessions. Participants entered task responses and navigated between task stages using the interface, while the robot autonomously tracked task state and adapted its interaction based on participant input.
</figcaption>
</figure>
</div>
<p><strong>Stage Overview</strong></p>
<ol type="1">
<li><strong>Greeting:</strong> The robot introduced itself and engaged in brief rapport-building dialogue.<br>
</li>
<li><strong>Mission Brief:</strong> The robot explained the narrative context and overall objectives.<br>
</li>
<li><strong>Task 1:</strong> Robot-dependent collaborative reasoning task.<br>
</li>
<li><strong>Task 2:</strong> Open-ended problem solving with optional robot support.<br>
</li>
<li><strong>Wrap-up:</strong> The robot provided closing feedback and concluded the interaction.</li>
</ol>
<p>Participants advanced between stages using the interface, either at the robot’s prompting or at their own discretion. All spoken dialogue and interaction events were handled by the robot and logged automatically.</p>
</section>
<section id="task-1-robot-dependent-collaborative-reasoning" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="task-1-robot-dependent-collaborative-reasoning">Task 1: Robot-Dependent Collaborative Reasoning</h3>
<p>In the first task, participants were asked to identify a perpetrator from a 6 × 4 grid of 24 ‘suspects’ by asking the robot a series of yes/no questions about the suspect’s features (e.g., “was the suspect wearing a hat?”). The grid was displayed on the interface, while questions were posed verbally.</p>
<div id="fig-task1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-task1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/task1-whodunnit2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="482">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-task1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <em>Task 1 interface including the 6 × 4 grid of 24 candidates. Participants could track those eliminated by clicking on subjects which would grey them out. A box was provided to input their final answer and a button included to move to the next task.</em>
</figcaption>
</figure>
</div>
<p>The robot possessed ground-truth information necessary to answer each question correctly. Successful task completion was therefore dependent on interaction with the robot, creating a forced collaborative dynamic. Participants were required to coordinate questioning strategies with the robot to narrow down the suspect within a five-minute time limit. The structured nature of the task ensured consistent interaction demands across participants and conditions.</p>
</section>
<section id="task-2-open-ended-collaborative-problem-solving" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="task-2-open-ended-collaborative-problem-solving">Task 2: Open-Ended Collaborative Problem Solving</h3>
<p>The second task involved a more open-ended reasoning challenge. Participants were presented with multiple technical logs through a simulated terminal interface that could be used to infer the location of the missing robot.</p>
<div id="fig-task2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-task2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/task2-cryptic-puzzle.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="482">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-task2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: The task 2 interface presented multiple technical logs through a simulated terminal interface that could be used to determine the location of the missing robot.
</figcaption>
</figure>
</div>
<p>Unlike Task 1, the robot did not have access to ground-truth information or the contents of the logs. The robot’s assistance was limited to general reasoning support derived from its language model, such as explaining how to interpret log formats, suggesting problem-solving strategies, or prompting participants to reflect on inconsistencies.</p>
<p>Participants could complete this task independently or solicit assistance from the robot at their discretion <span class="citation" data-cites="lin2022"><a href="#ref-lin2022" role="doc-biblioref">[15]</a></span>. This design allowed collaboration to emerge voluntarily rather than being enforced by task structure, positioning the robot as a collaborative partner rather than an authoritative source.</p>
</section>
<section id="study-protocol" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="study-protocol"><span class="header-section-number">2.3</span> Study Protocol</h3>
<p>Participants signed up for the study and completed a pre-session questionnaire before their in-person session via Qualtrics. The pre-session questionnaire colleced basic demographics information and assessed baseline characteristics, including the Negative Attitudes Toward Robots Scale (NARS) and the short form of the Need for Cognition scale (NFC-s). These measures were used to capture individual differences that may moderate responses to robot interaction.</p>
<p>In-person sessions were conducted in a quiet, private room at Laurentian University between November and December 2025. Prior to each session, the robot’s interaction policy was configured to the assigned experimental condition.</p>
<p>Upon arrival, participants were greeted by the researcher, provided with a brief overview of the session, and given instructions for effective communication with the robot, including waiting for a visual indicator before speaking. Once participants indicated readiness, the researcher exited the room, leaving the participant and robot to complete the interaction without human presence or observation. Participants initiated the interaction by clicking a start button on the interface and were informed that they could terminate the session at any time without penalty.</p>
<p>Following task completion, participants completed a 21-item post-interaction questionnaire assessing trust. Participants then engaged in a brief debrief with the researcher and were awarded a $15 gift card. Total session duration averaged approximately 30 minutes.</p>
</section>
<section id="measures" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="measures"><span class="header-section-number">2.4</span> Measures</h3>
<p>A combination of self-report and objective measures was used to assess trust, engagement, and task performance.</p>
<section id="self-report-measures" class="level4" data-number="2.4.1">
<h4 data-number="2.4.1" class="anchored" data-anchor-id="self-report-measures"><span class="header-section-number">2.4.1</span> Self-Report Measures</h4>
<p>Participants completed a pre-session questionnaire assessing baseline characteristics, including the Negative Attitudes Toward Robots Scale (NARS) and the short form of the Need for Cognition scale (NFC-s). These measures were used to capture individual differences that may moderate responses to robot interaction.</p>
<p>Trust was assessed using two established self-report instruments commonly used in human–robot interaction research: the Trust Perception Scale–HRI (TPS-HRI) and the Trust in Industrial Human–Robot Collaboration scale (TI-HRC) <span class="citation" data-cites="bartneck2009 charalambous2016"><a href="#ref-bartneck2009" role="doc-biblioref">[16]</a>, <a href="#ref-charalambous2016" role="doc-biblioref">[17]</a></span>. Both measures were adapted to reflect the specific task context and interaction modality of the present study. 9 items were retained from the TI-HRC and 12 items from the TPS-HRI. Item wording was modified to reference the robot’s behaviour during a dialogue-driven collaborative task, and response formats were adjusted to ensure interpretability for participants without prior robotics experience (see Appendix B for a full item list).</p>
<p>Together, these instruments capture complementary dimensions of trust, including perceived reliability, task competence, and affective comfort. However, they differ in their conceptual emphasis: the TPS-HRI primarily operationalizes trust as a reflective judgement of system performance (i.e., “What percent of the time was the robot reliable”), whereas the TI-HRC scale emphasizes trust as an experienced, embodied response arising during interaction (i.e., “The way the robot moved made me feel uneasy”). Despite this complementarity, both measures rely on retrospective self-report and may be insensitive to moment-to-moment trust dynamics as collaboration unfolds. For this reason, questionnaire data were interpreted alongside behavioural and interaction-level measures.</p>
</section>
<section id="objective-measures" class="level4" data-number="2.4.2">
<h4 data-number="2.4.2" class="anchored" data-anchor-id="objective-measures"><span class="header-section-number">2.4.2</span> Objective Measures</h4>
<p>Objective task metrics included task completion, task accuracy, time to completion, and the number of assistance requests made to the robot. behavioural engagement metrics were derived from interaction logs and manually coded dialogue transcripts, including number of dialogue turns, frequency of communication breakdowns, response timing, and task-relevant robot contributions.</p>
</section>
</section>
<section id="participants" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="participants"><span class="header-section-number">2.5</span> Participants</h3>
<p>A total of 29 participants were recruited from the Laurentian University community via word of mouth and the SONA recruitment system. Eligibility criteria required being 18 years or older, fluent in spoken and written English, and having normal or corrected-to-normal hearing and vision. Participants received a $15 gift card as compensation for their time; some students additionally received partial credit for participating depending on their program of study. All procedures were approved by the Laurentian University Research Ethics Board (REB #6021966).</p>
<p>Although English fluency was an eligibility requirement, in-person observation during data collection indicated meaningful variability in participants’ functional spoken-language proficiency. The researcher therefore recorded observed English proficiency for each session in anticipation of potential speech-based system limitations. Subsequent post-hoc review of interaction transcripts and system logs revealed that a subset of sessions exhibited severe and sustained communication failure. In these cases, automatic speech recognition (ASR) output was largely unintelligible or fragmented, preventing the robot from extracting sufficient linguistic content to maintain dialogue, respond meaningfully to participant queries, or support task progression. Interaction frequently stalled, participant input went unanswered or was misinterpreted, and collaborative problem-solving was not feasible. These sessions reflected a breakdown of language-mediated interaction, rendering the experimental manipulation inoperative.</p>
<p>Because the study relied fundamentally on spoken-language collaboration, sessions exhibiting persistent communication failure were classified as protocol non-adherence and excluded from task-level analyses (n = 5). Exclusion decisions were based solely on communication viability and interaction mechanics, not on task outcomes or trust measures.</p>
<section id="randomization-check" class="level4" data-number="2.5.1">
<h4 data-number="2.5.1" class="anchored" data-anchor-id="randomization-check"><span class="header-section-number">2.5.1</span> Randomization Check</h4>
<p>Across analyses, participants in the responsive and control conditions were comparable with respect to demographic characteristics, prior experience with robots, and baseline attitudes toward robots, including Negative Attitudes Toward Robots (NARS) and Need for Cognition scores (see <a href="#tbl-pre" class="quarto-xref">Table&nbsp;1</a>) <span class="citation" data-cites="cacioppo1982"><a href="#ref-cacioppo1982" role="doc-biblioref">[18]</a></span>. These patterns were consistent across both eligible and full samples, indicating successful random assignment.</p>
<div class="cell">
<div id="tbl-pre" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-pre-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Participant Demographics and Baseline Characteristics by Group
</figcaption>
<div aria-describedby="tbl-pre-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="nwpweiuguq" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#nwpweiuguq table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#nwpweiuguq thead, #nwpweiuguq tbody, #nwpweiuguq tfoot, #nwpweiuguq tr, #nwpweiuguq td, #nwpweiuguq th {
  border-style: none;
}

#nwpweiuguq p {
  margin: 0;
  padding: 0;
}

#nwpweiuguq .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 13px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#nwpweiuguq .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#nwpweiuguq .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#nwpweiuguq .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#nwpweiuguq .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#nwpweiuguq .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nwpweiuguq .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#nwpweiuguq .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#nwpweiuguq .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#nwpweiuguq .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#nwpweiuguq .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#nwpweiuguq .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#nwpweiuguq .gt_spanner_row {
  border-bottom-style: hidden;
}

#nwpweiuguq .gt_group_heading {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#nwpweiuguq .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#nwpweiuguq .gt_from_md > :first-child {
  margin-top: 0;
}

#nwpweiuguq .gt_from_md > :last-child {
  margin-bottom: 0;
}

#nwpweiuguq .gt_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#nwpweiuguq .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#nwpweiuguq .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#nwpweiuguq .gt_row_group_first td {
  border-top-width: 2px;
}

#nwpweiuguq .gt_row_group_first th {
  border-top-width: 2px;
}

#nwpweiuguq .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#nwpweiuguq .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#nwpweiuguq .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#nwpweiuguq .gt_last_summary_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nwpweiuguq .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#nwpweiuguq .gt_first_grand_summary_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#nwpweiuguq .gt_last_grand_summary_row_top {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#nwpweiuguq .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#nwpweiuguq .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nwpweiuguq .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#nwpweiuguq .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#nwpweiuguq .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#nwpweiuguq .gt_sourcenote {
  font-size: 90%;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#nwpweiuguq .gt_left {
  text-align: left;
}

#nwpweiuguq .gt_center {
  text-align: center;
}

#nwpweiuguq .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#nwpweiuguq .gt_font_normal {
  font-weight: normal;
}

#nwpweiuguq .gt_font_bold {
  font-weight: bold;
}

#nwpweiuguq .gt_font_italic {
  font-style: italic;
}

#nwpweiuguq .gt_super {
  font-size: 65%;
}

#nwpweiuguq .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#nwpweiuguq .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#nwpweiuguq .gt_indent_1 {
  text-indent: 5px;
}

#nwpweiuguq .gt_indent_2 {
  text-indent: 10px;
}

#nwpweiuguq .gt_indent_3 {
  text-indent: 15px;
}

#nwpweiuguq .gt_indent_4 {
  text-indent: 20px;
}

#nwpweiuguq .gt_indent_5 {
  text-indent: 25px;
}

#nwpweiuguq .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#nwpweiuguq div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>

<table class="gt_table do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-bootstrap="false">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="gt_col_headings header">
<th id="label" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col"><strong>Characteristic</strong></th>
<th id="n" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>N</strong></th>
<th id="stat_1" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>CONTROL</strong><br>
N = 13<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span></th>
<th id="stat_2" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>RESPONSIVE</strong><br>
N = 16<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span></th>
<th id="p.value" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>p-value</strong><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>2</sup></span></th>
</tr>
</thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_left" headers="label" style="font-weight: bold">Gender</td>
<td class="gt_row gt_center" headers="n">27</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value">0.84</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Woman</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">6 / 13 (46%)</td>
<td class="gt_row gt_center" headers="stat_2">7 / 14 (50%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Man</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">7 / 13 (54%)</td>
<td class="gt_row gt_center" headers="stat_2">7 / 14 (50%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label" style="font-weight: bold">Age Group</td>
<td class="gt_row gt_center" headers="n">27</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value">0.35</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;18-24</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">5 / 13 (38%)</td>
<td class="gt_row gt_center" headers="stat_2">7 / 14 (50%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;25-34</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">4 / 13 (31%)</td>
<td class="gt_row gt_center" headers="stat_2">2 / 14 (14%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;34-44</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">1 / 13 (7.7%)</td>
<td class="gt_row gt_center" headers="stat_2">4 / 14 (29%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;45+</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">3 / 13 (23%)</td>
<td class="gt_row gt_center" headers="stat_2">1 / 14 (7.1%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label" style="font-weight: bold">Program</td>
<td class="gt_row gt_center" headers="n">25</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value">&gt;0.99</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Psychology</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">1 / 13 (7.7%)</td>
<td class="gt_row gt_center" headers="stat_2">1 / 12 (8.3%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Engineering</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">2 / 13 (15%)</td>
<td class="gt_row gt_center" headers="stat_2">1 / 12 (8.3%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Computer Science</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">7 / 13 (54%)</td>
<td class="gt_row gt_center" headers="stat_2">6 / 12 (50%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Earth Sciences</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">0 / 13 (0%)</td>
<td class="gt_row gt_center" headers="stat_2">1 / 12 (8.3%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Other</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">3 / 13 (23%)</td>
<td class="gt_row gt_center" headers="stat_2">3 / 12 (25%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label" style="font-weight: bold">Experience w/Robots</td>
<td class="gt_row gt_center" headers="n">29</td>
<td class="gt_row gt_center" headers="stat_1">7 / 13 (54%)</td>
<td class="gt_row gt_center" headers="stat_2">4 / 16 (25%)</td>
<td class="gt_row gt_center" headers="p.value">0.14</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label" style="font-weight: bold">Native English Speaker</td>
<td class="gt_row gt_center" headers="n">29</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value">0.53</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Native English</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">5 / 13 (38%)</td>
<td class="gt_row gt_center" headers="stat_2">8 / 16 (50%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Non-Native English</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">8 / 13 (62%)</td>
<td class="gt_row gt_center" headers="stat_2">8 / 16 (50%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label" style="font-weight: bold">NARS Overall</td>
<td class="gt_row gt_center" headers="n">29</td>
<td class="gt_row gt_center" headers="stat_1">38 (8)</td>
<td class="gt_row gt_center" headers="stat_2">38 (7)</td>
<td class="gt_row gt_center" headers="p.value">0.79</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label" style="font-weight: bold">Need for Cognition</td>
<td class="gt_row gt_center" headers="n">29</td>
<td class="gt_row gt_center" headers="stat_1">3.62 (0.78)</td>
<td class="gt_row gt_center" headers="stat_2">3.74 (0.74)</td>
<td class="gt_row gt_center" headers="p.value">0.55</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label" style="font-weight: bold">Dialogue Viability</td>
<td class="gt_row gt_center" headers="n">29</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value">0.63</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;exclude</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">3 / 13 (23%)</td>
<td class="gt_row gt_center" headers="stat_2">2 / 16 (13%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;include</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">10 / 13 (77%)</td>
<td class="gt_row gt_center" headers="stat_2">14 / 16 (88%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
</tbody><tfoot>
<tr class="gt_footnotes odd">
<td colspan="5" class="gt_footnote"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span> n / N (%); Mean (SD)</td>
</tr>
<tr class="gt_footnotes even">
<td colspan="5" class="gt_footnote"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>2</sup></span> Pearson’s Chi-squared test; Fisher’s exact test; Wilcoxon rank sum test</td>
</tr>
</tfoot>

</table>

</div>
</div>
</div>
</figure>
</div>
</div>
</section>
</section>
<section id="analytic-strategy" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="analytic-strategy"><span class="header-section-number">2.6</span> Analytic Strategy</h3>
<p>To ensure transparency and assess the impact of communication-based exclusions, analyses were conducted in three stages. First, an eligible-sample analysis (excluding non-viable sessions) served as the primary analysis, reflecting interactions in which the spoken-language protocol and experimental manipulation operated as intended. Second, a full-sample analysis including all participants was conducted as a sensitivity analysis to evaluate robustness to communication failures and protocol deviations. Third, a mechanism-focused analysis compared included and excluded sessions on interaction-process metrics (e.g., ASR failure rates, dialogue turn completion, task abandonment) to characterize how severe communication breakdown alters interaction dynamics.</p>
<p>While full-sample analyses are informative as robustness checks, trust measures obtained from sessions with complete communication breakdown were not interpreted as valid estimates of human–robot trust under functional interaction. In these cases, the robot was unable to sustain dialogue or collaborative behaviour, precluding meaningful evaluation of reliability, competence, or collaborative intent.</p>
<p>All analyses was conducted using R (version 4.5.1) within the Quarto framework. Data manipulation and visualization utilized the tidyverse suite of packages <span class="citation" data-cites="wickham2019"><a href="#ref-wickham2019" role="doc-biblioref"><strong>wickham2019?</strong></a></span>, with mixed-effects models fitted using the lme4 and lmerTest packages <span class="citation" data-cites="bates2015 kuznetsova2017"><a href="#ref-bates2015" role="doc-biblioref"><strong>bates2015?</strong></a>, <a href="#ref-kuznetsova2017" role="doc-biblioref"><strong>kuznetsova2017?</strong></a></span> while bayesian hierarchical models fitted using the brms package. Summary tables were generated using the gtsummary package <span class="citation" data-cites="sjoberg2021"><a href="#ref-sjoberg2021" role="doc-biblioref"><strong>sjoberg2021?</strong></a></span>. All code used for data processing and analysis is available at: <a href="">GitHub Repository</a></p>
</section>
</section>
<section id="results" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="results"><span class="header-section-number">3</span> Results</h2>
<p>Prior to hypothesis testing, interaction sessions were classified based on communication viability using a dialogue-level metric derived from manual coding of system logs. Specifically, the proportion of dialogue turns affected by speech-recognition failure or fragmented utterances was computed for each session. Sessions in which more than 60% of dialogue turns-considering half of all turns were dependent on human speech-were characterized by communication breakdown and were classified as non-viable (<span class="math inline">\(n=5\)</span>). This criterion closely matched sessions independently flagged during administration and reflects cases in which sustained spoken-language interaction was not possible. Of the 29 completed sessions, 5 were classified as non-viable due to severe and persistent communication failure resulting in unintelligble sentence fragments.</p>
<p>Because the experimental manipulation relied on language-mediated collaboration, analyses were conducted using three complementary approaches: (1) a primary eligible-sample analysis excluding non-viable sessions, (2) a full-sample sensitivity analysis including all sessions, and (3) a mechanism-focused analysis examining how communication breakdown altered interaction dynamics.</p>
<section id="primary-analysis-eligible-sample" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="primary-analysis-eligible-sample"><span class="header-section-number">3.1</span> Primary Analysis: Eligible Sample</h3>
<p>Descriptive comparisons of post-interaction trust measures indicated higher trust ratings in the responsive condition relative to the control condition across both trust scales (see <a href="#tbl-post-eligible" class="quarto-xref">Table&nbsp;2</a> for more detail). As indicated in <a href="#fig-post-eligible2" class="quarto-xref">Figure&nbsp;4</a>, average post-interaction scores on the TI-HRC differed by approximately 26 points (Likert 1-5 converted to 0-100 scale for easier comparison across scales). While differences in TPS-HRI scores were approximately 15 points higher in the responsive condition compared to the control as indicated in <a href="#fig-post-eligible" class="quarto-xref">Figure&nbsp;5</a>. Scores on the Behavioural summaries further indicated differences in dialogue patterns and robot assistance behaviours consistent with the intended interaction policies. TO DO: ADD DIALOGUE ANALYSES STATS</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-post-eligible2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post-eligible2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="misty-paper_files/figure-html/fig-post-eligible2-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post-eligible2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Distribution of Trust in Industrial Human–Robot Collaboration scores by interaction policy. Points represent individual observations; violins depict score distributions. Red points indicate group means with 95% confidence intervals. Statistical comparisons are reported in the Results section.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Importantly objective task accuracy did not differ between conditions across any task-level measures. This suggests that observed differences in trust were not driven by differential task success.</p>
<p>Despite similar task accuracy, interactions in the responsive condition were expectedly characterized by longer durations (more dialogue), slower robot response times (more dialogue), and a higher number of AI-detected engaged responses. These findings suggest that responsiveness altered the interaction dynamics and affective tone rather than task outcomes.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-post-eligible" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post-eligible-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="misty-paper_files/figure-html/fig-post-eligible-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post-eligible-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Distribution of Trust Perception in HRI by interaction policy. Points represent individual observations; violins depict score distributions. Red points indicate group means with 95% confidence intervals. Statistical comparisons are reported in the Results section.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div id="tbl-post-eligible" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-post-eligible-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Post-Interaction Raw Outcome Measures by Group
</figcaption>
<div aria-describedby="tbl-post-eligible-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="rfjneapeyz" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#rfjneapeyz table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#rfjneapeyz thead, #rfjneapeyz tbody, #rfjneapeyz tfoot, #rfjneapeyz tr, #rfjneapeyz td, #rfjneapeyz th {
  border-style: none;
}

#rfjneapeyz p {
  margin: 0;
  padding: 0;
}

#rfjneapeyz .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 13px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#rfjneapeyz .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#rfjneapeyz .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#rfjneapeyz .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#rfjneapeyz .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#rfjneapeyz .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#rfjneapeyz .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#rfjneapeyz .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#rfjneapeyz .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#rfjneapeyz .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#rfjneapeyz .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#rfjneapeyz .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#rfjneapeyz .gt_spanner_row {
  border-bottom-style: hidden;
}

#rfjneapeyz .gt_group_heading {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#rfjneapeyz .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#rfjneapeyz .gt_from_md > :first-child {
  margin-top: 0;
}

#rfjneapeyz .gt_from_md > :last-child {
  margin-bottom: 0;
}

#rfjneapeyz .gt_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#rfjneapeyz .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#rfjneapeyz .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#rfjneapeyz .gt_row_group_first td {
  border-top-width: 2px;
}

#rfjneapeyz .gt_row_group_first th {
  border-top-width: 2px;
}

#rfjneapeyz .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#rfjneapeyz .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#rfjneapeyz .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#rfjneapeyz .gt_last_summary_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#rfjneapeyz .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#rfjneapeyz .gt_first_grand_summary_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#rfjneapeyz .gt_last_grand_summary_row_top {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#rfjneapeyz .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#rfjneapeyz .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#rfjneapeyz .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#rfjneapeyz .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#rfjneapeyz .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#rfjneapeyz .gt_sourcenote {
  font-size: 90%;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#rfjneapeyz .gt_left {
  text-align: left;
}

#rfjneapeyz .gt_center {
  text-align: center;
}

#rfjneapeyz .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#rfjneapeyz .gt_font_normal {
  font-weight: normal;
}

#rfjneapeyz .gt_font_bold {
  font-weight: bold;
}

#rfjneapeyz .gt_font_italic {
  font-style: italic;
}

#rfjneapeyz .gt_super {
  font-size: 65%;
}

#rfjneapeyz .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#rfjneapeyz .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#rfjneapeyz .gt_indent_1 {
  text-indent: 5px;
}

#rfjneapeyz .gt_indent_2 {
  text-indent: 10px;
}

#rfjneapeyz .gt_indent_3 {
  text-indent: 15px;
}

#rfjneapeyz .gt_indent_4 {
  text-indent: 20px;
}

#rfjneapeyz .gt_indent_5 {
  text-indent: 25px;
}

#rfjneapeyz .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#rfjneapeyz div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>

<table class="gt_table do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-bootstrap="false">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="gt_col_headings header">
<th id="label" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col"><strong>Characteristic</strong></th>
<th id="stat_1" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>CONTROL</strong><br>
N = 10<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span></th>
<th id="stat_2" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>RESPONSIVE</strong><br>
N = 14<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span></th>
<th id="p.value" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>p-value</strong><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>2</sup></span></th>
</tr>
</thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_left" headers="label">Trust in Industrial HRI Collaboration</td>
<td class="gt_row gt_center" headers="stat_1">39 (22)</td>
<td class="gt_row gt_center" headers="stat_2">67 (21)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.004</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">Subscales</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Reliability subscale</td>
<td class="gt_row gt_center" headers="stat_1">40 (24)</td>
<td class="gt_row gt_center" headers="stat_2">65 (18)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.012</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Trust Perception subscale</td>
<td class="gt_row gt_center" headers="stat_1">42 (23)</td>
<td class="gt_row gt_center" headers="stat_2">60 (22)</td>
<td class="gt_row gt_center" headers="p.value">0.075</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Affective Trust subscale</td>
<td class="gt_row gt_center" headers="stat_1">50 (31)</td>
<td class="gt_row gt_center" headers="stat_2">79 (22)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.018</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">Trust Perception Scale–HRI</td>
<td class="gt_row gt_center" headers="stat_1">59 (17)</td>
<td class="gt_row gt_center" headers="stat_2">77 (18)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.022</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">Overall Task Accuracy</td>
<td class="gt_row gt_center" headers="stat_1">0.60 (0.21)</td>
<td class="gt_row gt_center" headers="stat_2">0.66 (0.23)</td>
<td class="gt_row gt_center" headers="p.value">0.47</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">Objective Measures</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Dialogue Turns</td>
<td class="gt_row gt_center" headers="stat_1">34 (9)</td>
<td class="gt_row gt_center" headers="stat_2">33 (5)</td>
<td class="gt_row gt_center" headers="p.value">0.45</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Avg Session Duration (mins)</td>
<td class="gt_row gt_center" headers="stat_1">13.24 (3.06)</td>
<td class="gt_row gt_center" headers="stat_2">15.26 (2.12)</td>
<td class="gt_row gt_center" headers="p.value">0.084</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Avg Robot Response Time (ms)</td>
<td class="gt_row gt_center" headers="stat_1">14.37 (3.76)</td>
<td class="gt_row gt_center" headers="stat_2">17.24 (2.52)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">&lt;0.001</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Silent Periods</td>
<td class="gt_row gt_center" headers="stat_1">5.60 (1.96)</td>
<td class="gt_row gt_center" headers="stat_2">4.71 (2.05)</td>
<td class="gt_row gt_center" headers="p.value">0.29</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Engaged Responses</td>
<td class="gt_row gt_center" headers="stat_1">2.00 (2.21)</td>
<td class="gt_row gt_center" headers="stat_2">3.50 (1.95)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.040</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Frustrated Responses</td>
<td class="gt_row gt_center" headers="stat_1">0.60 (0.70)</td>
<td class="gt_row gt_center" headers="stat_2">0.93 (1.21)</td>
<td class="gt_row gt_center" headers="p.value">0.68</td>
</tr>
</tbody><tfoot>
<tr class="gt_footnotes odd">
<td colspan="4" class="gt_footnote"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span> Mean (SD)</td>
</tr>
<tr class="gt_footnotes even">
<td colspan="4" class="gt_footnote"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>2</sup></span> Wilcoxon rank sum test; Wilcoxon rank sum exact test</td>
</tr>
</tfoot>

</table>

</div>
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="hierarchical-models" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="hierarchical-models"><span class="header-section-number">3.2</span> Hierarchical Models</h3>
<p>To evaluate interaction policy effects and to control for pre-test covariates on post-interaction trust, linear and bayesian mixed-effects models were fitted separately for both trust outcomes. All models included interaction policy (RESPONSIVE vs.&nbsp;CONTROL) as the primary fixed effect, along with baseline negative attitudes toward robots (NARS) and native English fluency as baseline covariates unless otherwise noted. Random intercepts for session and items were included in all models to account for repeated measurement at the participant level. Model building proceeded by comparing a baseline model containing interaction policy alone against models incorporating theoretically motivated covariates. Adding NARS scores significantly improved model fit (χ² = 4.82, p = .028), whereas prior experience with robots did not. While Native English fluency did not significantly improve model fit it was retained as a covariate due to its relevance for spoken-language interaction viability with the ASR system. The final model structure: robot_trust_post ~ policy + nars_pre_c + native_english + (1 | session_id) + (1 | trust_items)</p>
<section id="trust-in-industrial-humanrobot-collaboration" class="level4" data-number="3.2.1">
<h4 data-number="3.2.1" class="anchored" data-anchor-id="trust-in-industrial-humanrobot-collaboration"><span class="header-section-number">3.2.1</span> Trust in Industrial Human–Robot Collaboration</h4>
<p>For this outcome, inclusion of random intercepts for individual trust items significantly improved model fit, indicating meaningful item-level variability beyond session-level differences.</p>
<p>In the final model predicting Trust in Industrial Human–Robot Collaboration, participants who interacted with the responsive robot reported significantly higher post-interaction trust than those in the control condition (<span class="math inline">\(β = 16.28, SE = 5.14, t = 3.17, p = .005\)</span>). Higher baseline negative attitudes toward robots were associated with lower trust scores (<span class="math inline">\(β = −7.43, SE = 2.81, p = .016\)</span>). Native English fluency was not significantly associated with trust, although the estimated effect was negative.</p>
</section>
<section id="trust-perception-scalehri" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="trust-perception-scalehri">Trust Perception Scale–HRI</h4>
<p>For the Trust Perception Scale–HRI, a comparable mixed-effects model was fitted using the same fixed effects structure. In this model, interaction with the responsive robot was associated with higher post-interaction trust scores (<span class="math inline">\(β = 14.17, SE = 6.5, t = 2.00, p = 0.046\)</span>). Effects of baseline negative attitudes toward robots and native English fluency followed a similar directional pattern but did not reliably differ from zero.</p>
<p>In contrast to the collaboration trust scale, inclusion of random intercepts for individual trust items did not improve model fit for the Trust Perception Scale–HRI and was therefore omitted. This divergence likely reflects differences in scale format and response interface: the Trust Perception scale was administered using a continuous slider input, whereas the Trust in Industrial Human–Robot Collaboration scale employed discrete Likert-style response options.</p>
<p>Informal observation during administration and post-hoc inspection of item-level variance suggest that the slider-based interface, administered via a touchpad, may have reduced response precision relative to discrete response formats. While this likely attenuated item-level variability, the Trust Perception Scale–HRI nevertheless captured meaningful between-condition differences at the aggregate level.</p>
<p>Together, these models indicate that robot responsiveness had a consistent positive effect on post-interaction trust, with effect magnitude and measurement sensitivity varying by trust dimension and scale format.</p>
</section>
</section>
<section id="bayesian-analysis" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="bayesian-analysis"><span class="header-section-number">3.3</span> Bayesian analysis</h3>
<p>Trust outcomes were analysed using Bayesian linear mixed-effects models to account for repeated measurement across trust items and sessions. Two complementary trust measures were examined: task-oriented trust (TPS-HRI), reflecting evaluative judgments of robot reliability and competence, and experienced trust (TI-HRC), reflecting affective and experiential aspects of collaboration. All models included random intercepts for session and trust item. Convergence diagnostics indicated satisfactory model performance across all analyses (all R^≤1.01; effective sample sizes &gt; 1000).</p>
<p>Analyses are reported in three stages: (1) primary analyses conducted on the eligible sample (n=24; sessions with viable spoken-language interaction), (2) sensitivity analyses conducted on the full sample (<span class="math inline">\(n=29\)</span>; including sessions with severe communication breakdown), and (3) mechanism analyses examining communication breakdown as a moderator of interaction policy.</p>
<section id="primary-analyses-eligible-sample" class="level4" data-number="3.3.1">
<h4 data-number="3.3.1" class="anchored" data-anchor-id="primary-analyses-eligible-sample"><span class="header-section-number">3.3.1</span> Primary Analyses: Eligible Sample</h4>
<section id="task-oriented-trust-tps-hri" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="task-oriented-trust-tps-hri">Task-Oriented Trust (TPS-HRI)</h5>
<p>In the eligible sample (<span class="math inline">\(n=24\)</span>), interaction policy showed a strong and robust association with task-oriented trust. Participants who interacted with the responsive robot reported higher TPS-HRI scores than those in the control condition (posterior median <span class="math inline">\(β=12.73\)</span> credible interval <span class="math inline">\([2.93, 22.17]\)</span>). The posterior probability that this effect was positive exceeded 99%, with high probability that the effect was of moderate-to-large magnitude.</p>
<p>Baseline negative attitudes toward robots (NARS) were associated with lower task-oriented trust, although uncertainty remained moderate and the credible interval included zero. In contrast, native English fluency showed a credible negative association with TPS-HRI scores, indicating lower evaluative trust among non-native English speakers even in sessions where dialogue remained viable.</p>
<p>The model explained a substantial proportion of variance in TPS-HRI scores (conditional <span class="math inline">\(R^2=0.64\)</span>), with fixed effects accounting for approximately 16% of the variance. Random effects indicated meaningful variability across sessions and trust items.</p>
</section>
</section>
<section id="experienced-trust-ti-hrc" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="experienced-trust-ti-hrc">Experienced Trust (TI-HRC)</h4>
<p>A similar but stronger pattern was observed for experienced trust. Interaction with the responsive robot was associated with substantially higher TI-HRC scores compared to the control condition (posterior median <span class="math inline">\(β=14.86\)</span>, 95% credible interval <span class="math inline">\([7.20, 22.09]\)</span>), with near-unity posterior probability of a positive effect and a high probability of a large effect.</p>
<p>Baseline negative attitudes toward robots showed a clear and credible negative association with experienced trust. Native English fluency was also negatively associated with TI-HRC scores, although uncertainty was greater and the credible interval narrowly overlapped zero.</p>
<p>Overall model fit was moderate (conditional <span class="math inline">\(R^2=0.42\)</span>), with fixed effects explaining approximately 21% of the variance. Compared to TPS-HRI, item-level variance was smaller, suggesting greater coherence among affective trust items under functional interaction conditions.</p>
</section>
</section>
<section id="sensitivity-analyses-full-sample" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="sensitivity-analyses-full-sample"><span class="header-section-number">3.4</span> Sensitivity Analyses: Full Sample</h3>
<p>Sensitivity analyses were conducted including all sessions, regardless of communication viability (<span class="math inline">\(n=29\)</span>), to assess robustness of the primary findings.</p>
<section id="task-oriented-trust-tps-hri-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="task-oriented-trust-tps-hri-1">Task-Oriented Trust (TPS-HRI)</h4>
<p>In the full sample, the posterior estimate for interaction policy remained positive but was attenuated relative to the eligible sample (posterior median <span class="math inline">\(β=7.04\)</span>, 95% credible interval <span class="math inline">\([−1.83, 15.67]\)</span>). Although uncertainty increased and the credible interval included zero, the posterior probability of a positive effect remained high (&gt;94%).</p>
<p>Baseline negative attitudes toward robots continued to show a credible negative association with TPS-HRI scores. The effect of native English fluency was reduced and no longer credibly different from zero. Overall model fit decreased relative to the eligible sample (conditional R2=0.44), indicating increased unexplained variability when sessions with severe communication breakdown were included.</p>
</section>
<section id="experienced-trust-ti-hrc-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="experienced-trust-ti-hrc-1">Experienced Trust (TI-HRC)</h4>
<p>For experienced trust, attenuation effects were more pronounced. The posterior estimate for interaction policy decreased substantially in the full sample (posterior median <span class="math inline">\(β=7.17\)</span>, 95% credible interval <span class="math inline">\([−1.97, 16.70]\)</span>), with reduced probability of a large effect. Baseline negative attitudes toward robots remained negatively associated with trust, while the effect of native English fluency remained negative but uncertain.</p>
<p>Model fit remained moderate (conditional <span class="math inline">\(R^2=0.60\)</span>), but residual variance increased, consistent with the inclusion of interactions in which collaborative behaviour could not be sustained. These results indicate that experienced trust is particularly sensitive to interaction breakdown, and that trust ratings obtained under non-functional interaction conditions do not reflect graded variation in collaborative experience.</p>
</section>
</section>
<section id="mechanism-analyses-communication-breakdown" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="mechanism-analyses-communication-breakdown"><span class="header-section-number">3.5</span> Mechanism Analyses: Communication Breakdown</h3>
<p>To examine whether communication quality altered how interaction policy influenced trust, mechanism-focused analyses were conducted in the full sample modelling proportional communication breakdown as a moderator of interaction policy. These analyses were intended to isolate interaction-level dynamics rather than participant characteristics.</p>
<section id="task-oriented-trust-tps-hri-2" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="task-oriented-trust-tps-hri-2">Task-Oriented Trust (TPS-HRI)</h4>
<p>For TPS-HRI, proportional communication breakdown showed weak and unstable associations with trust. The posterior distribution of the interaction between interaction policy and communication breakdown was broad and centered near zero, indicating substantial uncertainty. This suggests that evaluative trust judgments were relatively insensitive to graded variation in communication quality once interaction viability was established.</p>
</section>
<section id="experienced-trust-ti-hrc-2" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="experienced-trust-ti-hrc-2">Experienced Trust (TI-HRC)</h4>
<p>In contrast, experienced trust showed a different pattern. Posterior estimates indicated a consistent negative tendency for the interaction between interaction policy and communication breakdown. While responsive behaviour was associated with higher experienced trust under low levels of breakdown, this advantage diminished as communication failures accumulated. Although uncertainty remained high, the posterior distribution indicated a meaningful probability that communication breakdown attenuated the trust benefits of responsive behaviour.</p>
<p>Across analyses, responsive interaction policies were consistently associated with higher trust, particularly when interaction functioned as intended. Task-oriented trust appeared relatively robust to communication degradation, whereas experienced trust was sensitive to interaction-level failures and the robot’s ability to sustain responsive behaviour. Sensitivity and mechanism analyses indicate that communication breakdown does not merely reduce trust uniformly, but alters how interaction policy shapes the trust experience. These findings support a distinction between trust as evaluative judgment and trust as lived experience, and highlight the importance of modelling interaction dynamics when evaluating trust in fully autonomous human–robot collaboration.</p>
<p>Notably, under conditions of severe communication breakdown, the RESPONSIVE robot continued to generate proactive assistance, encouragement, and meta-communication aimed at repairing the interaction. However, these efforts did not restore mutual understanding and, in several cases, appeared to increase participant confusion and cognitive load. In contrast, the CONTROL robot’s reactive interaction policy resulted in fewer unsolicited interventions, which—while less supportive under normal conditions—reduced interaction complexity when language-mediated collaboration was no longer viable.</p>
<div id="tbl-post-fullsample" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl quarto-uncaptioned" id="tbl-post-fullsample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3
</figcaption>
<div aria-describedby="tbl-post-fullsample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>

</div>
</div>
</figure>
</div>
<div id="tbl-post-nonviable" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl quarto-uncaptioned" id="tbl-post-nonviable-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4
</figcaption>
<div aria-describedby="tbl-post-nonviable-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>

</div>
</div>
</figure>
</div>
<p>As a result, trust ratings in non-viable sessions did not systematically track the intended responsiveness manipulation. These findings suggest that when spoken-language interaction collapses, higher-level constructs such as trust and collaboration are no longer meaningfully instantiated. Communication viability therefore represents a boundary condition for evaluating affect-adaptive interaction policies in autonomous social robots.</p>
</section>
</section>
<section id="task-performance" class="level3" data-number="3.6">
<h3 data-number="3.6" class="anchored" data-anchor-id="task-performance"><span class="header-section-number">3.6</span> Task performance</h3>
<p>Objective task accuracy did not differ between conditions across any task-level measures except suspect accuracy (robot dependendant task), indicating that increased trust was only attributable to improved task success when interaction was necessary to complete accurately.</p>
<p>ADD TABLE</p>
<p>Despite similar task accuracy, interactions in the responsive condition were characterized by longer durations, slower response times, and a higher number of AI-detected engaged responses. These findings suggest that responsiveness altered the interaction dynamics and affective tone rather than task outcomes.</p>
</section>
<section id="individual-differences-and-correlational-patterns" class="level3" data-number="3.7">
<h3 data-number="3.7" class="anchored" data-anchor-id="individual-differences-and-correlational-patterns"><span class="header-section-number">3.7</span> Individual differences and correlational patterns</h3>
<p>As expected, we found that higher Need for Cognition (NFC) scores were negatively associated with Negative Attitudes Towards Robots (NARS), indicating that individuals who enjoy effortful thinking tend to have more positive attitudes towards robots. This relationship is consistent with prior literature suggesting that cognitive engagement is associated with openness to new technologies. In terms of NARS subscales, NFC was negatively correlated with all three subscales, but significantly so only in the domain of Situations of Interaction with Robots. This suggests that individuals with higher NFC are less likely to hold negative attitudes across various dimensions of robot interaction but especially around direct interaction with robots.</p>
<p>–&gt; how to talk about post-interaction correlations w/pre-interaction measures Several behavioural and task-level measures were correlated with post-interaction trust, consistent with the interpretation that trust judgments were shaped by interaction quality; these variables were not included as covariates in primary models to avoid conditioning on potential mediators.</p>
<p>Baseline negative attitudes toward robots were negatively correlated with post-interaction trust, with the strongest associations observed for affective trust subscales. In contrast, objective task performance was selectively associated with perceived reliability. Need for cognition was negatively correlated with negative robot attitudes and interaction-level negative affect, suggesting that individual differences contributed to variability in trust responses.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-corr" class="quarto-float quarto-figure quarto-figure-center anchored" width="672">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-corr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="misty-paper_files/figure-html/fig-corr-1.png" id="fig-corr" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-corr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6
</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>TO DO:
</div>
</div>
<div class="callout-body-container callout-body">
<p>CITATIONS</p>
<ul>
<li>add subscale column to long format data</li>
<li>run an analysis of performance by robot-dependent versus robot-independent tasks</li>
<li>write up a future directions section for the planned larger study</li>
<li>talk about unexpected language issues with people signing up with difficultly speaking and understanding english which cuased problems with asr and interaction</li>
<li>run analysis of dialogue dynamics included Bertopic or some other analysis of the actual content of the conversations/interactions</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>TODO2
</div>
</div>
<div class="callout-body-container callout-body">
<p>Manually score each dialogue series.</p>
<p>For each interaction and stage:</p>
<ul>
<li>did the participant ask for help?</li>
<li>how many times?</li>
<li>did the robot give useful help?</li>
<li>did the robot give misleading or incorrect help?</li>
<li>did the robot stick to the policy?</li>
<li>how many times did the robot fail to understand the participant?</li>
</ul>
<p>For each task:</p>
<ul>
<li>is there evidence that the robot helped complete the task?</li>
<li>is there evidence that the participant solved the problem without help?</li>
</ul>
</div>
</div>
</section>
</section>
<section id="discussion" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="discussion"><span class="header-section-number">4</span> Discussion</h2>
<p>An additional objective of this pilot study was to inform the design of an autonomous affect-adaptive interaction system under real-time constraints. The initial system concept included multimodal affect inference based on facial expressions, vocal prosody, and interaction dynamics. However, early integration testing revealed substantial challenges related to latency, model orchestration, and timing sensitivity when deploying multiple perception models concurrently on an edge-supported mobile robot platform. Given the small-scale nature of the pilot and the central importance of maintaining stable, real-time dialogue, the deployed system prioritized robustness of spoken-language interaction and dialogue-based affect inference over broader multimodal sensing. Affect adaptation in this study was therefore driven primarily by speech-based affect signals and conversational context, allowing us to evaluate responsiveness within a fully autonomous interaction while preserving realistic system constraints.</p>
<p>The use of two trust instruments highlights an important distinction in how trust is operationalized in HRI research. The Trust Perception Scale–HRI emphasizes task-oriented and cognitive evaluations of system performance, whereas the Trust in Industrial Human–Robot Collaboration scale captures experiential and affective aspects of trust arising from embodied interaction. While both measures converged on perceived reliability, affective trust indicators were more strongly aligned with behavioural engagement during interaction, suggesting that subjective trust judgments alone may obscure how trust is enacted in practice. Trust as judgement versus trust as experience.</p>
<p>Mention language confounders!! The present findings also highlight an important boundary condition for trust measurement in spoken-language HRI. When language-mediated interaction collapses entirely, higher-level constructs such as trust and collaboration are no longer meaningfully defined. Under such conditions, trust does not simply decrease; rather, the interaction fails to instantiate the prerequisites necessary for trust formation. This distinction is critical for both system evaluation and experimental design, particularly as autonomous robots are deployed in linguistically diverse, real-world environments.</p>
<p>Because the study relied fundamentally on spoken-language collaboration, sessions exhibiting persistent communication failure were classified as protocol non-adherence and excluded from task-level analyses (<em>n</em> = 5). While the experimenter documented all cases where language might pose an issue (as observed when meeting each participant), exclusion decisions were based solely on actual communication viability and interaction mechanics, not on task outcomes or trust measures.</p>
<p>The second task was intentionally designed to be sufficiently challenging that completing it within the allotted time was difficult without assistance. This ensured that interaction with the robot represented a meaningful opportunity for collaboration rather than a trivial or purely optional exchange. By contrasting a robot-dependent task with an open-ended advisory task, the study examined trust formation across interaction contexts that varied in both informational asymmetry and reliance on the robot.</p>
<p>This pilot study examined trust outcomes following in-person interaction with an autonomous social robot under two interaction policies: a responsive, affect-adaptive condition and a neutral, non-responsive control condition. By leveraging a fully autonomous dialogue system integrated with speech recognition and affect detection, the study aimed to evaluate how robot responsiveness influences trust formation in realistic human–robot collaboration scenarios.</p>
<p>Descriptive comparisons of post-interaction measures indicated that participants in the responsive condition reported consistently higher trust across all trust measures, with differences ranging from approximately 8 to 16 points on a 0–100 scale, although uncertainty remained high given the small sample. Notably, the responsive condition did not differ from control in objective task accuracy, suggesting that increased trust was not driven by improved task success. Instead, responsive interactions were characterized by longer durations, slower response times, and a higher number of AI-detected engaged responses, indicating a shift in interaction dynamics rather than performance.</p>
<p>Baseline negative attitudes toward robots were most strongly associated with affective components of trust rather than perceptions of reliability, suggesting that pre-existing attitudes primarily shape emotional responses to interaction rather than judgments of system competence. Conversely, objective task performance was selectively associated with perceived reliability, indicating that participants distinguished between affective and functional aspects of trust.</p>
<p>Future work with larger samples could formally test mediation pathways linking robot responsiveness, interaction fluency, affective responses, and trust judgments, as well as moderation by baseline attitudes toward robots and need for cognition.</p>
<p>Participants in the responsive condition also exhibited higher levels of AI-detected engagement during interaction, as indexed by a greater number of responses classified as positive affect (t-test result). This suggests that responsive behaviours altered the affective tone of the interaction itself.</p>
</section>
<section id="technical-challenges" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="technical-challenges"><span class="header-section-number">5</span> Technical challenges</h2>
<p>Need to discuss that these items were on a 0-100 scale that required sliding a bar, while the other trust scale was on a 1-5 Likert that required simply clicking. The post test was administered on a laptop with a trackpad which may have caused difficulties for some participants who found it difficult to drag the slider with the trackpad. This could have introduced additional noise into the measurement of this scale, which may explain why the effects were somewhat weaker here.</p>
<ul>
<li>Need to talk about language issues with participants who had difficulty speaking and understanding English which caused problems with ASR and interaction.</li>
<li>Need to talk about issues where the AI was not able to flexibly handle when people asked a question about the suspect that was close to or another word for a ground-truth feature but not exactly the same word, causing confusion and miscommunication. E.g., “Was the suspect wearing pink?” The ground-truth feature was top: PINK, top-type: HOODIE; but the ASR and NLU did not extrapolate to understand that “wearing pink” referred to the same feature as “top: PINK”, causing confusion and miscommunication. Maybe the prompt could have included some examples of different phrasing which could improve this? To solve this issue in future work, we can expand the NLU training data to include more paraphrases and synonyms for each feature.</li>
</ul>
<p>There was also a case where someone asked ‘is the top shirt hoodie red?’ to which the AI answered YES. It may have been confused by the multiple descriptors in the question. Future work could involve improving the NLU to handle more complex queries with multiple attributes.</p>
<p>Discuss future work where we will look investigate the ‘embodied’ effect of having a physical robot versus a virtual agent on trust and collaboration in HRI.</p>
<p>Also, prompt could include examples of what to do when dialogue appears fragmented, to remind participants to wait until the blue light is on before speaking and to switch up its phrasing if the robot seems to not understand.</p>
<p>Also, the control condition seemed to be somewhat neutered in terms of flexibility in responding in different ways. it would always respond with the exact same phrase when confronted with a sentence fragment or a question it could not directly answer.</p>
<p>Also issues with people not paying attention to the robot’s visual cues to know when to speak, leading to more fragmented dialogue. Future work could involve improving participant instructions, improved latency and ‘listening’ … and the robot’s feedback mechanisms to better manage turn-taking.</p>
<p>Need to remember to flag participants who did not complete/skipped specific tasks. E.g. P56 skipped the wrapup entirely. Many skipped the brief (by advancing on their own through the dashboard).</p>
</section>
<section id="conclusion-and-future-work" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="conclusion-and-future-work"><span class="header-section-number">6</span> Conclusion and Future Work</h2>
</section>




<div id="quarto-appendix" class="default"><section id="sec-appendix-a" class="level2 appendix" data-number="7"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">7</span> Appendix A</h2><div class="quarto-appendix-contents">

<section id="system-overview" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="system-overview"><span class="header-section-number">7.1</span> System Overview</h3>
<p>The experimental system comprised a fully autonomous, multi-stage collaborative task in which participants interacted with the Misty II social robot to solve a two-part investigative scenario. Interaction was mediated through spoken dialogue and a companion web interface, allowing the robot and participant to jointly reason about task information. The system was designed as a mixed-initiative dialogue architecture with optional affect-responsive behaviour, implemented without human intervention during experimental sessions.</p>
<section id="hardware-platform" class="level4" data-number="7.1.1">
<h4 data-number="7.1.1" class="anchored" data-anchor-id="hardware-platform"><span class="header-section-number">7.1.1</span> Hardware Platform</h4>
<p>The robot platform used in this study was the Misty II social robot. Misty II is a mobile social robot equipped with an expressive display, articulated head and arms, and programmable RGB LEDs. These components were used to produce synchronized verbal and nonverbal behaviour, including eye expressions, head movements, arm gestures, and colour-based state indicators. Audio input was captured via the robot’s RTSP video stream, which provided real-time access to the microphone signal for downstream speech processing.</p>
</section>
<section id="software-architecture" class="level4" data-number="7.1.2">
<h4 data-number="7.1.2" class="anchored" data-anchor-id="software-architecture"><span class="header-section-number">7.1.2</span> Software Architecture</h4>
<p>All system components were implemented in Python (version 3.10). The software architecture integrated robot control, speech processing, dialogue management, task logic, and data logging into a single autonomous pipeline. Core dependencies included the Misty Robotics Python SDK for robot control, the Deepgram SDK for speech recognition, FFmpeg for audio stream processing, Flask and Flask-SocketIO for the web-based task interface, and DuckDB for structured data logging.</p>
</section>
<section id="dialogue-management-and-large-language-model-integration" class="level4" data-number="7.1.3">
<h4 data-number="7.1.3" class="anchored" data-anchor-id="dialogue-management-and-large-language-model-integration"><span class="header-section-number">7.1.3</span> Dialogue Management and Large Language Model Integration</h4>
<p>Dialogue was managed using the LangChain framework, which provided abstraction over message handling, memory persistence, and large language model integration. The system used Google’s Gemini API as the underlying language model, configured to produce strictly JSON-formatted outputs to ensure reliable downstream parsing and execution on the robot.</p>
<p>The deployed model was <code>gemini-2.5-flash-lite</code>, selected for its low-latency response characteristics. Generation temperature was set to 0.7 to balance coherence and variability. Conversation history was maintained using a buffer-based memory mechanism, allowing the robot to reference prior exchanges within a session while resetting memory between participants. Conversation histories were stored as session-specific JSON files to enable post-hoc analysis and recovery.</p>
</section>
<section id="prompt-structure-and-context-injection" class="level4" data-number="7.1.4">
<h4 data-number="7.1.4" class="anchored" data-anchor-id="prompt-structure-and-context-injection"><span class="header-section-number">7.1.4</span> Prompt Structure and Context Injection</h4>
<p>System prompts were constructed dynamically at each dialogue turn. Each prompt consisted of a system message defining task rules, role constraints, and output format requirements, followed by the accumulated conversation history and the current participant utterance. In addition to transcribed speech, structured contextual variables were injected into the prompt as JSON fields, including the current task stage, detected emotion labels, timer expiration flags, and task submission status. This approach allowed the language model to access environmental state without embedding control information directly into conversational text.</p>
</section>
</section>
<section id="speech-processing" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="speech-processing"><span class="header-section-number">7.2</span> Speech Processing</h3>
<p>Speech-to-text processing was handled by Deepgram’s Nova-2 model using real-time WebSocket streaming. The system employed adaptive endpointing and voice activity detection to support conversational turn-taking. Endpointing thresholds differed across task stages, with shorter timeouts during dialogue-driven stages and longer timeouts during log-reading phases.</p>
<p>Text-to-speech output was generated using Misty II’s onboard TTS engine, which produces a synthetic robotic voice. Although external TTS options (including OpenAI and Deepgram Aura voices) were implemented and tested, the onboard voice was selected to avoid introducing human-like vocal qualities that could independently influence trust perceptions.</p>
</section>
<section id="emotion-detection-and-affective-state-mapping" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="emotion-detection-and-affective-state-mapping"><span class="header-section-number">7.3</span> Emotion Detection and Affective State Mapping</h3>
<p>Participant affect was inferred from transcribed utterances using a DistilRoBERTa-based emotion classification model fine-tuned for English-language emotion detection. The model produced categorical predictions (e.g., joy, frustration, anxiety, neutral), which were mapped to higher-level interaction states such as positive engagement, irritation, or confusion. In the responsive condition, these inferred states were used to guide dialogue strategy and nonverbal behaviour selection.</p>
</section>
<section id="multimodal-behaviour-generation" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="multimodal-behaviour-generation"><span class="header-section-number">7.4</span> Multimodal Behaviour Generation</h3>
<p>The robot’s nonverbal behaviour was implemented through a library of custom action scripts combining facial expressions, LED patterns, arm movements, and head motions. At each dialogue turn, the language model selected an expression label from a predefined set, which was then translated into a coordinated multimodal action. In the responsive condition, additional backchannel behaviours were triggered during participant speech, including listening cues and emotion-matched expressions.</p>
<p>LED colours were used to signal system state to participants. A blue LED indicated active listening, while a purple LED indicated processing or speaking.</p>
</section>
<section id="collaborative-tasks" class="level3" data-number="7.5">
<h3 data-number="7.5" class="anchored" data-anchor-id="collaborative-tasks"><span class="header-section-number">7.5</span> Collaborative Tasks</h3>
<p>The interaction consisted of two collaborative tasks. In the first task, participants and the robot jointly solved a “who-dunnit” problem by eliminating suspects from a grid based on yes/no questions. The robot possessed ground-truth knowledge but was constrained to answering only feature-based yes/no queries. In the second task, participants and the robot attempted to locate a missing robot by interpreting cryptic system and sensor logs. In this task, the robot did not know the solution and instead provided guidance based on general technical knowledge and logical reasoning.</p>
<p>Task information and participant responses were presented through a web-based dashboard. The dashboard displayed suspect grids, system logs, and response input fields, and communicated task progression events back to the robot via REST API calls.</p>
</section>
<section id="data-collection-and-logging" class="level3" data-number="7.6">
<h3 data-number="7.6" class="anchored" data-anchor-id="data-collection-and-logging"><span class="header-section-number">7.6</span> Data Collection and Logging</h3>
<p>All interaction data were logged to a DuckDB relational database. Logged data included session metadata, turn-level dialogue transcripts, language model responses, nonverbal behaviour selections, response latencies, task submissions, detected emotions, and system events such as stage transitions and timer expirations. This structure enabled detailed post-hoc analysis of interaction dynamics, communication failures, and trust-related behaviours.</p>
</section>
<section id="interaction-dynamics-and-control-modes" class="level3" data-number="7.7">
<h3 data-number="7.7" class="anchored" data-anchor-id="interaction-dynamics-and-control-modes"><span class="header-section-number">7.7</span> Interaction Dynamics and Control Modes</h3>
<p>Two interaction policies were implemented and toggled programmatically at runtime: a responsive mode and a control mode. In the responsive mode, the robot proactively offered assistance, adjusted its dialogue based on inferred affect, and produced supportive backchannel behaviours. In the control mode, the robot provided information only when explicitly prompted and did not adapt its behaviour based on affective cues. The active mode was set prior to each session and remained fixed throughout the interaction.</p>
<p>Silence handling was implemented using a fixed threshold, after which the robot issued a check-in prompt. The phrasing of these prompts differed across conditions to reflect proactive versus reactive interaction strategies.</p>
</section>
<section id="inter-process-communication" class="level3" data-number="7.8">
<h3 data-number="7.8" class="anchored" data-anchor-id="inter-process-communication"><span class="header-section-number">7.8</span> Inter-process Communication</h3>
<p>System components communicated via a set of Flask-based REST endpoints. These endpoints synchronized task stage state, detected participant submissions, managed timer events, and allowed limited facilitator override when necessary. All communication between the web interface and the robot occurred locally to ensure low latency and experimental reliability.</p>
</section>
</div></section><section id="sec-appendix-b" class="level2 appendix" data-number="8"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">8</span> Appendix B</h2><div class="quarto-appendix-contents">

<section id="trust-perception-scalehri-tps-hri" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="trust-perception-scalehri-tps-hri"><span class="header-section-number">8.1</span> Trust Perception Scale–HRI (TPS-HRI)</h3>
<p>Participants rated the following items on a percentage scale (0–100%), indicating the proportion of time each statement applied to the robot during the interaction.</p>
<ul>
<li>What percent of the time was the robot dependable?</li>
<li>What percent of the time was the robot reliable?</li>
<li>What percent of the time was the robot responsive?</li>
<li>What percent of the time was the robot trustworthy?</li>
<li>What percent of the time was the robot supportive?</li>
<li>What percent of the time did this robot act consistently?</li>
<li>What percent of the time did this robot provide feedback?</li>
<li>What percent of the time did this robot meet the needs of the mission task?</li>
<li>What percent of the time did this robot provide appropriate information?</li>
<li>What percent of the time did this robot communicate appropriately?</li>
<li>What percent of the time did this robot follow directions?</li>
<li>What percent of the time did this robot answer the questions asked?</li>
</ul>
</section>
<section id="trust-in-industrial-humanrobot-collaboration-ti-hrc" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="trust-in-industrial-humanrobot-collaboration-ti-hrc"><span class="header-section-number">8.2</span> Trust in Industrial Human–Robot Collaboration (TI-HRC)</h3>
<p>Participants indicated their agreement with the following statements using a 5-point Likert-type scale. Negatively worded items were reverse-scored prior to analysis.</p>
<p><strong><em>Reliability</em></strong></p>
<ul>
<li>I trusted that the robot would give me accurate answers.</li>
<li>The robot’s responses seemed reliable.</li>
<li>I felt I could rely on the robot to do what it was supposed to do.</li>
</ul>
<p><strong><em>Perceptual / Affective Trust</em></strong></p>
<ul>
<li>The robot seemed to enjoy helping me.</li>
<li>The robot was responsive to my needs.</li>
<li>The robot seemed to care about helping me.</li>
</ul>
<p><strong><em>Discomfort / Unease</em></strong></p>
<ul>
<li>The way the robot moved made me uncomfortable. (R)</li>
<li>The way the robot spoke made me uncomfortable. (R)</li>
<li>Talking to the robot made me uneasy. (R)</li>
</ul>
</section>
</div></section><section id="sec-appendix-c" class="level2 appendix" data-number="9"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9</span> Appendix C</h2><div class="quarto-appendix-contents">

<section id="dialogue-coding-scheme" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="dialogue-coding-scheme"><span class="header-section-number">9.1</span> Dialogue Coding Scheme</h3>
<section id="task-outcome-layer-stage-level" class="level4" data-number="9.1.1">
<h4 data-number="9.1.1" class="anchored" data-anchor-id="task-outcome-layer-stage-level"><span class="header-section-number">9.1.1</span> Task Outcome Layer (Stage-Level)</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 37%">
<col style="width: 25%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>task_outcome</code></td>
<td>categorical</td>
<td>Final task status (<code>completed</code>, <code>timeout</code>, <code>skipped</code>, <code>partial</code>, <code>abandoned</code>).</td>
</tr>
<tr class="even">
<td><code>task_completed</code></td>
<td>binary</td>
<td>Task goal was fully completed.</td>
</tr>
<tr class="odd">
<td><code>task_timed_out</code></td>
<td>binary</td>
<td>Task ended due to expiration of the time limit.</td>
</tr>
<tr class="even">
<td><code>task_skipped</code></td>
<td>binary</td>
<td>Participant explicitly skipped or advanced past the stage.</td>
</tr>
<tr class="odd">
<td><code>task_partially_completed</code></td>
<td>binary</td>
<td>Task progress was made, but the full solution was not reached.</td>
</tr>
<tr class="even">
<td><code>task_abandoned</code></td>
<td>binary</td>
<td>Participant disengaged or stopped attempting the task before timeout.</td>
</tr>
<tr class="odd">
<td><code>task_completed_without_help</code></td>
<td>binary</td>
<td>Task was completed without any help requests to the robot.</td>
</tr>
<tr class="even">
<td><code>task_required_robot_help</code></td>
<td>binary</td>
<td>At least one robot help interaction was required for task completion.</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="dialogue-interaction-layer-turn-level" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="dialogue-interaction-layer-turn-level"><span class="header-section-number">9.2</span> Dialogue Interaction Layer (Turn-Level)</h3>
<section id="human-turn-codes" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="human-turn-codes">Human Turn Codes</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 37%">
<col style="width: 25%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>human_help_request</code></td>
<td>binary</td>
<td>Participant explicitly or implicitly asks the robot for help or guidance.</td>
</tr>
<tr class="even">
<td><code>human_reasoning</code></td>
<td>binary</td>
<td>Participant reasons out loud with the robot toward problem-solving.</td>
</tr>
<tr class="odd">
<td><code>human_confusion</code></td>
<td>binary</td>
<td>Participant expresses confusion or uncertainty.</td>
</tr>
<tr class="even">
<td><code>human_confirmation_seeking</code></td>
<td>binary</td>
<td>Participant seeks confirmation of a tentative belief or solution.</td>
</tr>
</tbody>
</table>
</section>
<section id="robot-turn-codes" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="robot-turn-codes">Robot Turn Codes</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 25%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>robot_helpful_guidance</code></td>
<td>binary</td>
<td>Robot provides accurate, task-relevant information or guidance.</td>
</tr>
<tr class="even">
<td><code>robot_misleading_guidance</code></td>
<td>binary</td>
<td>Robot provides misleading or incorrect guidance.</td>
</tr>
<tr class="odd">
<td><code>robot_factually_incorrect</code></td>
<td>binary</td>
<td>Robot states information that is objectively incorrect (though it may not know it is incorrect).</td>
</tr>
<tr class="even">
<td><code>robot_policy_violation</code></td>
<td>binary</td>
<td>Robot violates stated system or task constraints.</td>
</tr>
<tr class="odd">
<td><code>robot_on_policy_unhelpful</code></td>
<td>binary</td>
<td>Robot adheres to policy but provides vague or non-actionable assistance.</td>
</tr>
<tr class="even">
<td><code>robot_stt_failure</code></td>
<td>binary</td>
<td>Robot response reflects a speech-to-text or input understanding failure.</td>
</tr>
<tr class="odd">
<td><code>robot_clarification_request</code></td>
<td>binary</td>
<td>Robot asks the participant for information or to repeat or clarify their input.</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="affective-interaction-layer-turn-level" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="affective-interaction-layer-turn-level"><span class="header-section-number">9.3</span> Affective Interaction Layer (Turn-Level)</h3>
<section id="robot-affective-behaviour" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="robot-affective-behaviour">Robot Affective behaviour</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 25%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>robot_empathy_expression</code></td>
<td>binary</td>
<td>Robot expresses empathy, encouragement, or reassurance.</td>
</tr>
<tr class="even">
<td><code>robot_emotion_acknowledgement</code></td>
<td>binary</td>
<td>Robot explicitly references an inferred participant emotional state.</td>
</tr>
</tbody>
</table>
</section>
<section id="human-affective-response" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="human-affective-response">Human Affective Response</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 40%">
<col style="width: 25%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>human_affective_engagement</code></td>
<td>binary</td>
<td>Participant responds in a socially warm or engaged manner.</td>
</tr>
<tr class="even">
<td><code>human_social_reciprocity</code></td>
<td>binary</td>
<td>Participant mirrors or responds to the robot’s affective expression.</td>
</tr>
<tr class="odd">
<td><code>human_anthropomorphic_language</code></td>
<td>binary</td>
<td>Participant treats the robot as a social agent.</td>
</tr>
<tr class="even">
<td><code>human_emotional_disengagement</code></td>
<td>binary</td>
<td>Participant responds in a curt, dismissive, or withdrawn manner.</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="notes" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="notes"><span class="header-section-number">9.4</span> Notes</h3>
<ul>
<li>Turn-level variables are coded per dialogue turn.</li>
<li>Task outcome variables are coded once per <code>session_id × stage</code>.</li>
<li>Raw dialogue text was retained during coding and removed prior to aggregation.</li>
<li>Multiple turn-level codes may co-occur unless otherwise specified.</li>
</ul>

</section>
</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list">
<div id="ref-fu2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">W. Fu, Y. Xu, L. Liu, and L. Zhang, <span>“Design and Research of Intelligent Safety Monitoring Robot for Coal Mine Shaft Construction,”</span> <em>Advances in Civil Engineering</em>, vol. 2021, no. 1, p. 6897767, Jan. 2021, doi: <a href="https://doi.org/10.1155/2021/6897767">10.1155/2021/6897767</a>. Available: <a href="https://onlinelibrary.wiley.com/doi/10.1155/2021/6897767">https://onlinelibrary.wiley.com/doi/10.1155/2021/6897767</a></div>
</div>
<div id="ref-ciuffreda2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">I. Ciuffreda <em>et al.</em>, <span>“Design and Development of a Technological Platform Based on a Sensorized Social Robot for Supporting Older Adults and Caregivers: GUARDIAN Ecosystem,”</span> <em>International Journal of Social Robotics</em>, vol. 17, no. 5, pp. 803–822, May 2025, doi: <a href="https://doi.org/10.1007/s12369-023-01038-5">10.1007/s12369-023-01038-5</a>. Available: <a href="https://doi.org/10.1007/s12369-023-01038-5">https://doi.org/10.1007/s12369-023-01038-5</a></div>
</div>
<div id="ref-diab2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">M. Diab and Y. Demiris, <span>“TICK: A Knowledge Processing Infrastructure for Cognitive Trust in Human<span></span>Robot Interaction,”</span> <em>International Journal of Social Robotics</em>, pp. 1–33, Jan. 2025, doi: <a href="https://doi.org/10.1007/s12369-024-01206-1">10.1007/s12369-024-01206-1</a>. Available: <a href="https://link.springer.com/article/10.1007/s12369-024-01206-1">https://link.springer.com/article/10.1007/s12369-024-01206-1</a></div>
</div>
<div id="ref-spitale2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">M. Spitale, M. Axelsson, and H. Gunes, <span>“Robotic mental well-being coaches for the workplace: An in-the-wild study on form,”</span> in HRI ’23. New York, NY, USA: Association for Computing Machinery, Mar. 2023, p. 301310. doi: <a href="https://doi.org/10.1145/3568162.3577003">10.1145/3568162.3577003</a>. Available: <a href="https://dl.acm.org/doi/10.1145/3568162.3577003">https://dl.acm.org/doi/10.1145/3568162.3577003</a></div>
</div>
<div id="ref-campagna2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">G. Campagna and M. Rehm, <span>“A systematic review of trust assessments in human<span></span>robot interaction,”</span> <em>J. Hum.-Robot Interact.</em>, vol. 14, no. 2, p. 30:130:35, Jan. 2025, doi: <a href="https://doi.org/10.1145/3706123">10.1145/3706123</a>. Available: <a href="https://doi.org/10.1145/3706123">https://doi.org/10.1145/3706123</a></div>
</div>
<div id="ref-emaminejad2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">N. Emaminejad and R. Akhavian, <span>“Trustworthy AI and robotics: Implications for the AEC industry,”</span> <em>Automation in Construction</em>, vol. 139, p. 104298, Jul. 2022, doi: <a href="https://doi.org/10.1016/j.autcon.2022.104298">10.1016/j.autcon.2022.104298</a>. Available: <a href="https://www.sciencedirect.com/science/article/pii/S0926580522001716">https://www.sciencedirect.com/science/article/pii/S0926580522001716</a></div>
</div>
<div id="ref-wischnewski2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">M. Wischnewski, N. Krämer, and E. Müller, <span>“Measuring and understanding trust calibrations for automated systems: A survey of the state-of-the-art and future directions,”</span> in CHI ’23. New York, NY, USA: Association for Computing Machinery, Apr. 2023, p. 116. doi: <a href="https://doi.org/10.1145/3544548.3581197">10.1145/3544548.3581197</a>. Available: <a href="https://doi.org/10.1145/3544548.3581197">https://doi.org/10.1145/3544548.3581197</a></div>
</div>
<div id="ref-devisser2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">E. J. de Visser <em>et al.</em>, <span>“Towards a Theory of Longitudinal Trust Calibration in Human<span></span>Robot Teams,”</span> <em>International Journal of Social Robotics</em>, vol. 12, no. 2, pp. 459–478, May 2020, doi: <a href="https://doi.org/10.1007/s12369-019-00596-x">10.1007/s12369-019-00596-x</a>. Available: <a href="https://doi.org/10.1007/s12369-019-00596-x">https://doi.org/10.1007/s12369-019-00596-x</a></div>
</div>
<div id="ref-shayganfar2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">M. Shayganfar, C. Rich, C. Sidner, and B. Hylák, <span>“2019 IEEE international conference on humanized computing and communication (HCC),”</span> Sep. 2019, pp. 7–15. doi: <a href="https://doi.org/10.1109/HCC46620.2019.00010">10.1109/HCC46620.2019.00010</a>. Available: <a href="https://ieeexplore.ieee.org/document/8940829">https://ieeexplore.ieee.org/document/8940829</a></div>
</div>
<div id="ref-fartook2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">O. Fartook, Z. McKendrick, T. Oron-Gilad, and J. R. Cauchard, <span>“Enhancing emotional support in human-robot interaction: Implementing emotion regulation mechanisms in a personal drone,”</span> <em>Computers in Human Behavior: Artificial Humans</em>, vol. 4, p. 100146, May 2025, doi: <a href="https://doi.org/10.1016/j.chbah.2025.100146">10.1016/j.chbah.2025.100146</a>. Available: <a href="https://www.sciencedirect.com/science/article/pii/S2949882125000301">https://www.sciencedirect.com/science/article/pii/S2949882125000301</a></div>
</div>
<div id="ref-muir1994" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">B. M. MUIR, <span>“Trust in automation: Part i. Theoretical issues in the study of trust and human intervention in automated systems,”</span> <em>Ergonomics</em>, vol. 37, no. 11, pp. 1905–1922, Nov. 1994, doi: <a href="https://doi.org/10.1080/00140139408964957">10.1080/00140139408964957</a>. Available: <a href="https://doi.org/10.1080/00140139408964957">https://doi.org/10.1080/00140139408964957</a></div>
</div>
<div id="ref-hancock2011" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">P. A. Hancock, D. R. Billings, K. E. Schaefer, J. Y. C. Chen, E. J. de Visser, and R. Parasuraman, <span>“A meta-analysis of factors affecting trust in human-robot interaction,”</span> <em>Human Factors</em>, vol. 53, no. 5, pp. 517–527, Oct. 2011, doi: <a href="https://doi.org/10.1177/0018720811417254">10.1177/0018720811417254</a></div>
</div>
<div id="ref-birnbaum2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">G. E. Birnbaum, M. Mizrahi, G. Hoffman, H. T. Reis, E. J. Finkel, and O. Sass, <span>“What robots can teach us about intimacy: The reassuring effects of robot responsiveness to human disclosure,”</span> <em>Computers in Human Behavior</em>, vol. 63, pp. 416–423, Oct. 2016, doi: <a href="https://doi.org/10.1016/j.chb.2016.05.064">10.1016/j.chb.2016.05.064</a>. Available: <a href="https://www.sciencedirect.com/science/article/pii/S0747563216303910">https://www.sciencedirect.com/science/article/pii/S0747563216303910</a></div>
</div>
<div id="ref-mistyrobotics" class="csl-entry" role="listitem">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">Furhat Robotics, <span>“Misty-II robot platform.”</span> 2023. Available: <a href="https://www.mistyrobotics.com">https://www.mistyrobotics.com</a></div>
</div>
<div id="ref-lin2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">T.-H. Lin, S. Ng, and S. Sebo, <span>“2022 31st IEEE international conference on robot and human interactive communication (RO-MAN),”</span> Aug. 2022, pp. 37–44. doi: <a href="https://doi.org/10.1109/RO-MAN53752.2022.9900828">10.1109/RO-MAN53752.2022.9900828</a>. Available: <a href="https://ieeexplore.ieee.org/document/9900828">https://ieeexplore.ieee.org/document/9900828</a></div>
</div>
<div id="ref-bartneck2009" class="csl-entry" role="listitem">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">C. Bartneck, D. Kulić, E. Croft, and S. Zoghbi, <span>“Measurement Instruments for the Anthropomorphism, Animacy, Likeability, Perceived Intelligence, and Perceived Safety of Robots,”</span> <em>International Journal of Social Robotics</em>, vol. 1, no. 1, pp. 71–81, Jan. 2009, doi: <a href="https://doi.org/10.1007/s12369-008-0001-3">10.1007/s12369-008-0001-3</a>. Available: <a href="https://doi.org/10.1007/s12369-008-0001-3">https://doi.org/10.1007/s12369-008-0001-3</a></div>
</div>
<div id="ref-charalambous2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">G. Charalambous, S. Fletcher, and P. Webb, <span>“The Development of a Scale to Evaluate Trust in Industrial Human-robot Collaboration,”</span> <em>International Journal of Social Robotics</em>, vol. 8, no. 2, pp. 193–209, Apr. 2016, doi: <a href="https://doi.org/10.1007/s12369-015-0333-8">10.1007/s12369-015-0333-8</a>. Available: <a href="https://doi.org/10.1007/s12369-015-0333-8">https://doi.org/10.1007/s12369-015-0333-8</a></div>
</div>
<div id="ref-cacioppo1982" class="csl-entry" role="listitem">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">J. T. Cacioppo and R. E. Petty, <span>“The need for cognition,”</span> <em>Journal of Personality and Social Psychology</em>, vol. 42, no. 1, pp. 116–131, 1982, doi: <a href="https://doi.org/10.1037/0022-3514.42.1.116">10.1037/0022-3514.42.1.116</a></div>
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>