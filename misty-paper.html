<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="M.C. Lau">
<meta name="author" content="Shauna Heron">
<meta name="dcterms.date" content="2026-01-02">
<meta name="keywords" content="human-robot interaction, HRI, socially assistive robotics, cobots, autonomous robot systems, spoken language interaction, trust in automation, trust in human-robot interaction, affect-adaptive systems">

<title>Responsive Robotics to Increase Trust in Autonomous Human–Robot Interaction</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="misty-paper_files/libs/clipboard/clipboard.min.js"></script>
<script src="misty-paper_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="misty-paper_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="misty-paper_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="misty-paper_files/libs/quarto-html/popper.min.js"></script>
<script src="misty-paper_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="misty-paper_files/libs/quarto-html/anchor.min.js"></script>
<link href="misty-paper_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="misty-paper_files/libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="misty-paper_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="misty-paper_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="misty-paper_files/libs/bootstrap/bootstrap-9e3ffae467580fdb927a41352e75a2e0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="misty-paper.pdf"><i class="bi bi-file-pdf"></i>PDF (elsevier)</a></li></ul></div></div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Responsive Robotics to Increase Trust in Autonomous Human–Robot Interaction</h1>
<p class="subtitle lead">An In-Person Pilot Study</p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">M.C. Lau <a href="mailto:mclau@laurentian.ca" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Laurentian University
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Shauna Heron <a href="mailto:sheron@laurentian.ca" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Laurentian University
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 2, 2026</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>This study implements a multi-stage collaborative task system where participants collaborate with the Misty-II social robot to solve a who-dunnit type task. The system utilizes an autonomous, mixed-initiative dialogue architecture with affect-responsive capabilities.</p>
  </div>
</div>

<div>
  <div class="keywords">
    <div class="block-title">Keywords</div>
    <p>human-robot interaction, HRI, socially assistive robotics, cobots, autonomous robot systems, spoken language interaction, trust in automation, trust in human-robot interaction, affect-adaptive systems</p>
  </div>
</div>

</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>As automation expands across safety-critical domains such as manufacturing, healthcare, and mining, robotic systems are increasingly expected to operate alongside humans rather than in isolation <span class="citation" data-cites="fu2021 racette2024">(<a href="#ref-fu2021" role="doc-biblioref">Fu et al. 2021</a>; <a href="#ref-racette2024" role="doc-biblioref">Racette 2024</a>)</span>. In these collaborative settings, successful deployment depends not only on technical performance but on whether human users are willing to rely on, communicate with, and coordinate their actions around autonomous systems <span class="citation" data-cites="campagna2025 emaminejad2022">(<a href="#ref-campagna2025" role="doc-biblioref">Campagna and Rehm 2025</a>; <a href="#ref-emaminejad2022" role="doc-biblioref">Emaminejad and Akhavian 2022</a>)</span>. Trust has therefore emerged as a central determinant of adoption and effective use in human–robot collaboration (HRC). Insufficient trust can lead to disuse or rejection of automation, whereas excessive trust risks overreliance and automation bias, particularly in environments characterized by uncertainty or incomplete information <span class="citation" data-cites="devisser2020">(<a href="#ref-devisser2020" role="doc-biblioref">Visser et al. 2020</a>)</span>.</p>
<p>A substantial body of human–robot interaction (HRI) research has examined how robot behaviour shapes user trust, perceived reliability, and cooperation <span class="citation" data-cites="shayganfar2019 fartook2025">(<a href="#ref-shayganfar2019" role="doc-biblioref">Shayganfar et al. 2019</a>; <a href="#ref-fartook2025" role="doc-biblioref">Fartook et al. 2025</a>)</span>. Prior work has demonstrated that trust influences both subjective evaluations of robotic partners and objective outcomes such as compliance, task performance, and collaborative efficiency. However, much of this literature relies on interactions conducted under highly controlled conditions, including scripted behaviours, simulated environments, or Wizard-of-Oz paradigms in which a human operator covertly manages aspects of the robot’s behaviour. While these approaches are valuable for isolating specific design factors, they often obscure the interaction breakdowns and system imperfections that characterize real-world autonomous robots.</p>
<p>In deployed systems, limitations such as speech recognition errors, delayed responses, misinterpretations of user intent, and incomplete affect sensing are not peripheral issues but defining features of interaction. These failures are likely to play a decisive role in shaping trust and collaboration, yet remain underrepresented in empirical HRI research. Understanding how trust emerges—and sometimes deteriorates—under realistic autonomous conditions is therefore critical for the design of robots intended for real-world collaborative use.</p>
<p>One proposed mechanism for supporting trust in HRI is <strong>responsiveness</strong>: the extent to which a robot adapts its behaviour based on user state and interaction context <span class="citation" data-cites="shayganfar2019 fartook2025">(<a href="#ref-shayganfar2019" role="doc-biblioref">Shayganfar et al. 2019</a>; <a href="#ref-fartook2025" role="doc-biblioref">Fartook et al. 2025</a>)</span>. Responsive robots may adjust their dialogue, timing, or support strategies in response to inferred affective cues such as confusion, frustration, or disengagement. Prior studies suggest that such adaptive behaviour can enhance perceived social intelligence and trustworthiness, particularly in dialogue-driven tasks <span class="citation" data-cites="birnbaum2016">(<a href="#ref-birnbaum2016" role="doc-biblioref"><strong>birnbaum2016?</strong></a>)</span>. However, most evidence for these effects comes from simulated or semi-autonomous systems, leaving open questions about how responsiveness operates when implemented in fully autonomous, in-person interactions.</p>
<p>From an engineering perspective, responsiveness represents an interaction policy rather than a superficial social cue. Proactive, state-contingent assistance differs fundamentally from reactive, request-based behaviour, particularly when implemented under strict autonomy constraints. Designing and evaluating such policies requires systems capable of managing spoken-language dialogue, maintaining interaction state, and coordinating verbal and nonverbal responses in real time—while remaining robust to noise, latency, and sensing errors.</p>
<p>The present work addresses these gaps through a pilot study examining trust and collaboration during in-person interaction with a fully autonomous social robot. Participants collaborated with one of two versions of the same robot platform during a dialogue-driven puzzle task requiring shared problem solving. In both conditions, all interaction management—including speech recognition, dialogue state tracking, task progression, and response generation—was handled autonomously by the robot without human intervention. In the <strong>responsive</strong> condition, the robot employed a proactive interaction policy, adapting its assistance based on conversational cues and inferred user affect. In the <strong>neutral</strong> condition, the robot followed a reactive policy, providing assistance only when explicitly requested.</p>
<p>This study was conducted as a pilot with three primary objectives: (1) to evaluate the feasibility of deploying an autonomous spoken-language interaction system with affect-responsive behaviour on a mobile robot platform; (2) to assess whether differences in interaction policy influence trust, perceived social intelligence, and collaborative experience under realistic autonomous conditions; and (3) to examine how individual differences in baseline attitudes toward robots and cognitive engagement may moderate responses to adaptive robotic behaviour. Rather than optimizing for flawless interaction, the system was intentionally designed to reflect the capabilities and limitations of contemporary social robots, allowing interaction breakdowns to surface naturally.</p>
<p>An additional objective of this pilot study was to inform the design of an autonomous affect-adaptive interaction system under real-time constraints. The initial system concept included multimodal affect inference based on facial expressions, vocal prosody, and interaction dynamics. However, early integration testing revealed substantial challenges related to latency, model orchestration, and timing sensitivity when deploying multiple perception models concurrently on an edge-supported mobile robot platform. Given the small-scale nature of the pilot and the central importance of maintaining stable, real-time dialogue, the deployed system prioritized robustness of spoken-language interaction and dialogue-based affect inference over broader multimodal sensing. Affect adaptation in this study was therefore driven primarily by speech-based affect signals and conversational context, allowing us to evaluate responsiveness within a fully autonomous interaction while preserving realistic system constraints.</p>
<p>By combining post-interaction trust measures with task-level and behavioural observations, this pilot study aims to contribute empirical evidence on how trust in human–robot collaboration emerges in fully autonomous settings. The findings are intended to inform the design of a larger, subsequent study by identifying technical, interactional, and methodological challenges—including speech recognition limitations, language barriers, and interaction design trade-offs—that must be addressed when evaluating affect-responsive robots in real-world contexts.</p>
<p>The remainder of this paper is structured as follows. Section 2 reviews related work on spoken-language interaction, trust, and responsiveness in HRI. Section 3 describes the autonomous system architecture, experimental design, and measurement approach. Section 4 presents results from the pilot study, followed by a discussion of implications, limitations, and directions for future work.</p>
</section>
<section id="methods" class="level1">
<h1>Methods</h1>
<section id="experimental-design-and-conditions" class="level2">
<h2 class="anchored" data-anchor-id="experimental-design-and-conditions">Experimental Design and Conditions</h2>
<p>This study employed a between-subjects experimental design to examine how robot interaction policy influences trust and collaboration during fully autonomous, in-person human–robot interaction. The sole experimental factor was the robot’s interaction policy, with participants randomly assigned to interact with either a <strong>responsive</strong> or <strong>neutral</strong> version of the same robot system.</p>
<p>Participants interacted with a Misty-II social robot in a shared physical workspace that included a participant-facing computer interface <span class="citation" data-cites="mistya">(<a href="#ref-mistya" role="doc-biblioref"><strong>mistya?</strong></a>)</span>. The interface was used to display task materials, collect participant inputs, and manage task progression. Importantly, the interface did not function as a control mechanism for the robot. Instead, the robot autonomously monitored task state and participant inputs via the interface and managed dialogue and behaviour accordingly, without real-time human intervention.</p>
<p>Random assignment to condition was performed at sign-up using Qualtrics. Due to no-shows, last-minute cancellations, and technical exclusions (described below), final group sizes were <em>n</em> = 14 in the RESPONSIVE condition and <em>n</em> = 9 in the CONTROL condition.</p>
<section id="interaction-policies" class="level3">
<h3 class="anchored" data-anchor-id="interaction-policies">Interaction Policies</h3>
<ul>
<li><p><strong>RESPONSIVE condition (experimental):</strong><br>
The robot employed a proactive, affect-adaptive interaction policy. Robot responses were modulated based on inferred participant affect, dialogue context, and task demands, resulting in unsolicited encouragement, clarification, and engagement-oriented behaviours when appropriate.</p></li>
<li><p><strong>CONTROL condition (baseline):</strong><br>
The robot employed a neutral, reactive interaction policy. Assistance and information were provided only when explicitly requested by the participant, without affect-based adaptation or proactive support.</p></li>
</ul>
<p>Both conditions used identical hardware, software infrastructure, sensing capabilities, and task logic. The only difference between conditions was the robot’s interaction policy.</p>
</section>
<section id="collaborative-task-design" class="level3">
<h3 class="anchored" data-anchor-id="collaborative-task-design">Collaborative Task Design</h3>
<p>Participants completed an immersive, narrative-driven puzzle game consisting of five sequential stages and two timed reasoning tasks. The game context positioned participants as investigators searching for a missing robot colleague, with the robot serving as a diegetic guide and collaborative partner. The overall interaction lasted approximately 15 minutes.</p>
<div id="fig-setup" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-setup-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/misty-pullback.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="482">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-setup-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Experimental setup showing the autonomous robot and participant-facing task interface used during in-person sessions. Participants entered task responses and navigated between task stages using the interface, while the robot autonomously tracked task state and adapted its interaction based on participant input. No real-time human intervention occurred during the interaction.
</figcaption>
</figure>
</div>
<p>The task structure was designed to elicit collaboration under two distinct dependency conditions: (1) enforced collaboration, where the robot was required to complete the task, and (2) optional collaboration, where participants could choose whether to engage the robot.</p>
<section id="stage-overview" class="level4">
<h4 class="anchored" data-anchor-id="stage-overview">Stage Overview</h4>
<ol type="1">
<li><strong>Greeting:</strong> The robot introduced itself and engaged in brief rapport-building dialogue.<br>
</li>
<li><strong>Mission Brief:</strong> The robot explained the narrative context and overall objectives.<br>
</li>
<li><strong>Task 1:</strong> Robot-dependent collaborative reasoning task.<br>
</li>
<li><strong>Task 2:</strong> Open-ended problem solving with optional robot support.<br>
</li>
<li><strong>Wrap-up:</strong> The robot provided closing feedback and concluded the interaction.</li>
</ol>
<p>Participants advanced between stages using the interface, either at the robot’s prompting or at their own discretion. All spoken dialogue and interaction events were logged automatically.</p>
</section>
<section id="task-1-robot-dependent-collaborative-reasoning" class="level4">
<h4 class="anchored" data-anchor-id="task-1-robot-dependent-collaborative-reasoning">Task 1: Robot-Dependent Collaborative Reasoning</h4>
<p>In Task 1, participants were required to identify a suspect from a 6 × 4 grid of 24 candidates by asking the robot a series of yes/no questions about the suspect’s features (e.g., clothing, accessories). The grid was displayed on the interface, while questions were posed verbally.</p>
<div id="fig-task1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-task1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/task1-whodunnit.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="482">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-task1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: In the first task, participants were required to identify a suspect from a 6 × 4 grid of 24 candidates by asking the robot a series of yes/no questions about the suspect’s features (e.g., hair color, accessories, clothing). The grid was displayed on the interface, while questions were posed verbally to the robot. Participants could track those eliminated here and input their final answer.
</figcaption>
</figure>
</div>
<p>The robot possessed ground-truth information necessary to answer each question correctly. Successful task completion was therefore dependent on interaction with the robot, creating a forced collaborative dynamic. Participants were required to coordinate questioning strategies with the robot to narrow down the suspect within a five-minute time limit. The structured nature of the task ensured consistent interaction demands across participants and conditions.</p>
</section>
<section id="task-2-open-ended-collaborative-problem-solving" class="level4">
<h4 class="anchored" data-anchor-id="task-2-open-ended-collaborative-problem-solving">Task 2: Open-Ended Collaborative Problem Solving</h4>
<p>Task 2 involved a more open-ended reasoning challenge. Participants were presented with multiple technical logs through a simulated terminal interface that could be used to infer the location of the missing robot.</p>
<div id="fig-task2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-task2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/task2-cryptic-puzzle.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="482">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-task2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: The task 2 interface presented multiple technical logs through a simulated terminal interface that could be used to determine the location of the missing robot.
</figcaption>
</figure>
</div>
<p>Unlike Task 1, the robot did not have access to ground-truth information or the contents of the logs. The robot’s assistance was limited to general reasoning support derived from its language model, such as explaining how to interpret log formats, suggesting problem-solving strategies, or prompting participants to reflect on inconsistencies.</p>
<p>Participants could complete this task independently or solicit assistance from the robot at their discretion <span class="citation" data-cites="lin2022">(<a href="#ref-lin2022" role="doc-biblioref">Lin, Ng, and Sebo 2022</a>)</span>. This design allowed collaboration to emerge voluntarily rather than being enforced by task structure, positioning the robot as an advisory partner rather than an authoritative source.</p>
</section>
</section>
<section id="study-protocol" class="level3">
<h3 class="anchored" data-anchor-id="study-protocol">Study Protocol</h3>
<p>In-person sessions were conducted in a quiet, private room at Laurentian University between November and December. Prior to each session, the robot’s interaction policy was configured to the assigned experimental condition.</p>
<p>Upon arrival, participants were greeted by the researcher, provided with a brief overview of the session, and given instructions for effective communication with the robot, including waiting for a visual indicator before speaking. Once participants indicated readiness, the researcher exited the room, leaving the participant and robot to complete the interaction without human presence or observation. Participants initiated the interaction by clicking a start button on the interface and were informed that they could terminate the session at any time without penalty.</p>
<p>Following task completion, participants completed a post-interaction questionnaire assessing trust. Participants then engaged in a brief debrief with the researcher. Total session duration averaged approximately 30 minutes.</p>
</section>
<section id="measures" class="level3">
<h3 class="anchored" data-anchor-id="measures">Measures</h3>
<p>A combination of self-report and objective measures was used to assess trust, engagement, and task performance.</p>
<section id="self-report-measures" class="level4">
<h4 class="anchored" data-anchor-id="self-report-measures">Self-Report Measures</h4>
<p>Participants completed a pre-session questionnaire assessing baseline characteristics, including the Negative Attitudes Toward Robots Scale (NARS) and the short form of the Need for Cognition scale (NFC-s). These measures were used to capture individual differences that may moderate responses to robot interaction.</p>
<p>Post-interaction trust was assessed using two validated instruments: the Trust Perception Scale–HRI and the Trust in Industrial Human–Robot Collaboration scale <span class="citation" data-cites="bartneck2009 charalambous">(<a href="#ref-bartneck2009" role="doc-biblioref"><strong>bartneck2009?</strong></a>; <a href="#ref-charalambous" role="doc-biblioref"><strong>charalambous?</strong></a>)</span>. Together, these measures capture multiple dimensions of trust, including perceived reliability, competence, and collaborative suitability.</p>
</section>
<section id="objective-and-behavioural-measures" class="level4">
<h4 class="anchored" data-anchor-id="objective-and-behavioural-measures">Objective and behavioural Measures</h4>
<p>Objective task metrics included task completion, task accuracy, time to completion, and the number of assistance requests made to the robot. behavioural engagement metrics were derived from interaction logs and manually coded dialogue transcripts, including number of dialogue turns, frequency of communication breakdowns, response timing, and task-relevant robot contributions.</p>
</section>
</section>
<section id="participants-communication-viability-and-analytic-strategy" class="level3">
<h3 class="anchored" data-anchor-id="participants-communication-viability-and-analytic-strategy">Participants, Communication Viability, and Analytic Strategy</h3>
<p>A total of 29 participants were recruited from the Laurentian University community via word of mouth and the SONA recruitment system. Eligibility criteria included being 18 years or older, fluent in spoken and written English, and having normal or corrected-to-normal hearing and vision. Participants received a $15 gift card as compensation for their time. All procedures were approved by the Laurentian University Research Ethics Board (REB #6021966).</p>
<p>Although English fluency was an eligibility requirement, in-person observation of variability in actual english language ability was noted for each session. Later post-hoc review of interaction transcripts and system logs revealed that a subset of sessions exhibited severe and sustained communication failure. In these sessions, automatic speech recognition (ASR) output was largely unintelligible or fragmented, preventing the robot from extracting sufficient linguistic content to maintain dialogue, respond meaningfully to participant queries, or support task progression. As a result, interaction frequently stalled, participant questions went unanswered or were misinterpreted, and collaborative problem-solving was effectively impossible. These sessions did not reflect degraded interaction quality but rather a complete breakdown of language-mediated communication, rendering the experimental manipulation inoperative.</p>
<p>Because the study relied fundamentally on spoken-language collaboration, sessions exhibiting persistent communication failure were classified as <strong>protocol non-adherence</strong> and excluded from task-level analyses (<em>n</em> = 6). Exclusion decisions were based solely on communication viability and interaction mechanics, not on task outcomes or trust measures.</p>
<p>To ensure transparency and to evaluate the impact of communication-based exclusions, analyses were conducted in three stages. First, the <strong>eligible-sample analysis</strong> (excluding non-viable sessions) was treated as the primary analysis because it reflects interactions in which the spoken-language protocol—and therefore the experimental manipulation—operated as intended. Second, a <strong>full-sample analysis</strong> including all participants was conducted as a sensitivity test to evaluate robustness to communication failures and protocol deviations. Third, a <strong>mechanism-focused analysis</strong> compared excluded and included sessions on interaction-process metrics (e.g., ASR failure rates, dialogue turn completion, task abandonment) to quantify how severe communication breakdown alters the interaction dynamics and renders the manipulation inoperative.</p>
<p>It is important to note that while full-sample analyses are informative as robustness checks, trust measures obtained from sessions with complete communication breakdown are not interpreted as valid estimates of human–robot trust in the intended sense. In these cases, the robot was unable to sustain dialogue or collaborative behaviour, meaning that participants could not meaningfully evaluate reliability, competence, or collaborative intent. Full-sample analyses are therefore treated as sensitivity analyses reflecting real-world failure conditions, rather than as alternative estimates of trust under functional interaction.</p>
<p>Across analyses, participants in the responsive and control conditions were comparable with respect to demographic characteristics, prior experience with robots, and baseline attitudes toward robots, including Negative Attitudes Toward Robots (NARS) and Need for Cognition scores <span class="citation" data-cites="nomura2006 cacioppo1984 cacioppo1996">(<a href="#ref-nomura2006" role="doc-biblioref"><strong>nomura2006?</strong></a>; <a href="#ref-cacioppo1984" role="doc-biblioref"><strong>cacioppo1984?</strong></a>; <a href="#ref-cacioppo1996" role="doc-biblioref"><strong>cacioppo1996?</strong></a>)</span>.</p>
</section>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<section id="communication-viability-and-analytic-samples" class="level2">
<h2 class="anchored" data-anchor-id="communication-viability-and-analytic-samples">Communication Viability and Analytic Samples</h2>
<p>Prior to hypothesis testing, interaction sessions were classified based on communication viability using a dialogue-level metric derived from system logs and manual coding. Specifically, the proportion of dialogue turns affected by speech-recognition failure or fragmented utterances was computed for each session. Sessions in which more than 50% of dialogue turns (half of all turns were dependent on human speech) were characterized by communication breakdown were classified as non-viable (n=6). This criterion closely matched sessions independently flagged during administration and reflects cases in which sustained spoken-language interaction was not possible.</p>
<p>Of the 29 completed sessions, 6 were classified as non-viable due to severe and persistent communication failure (i.e., unintelligble sentence fragments). Because the experimental manipulation relied on language-mediated collaboration, analyses were conducted using three complementary approaches: (1) a primary eligible-sample analysis excluding non-viable sessions, (2) a full-sample sensitivity analysis including all sessions, and (3) a mechanism-focused analysis examining how communication breakdown altered interaction dynamics.</p>
<p>Unless otherwise noted, inferential results reported below refer to the eligible sample.</p>
</section>
<section id="primary-analysis-eligible-sample" class="level2">
<h2 class="anchored" data-anchor-id="primary-analysis-eligible-sample">Primary Analysis: Eligible Sample</h2>
<section id="descriptive-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="descriptive-outcomes">Descriptive Outcomes</h3>
<p>Descriptive comparisons of post-interaction trust measures indicated higher trust ratings in the RESPONSIVE condition relative to the CONTROL condition across both trust scales (see <a href="#tbl-post-eligible" class="quarto-xref">Table&nbsp;1</a>). Average post-interaction scores on the Trust in Industrial Human–Robot Collaboration scale differed by approximately 26 points (Likert 1-5 converted to 0-100 scale for easier comparison across scales). While differences in Trust Perception Scale–HRI scores were approximately 15 points higher in the responsive condition, while scores on the Behavioural summaries further indicated differences in dialogue patterns and robot assistance behaviours consistent with the intended interaction policies.</p>
<p>Importantly objective task accuracy did not differ between conditions across any task-level measures. This suggests that observed differences in trust were not driven by differential task success.</p>
<div class="cell">
<div id="tbl-post-eligible" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-post-eligible-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Table 2. Post-Interaction Raw Outcome Measures by Group
</figcaption>
<div aria-describedby="tbl-post-eligible-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="bkszaxjrps" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#bkszaxjrps table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#bkszaxjrps thead, #bkszaxjrps tbody, #bkszaxjrps tfoot, #bkszaxjrps tr, #bkszaxjrps td, #bkszaxjrps th {
  border-style: none;
}

#bkszaxjrps p {
  margin: 0;
  padding: 0;
}

#bkszaxjrps .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 13px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#bkszaxjrps .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#bkszaxjrps .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#bkszaxjrps .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#bkszaxjrps .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#bkszaxjrps .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#bkszaxjrps .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#bkszaxjrps .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#bkszaxjrps .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#bkszaxjrps .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#bkszaxjrps .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#bkszaxjrps .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#bkszaxjrps .gt_spanner_row {
  border-bottom-style: hidden;
}

#bkszaxjrps .gt_group_heading {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#bkszaxjrps .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#bkszaxjrps .gt_from_md > :first-child {
  margin-top: 0;
}

#bkszaxjrps .gt_from_md > :last-child {
  margin-bottom: 0;
}

#bkszaxjrps .gt_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#bkszaxjrps .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#bkszaxjrps .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#bkszaxjrps .gt_row_group_first td {
  border-top-width: 2px;
}

#bkszaxjrps .gt_row_group_first th {
  border-top-width: 2px;
}

#bkszaxjrps .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#bkszaxjrps .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#bkszaxjrps .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#bkszaxjrps .gt_last_summary_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#bkszaxjrps .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#bkszaxjrps .gt_first_grand_summary_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#bkszaxjrps .gt_last_grand_summary_row_top {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#bkszaxjrps .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#bkszaxjrps .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#bkszaxjrps .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#bkszaxjrps .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#bkszaxjrps .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#bkszaxjrps .gt_sourcenote {
  font-size: 90%;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#bkszaxjrps .gt_left {
  text-align: left;
}

#bkszaxjrps .gt_center {
  text-align: center;
}

#bkszaxjrps .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#bkszaxjrps .gt_font_normal {
  font-weight: normal;
}

#bkszaxjrps .gt_font_bold {
  font-weight: bold;
}

#bkszaxjrps .gt_font_italic {
  font-style: italic;
}

#bkszaxjrps .gt_super {
  font-size: 65%;
}

#bkszaxjrps .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#bkszaxjrps .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#bkszaxjrps .gt_indent_1 {
  text-indent: 5px;
}

#bkszaxjrps .gt_indent_2 {
  text-indent: 10px;
}

#bkszaxjrps .gt_indent_3 {
  text-indent: 15px;
}

#bkszaxjrps .gt_indent_4 {
  text-indent: 20px;
}

#bkszaxjrps .gt_indent_5 {
  text-indent: 25px;
}

#bkszaxjrps .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#bkszaxjrps div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>

<table class="gt_table do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-bootstrap="false">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="gt_col_headings header">
<th id="label" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col"><strong>Characteristic</strong></th>
<th id="stat_1" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>CONTROL</strong><br>
N = 9<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span></th>
<th id="stat_2" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>RESPONSIVE</strong><br>
N = 14<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span></th>
<th id="p.value" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>p-value</strong><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>2</sup></span></th>
</tr>
</thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_left" headers="label">Trust in Industrial HRI Collaboration</td>
<td class="gt_row gt_center" headers="stat_1">41 (22)</td>
<td class="gt_row gt_center" headers="stat_2">67 (21)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.007</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">Subscales</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Reliability subscale</td>
<td class="gt_row gt_center" headers="stat_1">41 (25)</td>
<td class="gt_row gt_center" headers="stat_2">65 (18)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.022</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Trust Perception subscale</td>
<td class="gt_row gt_center" headers="stat_1">46 (21)</td>
<td class="gt_row gt_center" headers="stat_2">60 (22)</td>
<td class="gt_row gt_center" headers="p.value">0.14</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Affective Trust subscale</td>
<td class="gt_row gt_center" headers="stat_1">52 (32)</td>
<td class="gt_row gt_center" headers="stat_2">79 (22)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.030</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">Trust Perception Scale–HRI</td>
<td class="gt_row gt_center" headers="stat_1">62 (15)</td>
<td class="gt_row gt_center" headers="stat_2">77 (18)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.046</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">Overall Task Accuracy</td>
<td class="gt_row gt_center" headers="stat_1">0.60 (0.22)</td>
<td class="gt_row gt_center" headers="stat_2">0.66 (0.23)</td>
<td class="gt_row gt_center" headers="p.value">0.49</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">Objective Measures</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Dialogue Turns</td>
<td class="gt_row gt_center" headers="stat_1">36 (7)</td>
<td class="gt_row gt_center" headers="stat_2">33 (5)</td>
<td class="gt_row gt_center" headers="p.value">0.21</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Avg Task Duration (mins)</td>
<td class="gt_row gt_center" headers="stat_1">13.82 (2.60)</td>
<td class="gt_row gt_center" headers="stat_2">15.26 (2.12)</td>
<td class="gt_row gt_center" headers="p.value">0.16</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Avg Response Time (ms)</td>
<td class="gt_row gt_center" headers="stat_1">13.21 (0.84)</td>
<td class="gt_row gt_center" headers="stat_2">17.24 (2.52)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">&lt;0.001</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Silent Periods</td>
<td class="gt_row gt_center" headers="stat_1">5.67 (2.06)</td>
<td class="gt_row gt_center" headers="stat_2">4.71 (2.05)</td>
<td class="gt_row gt_center" headers="p.value">0.29</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Engaged Responses</td>
<td class="gt_row gt_center" headers="stat_1">2.22 (2.22)</td>
<td class="gt_row gt_center" headers="stat_2">3.50 (1.95)</td>
<td class="gt_row gt_center" headers="p.value">0.077</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Frustrated Responses</td>
<td class="gt_row gt_center" headers="stat_1">0.56 (0.73)</td>
<td class="gt_row gt_center" headers="stat_2">0.93 (1.21)</td>
<td class="gt_row gt_center" headers="p.value">0.58</td>
</tr>
</tbody><tfoot>
<tr class="gt_footnotes odd">
<td colspan="4" class="gt_footnote"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span> Mean (SD)</td>
</tr>
<tr class="gt_footnotes even">
<td colspan="4" class="gt_footnote"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>2</sup></span> Wilcoxon rank sum test; Wilcoxon rank sum exact test</td>
</tr>
</tfoot>

</table>

</div>
</div>
</div>
</figure>
</div>
</div>
<p>Despite similar task accuracy, interactions in the responsive condition were characterized by longer durations, slower response times, and a higher number of AI-detected engaged responses. These findings suggest that responsiveness altered the interaction dynamics and affective tone rather than task outcomes.</p>
</section>
<section id="hierarchical-models-of-post-interaction-trust" class="level3">
<h3 class="anchored" data-anchor-id="hierarchical-models-of-post-interaction-trust">Hierarchical Models of Post-Interaction Trust</h3>
<p>To evaluate condition effects on post-interaction trust, linear mixed-effects models were fitted separately for each trust outcome. All models included interaction policy (RESPONSIVE vs.&nbsp;CONTROL) as the primary fixed effect, along with baseline negative attitudes toward robots (NARS) and native English fluency as covariates. Random intercepts for session were included in all models to account for repeated measurement at the participant level.</p>
<p>Model building proceeded by comparing a baseline model containing interaction policy alone against models incorporating theoretically motivated covariates. Adding baseline negative attitudes toward robots significantly improved model fit (χ² = 4.82, p = .028), whereas prior experience with robots did not. Native English fluency did not significantly improve model fit but was retained as a covariate due to its relevance for spoken-language interaction viability.</p>
</section>
<section id="trust-in-industrial-humanrobot-collaboration" class="level3">
<h3 class="anchored" data-anchor-id="trust-in-industrial-humanrobot-collaboration">Trust in Industrial Human–Robot Collaboration</h3>
<p>In the final model predicting Trust in Industrial Human–Robot Collaboration, interaction with the responsive robot was associated with significantly higher post-interaction trust scores (β = 16.28, SE = 5.14, t = 3.17, p = .005). Higher baseline negative attitudes toward robots were associated with lower trust (β = −7.43, SE = 2.81, p = .016). Native English fluency showed a negative but non-significant association with trust.</p>
<p>For this outcome, inclusion of random intercepts for individual trust items significantly improved model fit, indicating meaningful item-level variability beyond session-level differences.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-post-eligible2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post-eligible2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="misty-paper_files/figure-html/fig-post-eligible2-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post-eligible2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Distribution of Trust in Industrial Human–Robot Collaboration scores by interaction policy. Points represent individual observations; violins depict score distributions. Red points indicate group means with 95% confidence intervals. Statistical comparisons are reported in the Results section.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="trust-perception-scalehri" class="level3">
<h3 class="anchored" data-anchor-id="trust-perception-scalehri">Trust Perception Scale–HRI</h3>
<p>For the Trust Perception Scale–HRI, a comparable mixed-effects model was fitted using the same fixed effects structure. In this model, interaction with the responsive robot was associated with higher post-interaction trust scores (β = 14.17, SE = 6.5, t = 2.00, p = 0.046). Effects of baseline negative attitudes toward robots and native English fluency followed a similar directional pattern but did not reliably differ from zero.</p>
<p>In contrast to the collaboration trust scale, inclusion of random intercepts for individual trust items did not improve model fit for the Trust Perception Scale–HRI and was therefore omitted. This divergence likely reflects differences in scale format and response interface: the Trust Perception scale was administered using a continuous slider input, whereas the Trust in Industrial Human–Robot Collaboration scale employed discrete Likert-style response options.</p>
<p>Informal observation during administration and post-hoc inspection of item-level variance suggest that the slider-based interface, administered via a touchpad, may have reduced response precision relative to discrete response formats. While this likely attenuated item-level variability, the Trust Perception Scale–HRI nevertheless captured meaningful between-condition differences at the aggregate level.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-post-eligible" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post-eligible-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="misty-paper_files/figure-html/fig-post-eligible-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post-eligible-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Distribution of Trust Perception in HRI by interaction policy. Points represent individual observations; violins depict score distributions. Red points indicate group means with 95% confidence intervals. Statistical comparisons are reported in the Results section.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Together, these models indicate that robot responsiveness had a consistent positive effect on post-interaction trust, with effect magnitude and measurement sensitivity varying by trust dimension and scale format.</p>
<section id="bayesian-analysis" class="level4">
<h4 class="anchored" data-anchor-id="bayesian-analysis">Bayesian analysis</h4>
<p>To complement frequentist mixed-effects models, Bayesian hierarchical models were fitted separately for each trust outcome using weakly informative priors. Models predicted post-interaction trust as a function of interaction policy (RESPONSIVE vs.&nbsp;CONTROL), with random intercepts for session and trust scale items to account for repeated measurement and item-level variability.</p>
<p>Across both trust measures, posterior estimates favored higher trust ratings in the RESPONSIVE condition. For the Trust in Industrial Human–Robot Collaboration scale, the estimated group difference was <em>[insert median]</em> points (95% credible interval [<em>lower</em>, <em>upper</em>]), with a posterior probability greater than <em>[insert]%</em> that the effect exceeded a practically meaningful threshold of five points on the 0–100 scale. Posterior mass exceeding ten points was <em>[insert]%</em>, indicating a substantial likelihood of a large effect.</p>
<p>For the Trust Perception Scale–HRI, the estimated group difference was smaller and more uncertain (<em>[insert median]</em> points; 95% credible interval [<em>lower</em>, <em>upper</em>]). Although the credible interval included zero, the posterior probability that the responsive condition increased trust was greater than <em>[insert]%</em>, suggesting a consistent directional effect with greater individual variability.</p>
<p>Sensitivity analyses using wider priors yielded nearly identical posterior estimates, indicating that results were not driven by prior specification.</p>
</section>
</section>
<section id="sensitivity-analysis-full-sample" class="level3">
<h3 class="anchored" data-anchor-id="sensitivity-analysis-full-sample">Sensitivity Analysis: Full Sample</h3>
<p>Including sessions classified as non-viable increased variability and attenuated estimated effect sizes across trust measures. As expected, posterior uncertainty increased relative to the eligible-sample analysis. However, directional trends favoring the RESPONSIVE condition remained evident across both trust outcomes.</p>
<div class="cell">
<div id="tbl-post-fullsample" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-post-fullsample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Table 3. Post-Interaction Raw Outcome Measures by Group
</figcaption>
<div aria-describedby="tbl-post-fullsample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="qqnrbcfrqe" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#qqnrbcfrqe table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#qqnrbcfrqe thead, #qqnrbcfrqe tbody, #qqnrbcfrqe tfoot, #qqnrbcfrqe tr, #qqnrbcfrqe td, #qqnrbcfrqe th {
  border-style: none;
}

#qqnrbcfrqe p {
  margin: 0;
  padding: 0;
}

#qqnrbcfrqe .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 13px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#qqnrbcfrqe .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#qqnrbcfrqe .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#qqnrbcfrqe .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#qqnrbcfrqe .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#qqnrbcfrqe .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#qqnrbcfrqe .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#qqnrbcfrqe .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#qqnrbcfrqe .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#qqnrbcfrqe .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#qqnrbcfrqe .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#qqnrbcfrqe .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#qqnrbcfrqe .gt_spanner_row {
  border-bottom-style: hidden;
}

#qqnrbcfrqe .gt_group_heading {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#qqnrbcfrqe .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#qqnrbcfrqe .gt_from_md > :first-child {
  margin-top: 0;
}

#qqnrbcfrqe .gt_from_md > :last-child {
  margin-bottom: 0;
}

#qqnrbcfrqe .gt_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#qqnrbcfrqe .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#qqnrbcfrqe .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#qqnrbcfrqe .gt_row_group_first td {
  border-top-width: 2px;
}

#qqnrbcfrqe .gt_row_group_first th {
  border-top-width: 2px;
}

#qqnrbcfrqe .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#qqnrbcfrqe .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#qqnrbcfrqe .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#qqnrbcfrqe .gt_last_summary_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#qqnrbcfrqe .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#qqnrbcfrqe .gt_first_grand_summary_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#qqnrbcfrqe .gt_last_grand_summary_row_top {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#qqnrbcfrqe .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#qqnrbcfrqe .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#qqnrbcfrqe .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#qqnrbcfrqe .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#qqnrbcfrqe .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#qqnrbcfrqe .gt_sourcenote {
  font-size: 90%;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#qqnrbcfrqe .gt_left {
  text-align: left;
}

#qqnrbcfrqe .gt_center {
  text-align: center;
}

#qqnrbcfrqe .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#qqnrbcfrqe .gt_font_normal {
  font-weight: normal;
}

#qqnrbcfrqe .gt_font_bold {
  font-weight: bold;
}

#qqnrbcfrqe .gt_font_italic {
  font-style: italic;
}

#qqnrbcfrqe .gt_super {
  font-size: 65%;
}

#qqnrbcfrqe .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#qqnrbcfrqe .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#qqnrbcfrqe .gt_indent_1 {
  text-indent: 5px;
}

#qqnrbcfrqe .gt_indent_2 {
  text-indent: 10px;
}

#qqnrbcfrqe .gt_indent_3 {
  text-indent: 15px;
}

#qqnrbcfrqe .gt_indent_4 {
  text-indent: 20px;
}

#qqnrbcfrqe .gt_indent_5 {
  text-indent: 25px;
}

#qqnrbcfrqe .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#qqnrbcfrqe div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>

<table class="gt_table do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-bootstrap="false">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="gt_col_headings header">
<th id="label" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col"><strong>Characteristic</strong></th>
<th id="stat_1" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>CONTROL</strong><br>
N = 13<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span></th>
<th id="stat_2" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>RESPONSIVE</strong><br>
N = 16<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span></th>
<th id="p.value" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>p-value</strong><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>2</sup></span></th>
</tr>
</thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_left" headers="label">Trust in Industrial HRI Collaboration</td>
<td class="gt_row gt_center" headers="stat_1">47 (26)</td>
<td class="gt_row gt_center" headers="stat_2">61 (26)</td>
<td class="gt_row gt_center" headers="p.value">0.094</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">Subscales</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Reliability subscale</td>
<td class="gt_row gt_center" headers="stat_1">46 (24)</td>
<td class="gt_row gt_center" headers="stat_2">62 (20)</td>
<td class="gt_row gt_center" headers="p.value">0.11</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Trust Perception subscale</td>
<td class="gt_row gt_center" headers="stat_1">49 (26)</td>
<td class="gt_row gt_center" headers="stat_2">57 (25)</td>
<td class="gt_row gt_center" headers="p.value">0.32</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Affective Trust subscale</td>
<td class="gt_row gt_center" headers="stat_1">59 (33)</td>
<td class="gt_row gt_center" headers="stat_2">72 (29)</td>
<td class="gt_row gt_center" headers="p.value">0.26</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">Trust Perception Scale–HRI</td>
<td class="gt_row gt_center" headers="stat_1">63 (17)</td>
<td class="gt_row gt_center" headers="stat_2">73 (19)</td>
<td class="gt_row gt_center" headers="p.value">0.16</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">Overall Task Accuracy</td>
<td class="gt_row gt_center" headers="stat_1">0.63 (0.20)</td>
<td class="gt_row gt_center" headers="stat_2">0.61 (0.27)</td>
<td class="gt_row gt_center" headers="p.value">0.98</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">Objective Measures</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Dialogue Turns</td>
<td class="gt_row gt_center" headers="stat_1">32 (10)</td>
<td class="gt_row gt_center" headers="stat_2">36 (11)</td>
<td class="gt_row gt_center" headers="p.value">0.95</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Avg Task Duration (mins)</td>
<td class="gt_row gt_center" headers="stat_1">12.84 (4.02)</td>
<td class="gt_row gt_center" headers="stat_2">16.81 (6.38)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.050</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Avg Response Time (ms)</td>
<td class="gt_row gt_center" headers="stat_1">15.1 (4.1)</td>
<td class="gt_row gt_center" headers="stat_2">17.2 (2.4)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.006</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Silent Periods</td>
<td class="gt_row gt_center" headers="stat_1">5.15 (2.27)</td>
<td class="gt_row gt_center" headers="stat_2">5.31 (2.82)</td>
<td class="gt_row gt_center" headers="p.value">0.88</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Engaged Responses</td>
<td class="gt_row gt_center" headers="stat_1">1.92 (2.25)</td>
<td class="gt_row gt_center" headers="stat_2">3.50 (1.83)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.020</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Frustrated Responses</td>
<td class="gt_row gt_center" headers="stat_1">0.54 (0.66)</td>
<td class="gt_row gt_center" headers="stat_2">0.88 (1.15)</td>
<td class="gt_row gt_center" headers="p.value">0.56</td>
</tr>
</tbody><tfoot>
<tr class="gt_footnotes odd">
<td colspan="4" class="gt_footnote"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span> Mean (SD)</td>
</tr>
<tr class="gt_footnotes even">
<td colspan="4" class="gt_footnote"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>2</sup></span> Wilcoxon rank sum test; Wilcoxon rank sum exact test</td>
</tr>
</tfoot>

</table>

</div>
</div>
</div>
</figure>
</div>
</div>
<p>These results indicate that while communication breakdown weakens the interpretability of trust measures, the overall pattern of results is not solely an artifact of exclusion decisions. Full-sample analyses are therefore treated as robustness checks reflecting real-world interaction variability rather than as alternative estimates of trust under functional interaction conditions.</p>
</section>
<section id="mechanism-analysis-communication-breakdown-as-a-failure-mode" class="level3">
<h3 class="anchored" data-anchor-id="mechanism-analysis-communication-breakdown-as-a-failure-mode">Mechanism Analysis: Communication Breakdown as a Failure Mode</h3>
<p>To examine why communication-based exclusion was necessary, interaction dynamics were compared between viable and non-viable sessions. Sessions classified as non-viable were characterized by substantially higher rates of speech-recognition failure, reduced dialogue coherence, increased task abandonment, and limited task-relevant information exchange.</p>
<p>Notably, under conditions of severe communication breakdown, the RESPONSIVE robot continued to generate proactive assistance, encouragement, and meta-communication aimed at repairing the interaction. However, these efforts did not restore mutual understanding and, in several cases, appeared to increase participant confusion and cognitive load. In contrast, the CONTROL robot’s reactive interaction policy resulted in fewer unsolicited interventions, which—while less supportive under normal conditions—reduced interaction complexity when language-mediated collaboration was no longer viable.</p>
<p>As a result, trust ratings in non-viable sessions did not systematically track the intended responsiveness manipulation. These findings suggest that when spoken-language interaction collapses, higher-level constructs such as trust and collaboration are no longer meaningfully instantiated. Communication viability therefore represents a boundary condition for evaluating affect-adaptive interaction policies in autonomous social robots.</p>
</section>
</section>
<section id="trust-subscale-patterns" class="level2">
<h2 class="anchored" data-anchor-id="trust-subscale-patterns">Trust subscale patterns</h2>
</section>
<section id="interaction-dynamics-and-task-performance" class="level2">
<h2 class="anchored" data-anchor-id="interaction-dynamics-and-task-performance">Interaction dynamics and task performance</h2>
<section id="task-performance" class="level3">
<h3 class="anchored" data-anchor-id="task-performance">Task performance</h3>
<p>Objective task accuracy did not differ between conditions across any task-level measures except suspect accuracy (robot dependendant task), indicating that increased trust was only attributable to improved task success when interaction was necessary to complete accurately.</p>
<p>Despite similar task accuracy, interactions in the responsive condition were characterized by longer durations, slower response times, and a higher number of AI-detected engaged responses. These findings suggest that responsiveness altered the interaction dynamics and affective tone rather than task outcomes.</p>
</section>
</section>
<section id="individual-differences-and-correlational-patterns" class="level2">
<h2 class="anchored" data-anchor-id="individual-differences-and-correlational-patterns">Individual differences and correlational patterns</h2>
<p>As expected, we found that higher Need for Cognition (NFC) scores were negatively associated with Negative Attitudes Towards Robots (NARS), indicating that individuals who enjoy effortful thinking tend to have more positive attitudes towards robots. This relationship is consistent with prior literature suggesting that cognitive engagement is associated with openness to new technologies. In terms of NARS subscales, NFC was negatively correlated with all three subscales, but significantly so only in the domain of Situations of Interaction with Robots. This suggests that individuals with higher NFC are less likely to hold negative attitudes across various dimensions of robot interaction but especially around direct interaction with robots.</p>
<p>–&gt; how to talk about post-interaction correlations w/pre-interaction measures Several behavioural and task-level measures were correlated with post-interaction trust, consistent with the interpretation that trust judgments were shaped by interaction quality; these variables were not included as covariates in primary models to avoid conditioning on potential mediators.</p>
<p>Baseline negative attitudes toward robots were negatively correlated with post-interaction trust, with the strongest associations observed for affective trust subscales. In contrast, objective task performance was selectively associated with perceived reliability. Need for cognition was negatively correlated with negative robot attitudes and interaction-level negative affect, suggesting that individual differences contributed to variability in trust responses.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-corr" class="quarto-float quarto-figure quarto-figure-center anchored" width="672">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-corr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="misty-paper_files/figure-html/fig-corr-1.png" id="fig-corr" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-corr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6
</figcaption>
</figure>
</div>
</div>
</div>
<section id="model-robustness-and-predictive-checks" class="level3">
<h3 class="anchored" data-anchor-id="model-robustness-and-predictive-checks">Model robustness and predictive checks</h3>
<p>Sensitivity analyses using alternative prior specifications yielded substantively similar estimates, and leave-one-out cross-validation indicated comparable predictive performance between models with and without the group effect.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>TO DO:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>add subscale column to long format data</li>
<li>run an analysis of performance by robot-dependent versus robot-independent tasks</li>
<li>write up a future directions section for the planned larger study</li>
<li>talk about unexpected language issues with people signing up with difficultly speaking and understanding english which cuased problems with asr and interaction</li>
<li>run analysis of dialogue dynamics included Bertopic or some other analysis of the actual content of the conversations/interactions</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>TODO2
</div>
</div>
<div class="callout-body-container callout-body">
<p>Manually score each dialogue series.</p>
<p>For each interaction and stage:</p>
<ul>
<li>did the participant ask for help?</li>
<li>how many times?</li>
<li>did the robot give useful help?</li>
<li>did the robot give misleading or incorrect help?</li>
<li>did the robot stick to the policy?</li>
<li>how many times did the robot fail to understand the participant?</li>
</ul>
<p>For each task:</p>
<ul>
<li>is there evidence that the robot helped complete the task?</li>
<li>is there evidence that the participant solved the problem without help?</li>
</ul>
</div>
</div>
</section>
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p>Mention language confounders!! The present findings also highlight an important boundary condition for trust measurement in spoken-language HRI. When language-mediated interaction collapses entirely, higher-level constructs such as trust and collaboration are no longer meaningfully defined. Under such conditions, trust does not simply decrease; rather, the interaction fails to instantiate the prerequisites necessary for trust formation. This distinction is critical for both system evaluation and experimental design, particularly as autonomous robots are deployed in linguistically diverse, real-world environments.</p>
<p>Because the study relied fundamentally on spoken-language collaboration, sessions exhibiting persistent communication failure were classified as <strong>protocol non-adherence</strong> and excluded from task-level analyses (<em>n</em> = 6). While the experimenter documented all cases where language might pose an issue (as observed when meeting each participant), exclusion decisions were based solely on actual communication viability and interaction mechanics, not on task outcomes or trust measures.</p>
<p>The second task was intentionally designed to be sufficiently challenging that completing it within the allotted time was difficult without assistance. This ensured that interaction with the robot represented a meaningful opportunity for collaboration rather than a trivial or purely optional exchange. By contrasting a robot-dependent task with an open-ended advisory task, the study examined trust formation across interaction contexts that varied in both informational asymmetry and reliance on the robot.</p>
<p>This pilot study examined trust outcomes following in-person interaction with an autonomous social robot under two interaction policies: a responsive, affect-adaptive condition and a neutral, non-responsive control condition. By leveraging a fully autonomous dialogue system integrated with speech recognition and affect detection, the study aimed to evaluate how robot responsiveness influences trust formation in realistic human–robot collaboration scenarios.</p>
<p>Descriptive comparisons of post-interaction measures indicated that participants in the responsive condition reported consistently higher trust across all trust measures, with differences ranging from approximately 8 to 16 points on a 0–100 scale, although uncertainty remained high given the small sample. Notably, the responsive condition did not differ from control in objective task accuracy, suggesting that increased trust was not driven by improved task success. Instead, responsive interactions were characterized by longer durations, slower response times, and a higher number of AI-detected engaged responses, indicating a shift in interaction dynamics rather than performance.</p>
<p>Baseline negative attitudes toward robots were most strongly associated with affective components of trust rather than perceptions of reliability, suggesting that pre-existing attitudes primarily shape emotional responses to interaction rather than judgments of system competence. Conversely, objective task performance was selectively associated with perceived reliability, indicating that participants distinguished between affective and functional aspects of trust.</p>
<p>Future work with larger samples could formally test mediation pathways linking robot responsiveness, interaction fluency, affective responses, and trust judgments, as well as moderation by baseline attitudes toward robots and need for cognition.</p>
<p>Participants in the responsive condition also exhibited higher levels of AI-detected engagement during interaction, as indexed by a greater number of responses classified as positive affect (t-test result). This suggests that responsive behaviours altered the affective tone of the interaction itself.</p>
<section id="technical-challenges" class="level2">
<h2 class="anchored" data-anchor-id="technical-challenges">Technical challenges</h2>
<p>Need to discuss that these items were on a 0-100 scale that required sliding a bar, while the other trust scale was on a 1-5 Likert that required simply clicking. The post test was administered on a laptop with a trackpad which may have caused difficulties for some participants who found it difficult to drag the slider with the trackpad. This could have introduced additional noise into the measurement of this scale, which may explain why the effects were somewhat weaker here.</p>
<ul>
<li>Need to talk about language issues with participants who had difficulty speaking and understanding English which caused problems with ASR and interaction.</li>
<li>Need to talk about issues where the AI was not able to flexibly handle when people asked a question about the suspect that was close to or another word for a ground-truth feature but not exactly the same word, causing confusion and miscommunication. E.g., “Was the suspect wearing pink?” The ground-truth feature was top: PINK, top-type: HOODIE; but the ASR and NLU did not extrapolate to understand that “wearing pink” referred to the same feature as “top: PINK”, causing confusion and miscommunication. Maybe the prompt could have included some examples of different phrasing which could improve this? To solve this issue in future work, we can expand the NLU training data to include more paraphrases and synonyms for each feature.</li>
</ul>
<p>There was also a case where someone asked ‘is the top shirt hoodie red?’ to which the AI answered YES. It may have been confused by the multiple descriptors in the question. Future work could involve improving the NLU to handle more complex queries with multiple attributes.</p>
<p>Discuss future work where we will look investigate the ‘embodied’ effect of having a physical robot versus a virtual agent on trust and collaboration in HRI.</p>
<p>Also, prompt could include examples of what to do when dialogue appears fragmented, to remind participants to wait until the blue light is on before speaking and to switch up its phrasing if the robot seems to not understand.</p>
<p>Also, the control condition seemed to be somewhat neutered in terms of flexibility in responding in different ways. it would always respond with the exact same phrase when confronted with a sentence fragment or a question it could not directly answer.</p>
<p>Also issues with people not paying attention to the robot’s visual cues to know when to speak, leading to more fragmented dialogue. Future work could involve improving participant instructions, improved latency and ‘listening’ … and the robot’s feedback mechanisms to better manage turn-taking.</p>
<p>Need to remember to flag participants who did not complete/skipped specific tasks. E.g. P56 skipped the wrapup entirely. Many skipped the brief (by advancing on their own through the dashboard).</p>
</section>
</section>
<section id="conclusion-and-future-work" class="level1">
<h1>Conclusion and Future Work</h1>
</section>

<section id="technical-specifications" class="level1">
<h1>Technical Specifications</h1>
<section id="system-overview" class="level2">
<h2 class="anchored" data-anchor-id="system-overview">System Overview</h2>
<p>This study implements a multi-stage collaborative task system where participants collaborate with the Misty II social robot to solve a who-dunniti type task. The system utilizes an autonomous, mixed-initiative dialogue architecture with affect-responsive capabilities.</p>
</section>
<section id="hardware-platform" class="level2">
<h2 class="anchored" data-anchor-id="hardware-platform">Hardware Platform</h2>
<p><strong>Robot</strong>: Misty II Social Robot (Furhat Robotics)</p>
<ul>
<li>Mobile social robot platform with expressive display, arm actuators, and head movement</li>
<li>RGB LED for state indication</li>
<li>RTSP video streaming (1920×1080, 30fps) for audio capture</li>
<li>Custom action scripting for synchronized multimodal expressions</li>
</ul>
</section>
<section id="software-architecture" class="level2">
<h2 class="anchored" data-anchor-id="software-architecture">Software Architecture</h2>
<section id="core-system-components" class="level3">
<h3 class="anchored" data-anchor-id="core-system-components">Core System Components</h3>
<p><strong>Programming Language</strong>: Python 3.10</p>
<p><strong>Primary Dependencies</strong>:</p>
<ul>
<li><code>misty-sdk</code> (Python SDK for Misty Robotics API) - Robot control and sensor access</li>
<li><code>deepgram-sdk</code> (4.8.1) - Speech-to-text processing</li>
<li><code>ffmpeg-python</code> (0.2.0) - Audio stream processing</li>
<li><code>flask</code> (3.1.2) + <code>flask-socketio</code> (5.5.1) - Web interface for task presentation</li>
<li><code>duckdb</code> (1.4.0) - Experimental data logging database</li>
</ul>
</section>
<section id="large-language-models" class="level3">
<h3 class="anchored" data-anchor-id="large-language-models">Large Language Models</h3>
<p><strong>LLM Provider</strong>:</p>
<p><strong>Google Gemini</strong>:</p>
<ul>
<li>Model: <code>gemini-2.5-flash-lite</code> (configurable via environment variable)</li>
<li>Integration: <code>langchain-google-genai</code> with <code>google-generativeai</code> API</li>
<li>Response format: JSON-only output (<code>response_mime_type: "application/json"</code>). This format is required by Misty-II for reliable parsing and for action execution.</li>
</ul>
<p><strong>LLM Configuration</strong>:</p>
<ul>
<li>Temperature: 0.7 (for balanced creativity and coherence)</li>
<li>Memory: Conversation buffer memory with file-based persistence (<code>langchain.memory.ConversationBufferMemory</code>)</li>
<li>Context window: Full conversation history maintained across interaction stages but reset between sessions.</li>
</ul>
</section>
</section>
<section id="langchain-framework-integration" class="level2">
<h2 class="anchored" data-anchor-id="langchain-framework-integration">LangChain Framework Integration</h2>
<section id="core-langchain-components" class="level3">
<h3 class="anchored" data-anchor-id="core-langchain-components">Core LangChain Components</h3>
<p><strong>Framework Version</strong>: <code>langchain-core</code> with modular provider packages</p>
<ul>
<li><code>langchain</code> (meta-package)</li>
<li><code>langchain-community</code> (0.3.31)</li>
<li><code>langchain-google-genai</code> Gemini integration</li>
</ul>
</section>
<section id="conversationchain-architecture" class="level3">
<h3 class="anchored" data-anchor-id="conversationchain-architecture">ConversationChain Architecture</h3>
<p><strong>Memory Management</strong> (<code>ConversationChain</code> class in <code>conversation_chain.py</code>):</p>
<ol type="1">
<li><strong>Conversation Buffer Memory</strong>:
<ul>
<li>Implementation: <code>langchain.memory.ConversationBufferMemory</code></li>
<li>Storage: File-based persistent chat history (<code>FileChatMessageHistory</code>)</li>
<li>Format: JSON files in <code>.memory/</code> directory, one per participant session</li>
<li>Memory key: <code>"history"</code></li>
<li>Return format: Message objects (full conversation context)</li>
</ul></li>
<li><strong>Memory Reset Policy</strong>:
<ul>
<li>Default: Reset on each new session launch</li>
<li>Archive previous session: Timestamped archive files stored in <code>.memory/archive/</code></li>
<li>Configuration: <code>RESET_MEMORY</code> and <code>ARCHIVE_MEMORY</code> environment variables</li>
</ul></li>
</ol>
</section>
<section id="prompt-construction" class="level3">
<h3 class="anchored" data-anchor-id="prompt-construction">Prompt Construction</h3>
<p><strong>Message Structure</strong></p>
<p>(LangChain message types): <code>python [SystemMessage, *history_messages, HumanMessage]</code></p>
<p>System Message Assembly:</p>
<ul>
<li>Core instructions (task framing, role definition)</li>
<li>Personality instructions (mode-specific behaviour)</li>
<li>Stage-specific instructions (current task context)</li>
<li>Output format constraints (JSON schema specification)</li>
</ul>
<pre><code>Human Message Format:   {     
"user": "&lt;transcribed_speech&gt;",     
"stage": "&lt;current_stage&gt;",     
"detected_emotion": "&lt;emotion_label&gt;",     
"frustration_note": "&lt;optional_alert&gt;",     
"timer_expired": "&lt;task_id&gt;",     ...   }</code></pre>
<ul>
<li>JSON-encoded context variables passed alongside user input</li>
<li>Enables LLM to access environmental state without breaking message history</li>
</ul>
</section>
<section id="memory-persistence" class="level3">
<h3 class="anchored" data-anchor-id="memory-persistence">Memory Persistence:</h3>
<ul>
<li>Save after each turn: memory.save_context({“input”: user_text}, {“output”: llm_response})</li>
<li>Maintains conversational coherence across multi-stage interaction</li>
<li>Enables LLM to reference previous exchanges (e.g., “As I mentioned earlier…”)</li>
</ul>
</section>
<section id="langchain-design-rationale" class="level3">
<h3 class="anchored" data-anchor-id="langchain-design-rationale">LangChain Design Rationale</h3>
<p>Why LangChain for this application:</p>
<ol type="1">
<li>Memory abstraction: Automatic conversation history management without manual message list handling</li>
<li>Provider flexibility: Easy switching between Gemini and OpenAI without rewriting prompt logic</li>
<li>Message typing: Structured SystemMessage/HumanMessage/AIMessage types maintain role clarity</li>
<li>File persistence: Built-in FileChatMessageHistory enables session recovery and archiving</li>
<li>Future extensibility: Framework supports adding tools, retrieval, or multi-agent patterns if needed</li>
</ol>
<p>Alternatives considered: Direct API calls would reduce dependencies but require reimplementing conversation history management, prompt templating, and cross-provider compatibility layers.</p>
</section>
<section id="langchain-limitations-in-this-context" class="level3">
<h3 class="anchored" data-anchor-id="langchain-limitations-in-this-context">LangChain Limitations in This Context</h3>
<ul>
<li>No chains used: Despite name ConversationChain, this is a direct LLM wrapper (no LangChain Expression Language chains)</li>
<li>No tools/agents: Simple request-response pattern (could extend for future tool-use capabilities)</li>
<li>Custom JSON parsing: LangChain’s built-in output parsers not used; custom extraction handles malformed responses more robustly</li>
</ul>
</section>
<section id="speech-processing" class="level3">
<h3 class="anchored" data-anchor-id="speech-processing">Speech Processing</h3>
<p><strong>Speech-to-Text (STT)</strong>:</p>
<ul>
<li>Provider: Deepgram Nova-2 (<code>deepgram-sdk</code> 4.8.1)</li>
<li>Model: <code>nova-2</code> with US English (<code>en-US</code>)</li>
<li>Smart formatting enabled</li>
<li>Interim results for real-time partial transcription</li>
<li>Voice Activity Detection (VAD) events</li>
<li>Adaptive endpointing: 200ms (conversational stages) / 500ms (log-reading task)</li>
<li>Utterance end timeout: 1000ms (conversational) / 2000ms (log-reading)</li>
<li>Audio processing: RTSP stream from Misty → FFmpeg MP3 encoding → Deepgram WebSocket</li>
</ul>
<p><strong>Text-to-Speech (TTS)</strong> - Three options:</p>
<ol type="1">
<li><p><strong>Misty Onboard TTS</strong> (this is the one we used): Native robot voice via onboard TTS</p></li>
<li><p><strong>OpenAI TTS</strong>:</p>
<ul>
<li>Model: <code>tts-1</code> (low-latency variant)</li>
<li>Voice: <code>sage</code></li>
<li>Format: MP3, served via HTTP (port 8000)</li>
<li>Ultimately chose not to use because we wanted a more robotic, non-human voice</li>
<li>Didn’t want the human voice influencing trust on its own (future research could look at trust in relation to type of voice)</li>
</ul></li>
<li><p><strong>Deepgram Aura</strong>:</p>
<ul>
<li>Model: <code>aura-stella-en</code> (conversational female voice)</li>
<li>Format: MP3, served via HTTP</li>
<li>Ultimately chose not to use because we wanted a more robotic, non-human voice</li>
</ul></li>
</ol>
</section>
<section id="emotion-detection" class="level3">
<h3 class="anchored" data-anchor-id="emotion-detection">Emotion Detection</h3>
<p><strong>Model</strong>: DistilRoBERTa-base fine-tuned on emotion classification</p>
<ul>
<li>HuggingFace identifier: <code>j-hartmann/emotion-english-distilroberta-base</code></li>
<li>Framework: <code>transformers</code> (4.57.1) pipeline</li>
<li>Hardware: CUDA GPU acceleration (automatic fallback to CPU)</li>
<li>Output classes: joy, anger, sadness, fear, disgust, surprise, neutral</li>
<li>Mapped to interaction states: positively engaged, irritated, disappointed, anxious, frustrated, curious, neutral</li>
</ul>
</section>
<section id="multimodal-robot-behaviour" class="level3">
<h3 class="anchored" data-anchor-id="multimodal-robot-behaviour">Multimodal Robot behaviour</h3>
<p><strong>Expression System</strong>: 25 custom action scripts combining:</p>
<ul>
<li>LLM was prompted to choose an appropriate expression from a predefined set based on context.</li>
<li>Facial displays (image eye-expression files on screen)</li>
<li>LED color patterns (solid, breathe, blink)</li>
<li>Arm movements (bilateral position control)</li>
<li>Head movements (pitch, yaw, roll control)</li>
</ul>
<p><strong>Nonverbal Backchannel behaviours</strong> (RESPONSIVE mode only):</p>
<ul>
<li>Real-time listening cues triggered by partial transcripts (disfluencies, hesitation markers)</li>
<li>Emotion-matched expressions (e.g., “concern” for hesitation, “excited” for breakthroughs)</li>
</ul>
<p><strong>LED State Indicators</strong>:</p>
<ul>
<li>Blue (0, 199, 252): Actively listening (microphone open)</li>
<li>Purple (100, 70, 160): Processing/speaking (microphone closed)</li>
</ul>
</section>
</section>
<section id="data-collection" class="level2">
<h2 class="anchored" data-anchor-id="data-collection">Data Collection</h2>
<p><strong>Database</strong>: DuckDB relational database (<code>experiment_data.duckdb</code>)</p>
<p><strong>Logged Data</strong>:</p>
<p>1. <strong>Sessions table</strong>: participant ID (auto-incremented P01, P02…), condition assignment, timestamps, duration</p>
<p>2. <strong>Dialogue turns table</strong>: turn-by-turn user input, LLM response, expression, response latency (ms), behavioural flags</p>
<p>3. <strong>Task responses table</strong>: submitted answers with timestamps and time-on-task</p>
<p>4. <strong>Events table</strong>: stage transitions, silence check-ins, timer expirations, detected emotions</p>
</section>
<section id="interaction-dynamics" class="level2">
<h2 class="anchored" data-anchor-id="interaction-dynamics">Interaction Dynamics</h2>
<section id="silence-handling" class="level3">
<h3 class="anchored" data-anchor-id="silence-handling">Silence Handling</h3>
<p><strong>Silence detection</strong>: 25-second threshold triggers check-in prompt</p>
<ul>
<li>RESPONSIVE: “Still working on it? No rush - I’m here if you need help!”</li>
<li>CONTROL: “I am ready when you have a question.”</li>
</ul>
</section>
<section id="emotion-responsive-behaviours-responsive-condition-only" class="level3">
<h3 class="anchored" data-anchor-id="emotion-responsive-behaviours-responsive-condition-only">Emotion-Responsive behaviours (RESPONSIVE condition only)</h3>
<p><strong>Frustration tracking</strong>:</p>
<ul>
<li>Consecutive detection of frustrated/anxious/irritated/disappointed states</li>
<li>Threshold: ≥2 consecutive frustrated turns triggers proactive support</li>
<li>RESPONSIVE adaptation: “This part can be tough. Want me to walk you through it?”</li>
</ul>
<p><strong>Positive emotion matching</strong>:</p>
<ul>
<li>Celebratory language for curious/engaged states</li>
<li>Momentum maintenance: “Yes! Great observation!”</li>
</ul>
<p><strong>Run Mode</strong>: Set programmatically in <code>mistyGPT_emotion.py</code> line 126:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>RUN_MODE <span class="op">=</span> <span class="st">"RESPONSIVE"</span>  <span class="co"># or "CONTROL"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
</section>
<section id="prompt-engineering" class="level2">
<h2 class="anchored" data-anchor-id="prompt-engineering">Prompt Engineering</h2>
<p>Modular prompt system (PromptLoader class):</p>
<ul>
<li>core_system.md: Task framing, role description, output format schema</li>
<li>role_responsive.md / role_control.md: Condition-specific personality instructions</li>
<li>stage1_greeting.md through stage5_wrap_up.md: Stage-specific task instructions.</li>
</ul>
<p>Context injection: Real-time contextual variables passed to LLM:</p>
<ul>
<li>Current stage</li>
<li>Detected emotion (if enabled)</li>
<li>Task submission status</li>
<li>Timer expiration notifications</li>
<li>Silence check-in flags</li>
</ul>
</section>
<section id="inter-process-communication" class="level2">
<h2 class="anchored" data-anchor-id="inter-process-communication">Inter-process Communication</h2>
<p>Flask REST API endpoints:</p>
<ul>
<li>GET /stage_current: Synchronize stage state with facilitator GUI</li>
<li>GET /task_submission_status: Detect participant task submissions</li>
<li>GET /timer_expired_status: Detect timer expirations</li>
<li>POST /stage: Update stage (facilitator override)</li>
<li>POST /reset_timer: Clear timer expiration flags</li>
</ul>

</section>
</section>

<div id="quarto-appendix" class="default"><section id="appendix" class="level1 appendix"><h2 class="anchored quarto-appendix-heading">Appendix</h2><div class="quarto-appendix-contents">

<section id="dialogue-coding-scheme" class="level2">
<h2 class="anchored" data-anchor-id="dialogue-coding-scheme">Dialogue Coding Scheme</h2>
<section id="task-outcome-layer-stage-level" class="level3">
<h3 class="anchored" data-anchor-id="task-outcome-layer-stage-level">Task Outcome Layer (Stage-Level)</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>task_outcome</code></td>
<td>categorical</td>
<td>Final task status (<code>completed</code>, <code>timeout</code>, <code>skipped</code>, <code>partial</code>, <code>abandoned</code>). Exactly one per task.</td>
</tr>
<tr class="even">
<td><code>task_completed</code></td>
<td>binary</td>
<td>Task goal was fully completed within the allotted time.</td>
</tr>
<tr class="odd">
<td><code>task_timed_out</code></td>
<td>binary</td>
<td>Task ended due to expiration of the time limit before completion.</td>
</tr>
<tr class="even">
<td><code>task_skipped</code></td>
<td>binary</td>
<td>Participant explicitly skipped or advanced past the task without completing it.</td>
</tr>
<tr class="odd">
<td><code>task_partially_completed</code></td>
<td>binary</td>
<td>Task progress was made, but the full solution was not reached.</td>
</tr>
<tr class="even">
<td><code>task_abandoned</code></td>
<td>binary</td>
<td>Participant disengaged or stopped attempting the task before timeout.</td>
</tr>
<tr class="odd">
<td><code>task_time_remaining_sec</code></td>
<td>numeric</td>
<td>Time remaining (in seconds) when the task ended; 0 if timed out.</td>
</tr>
<tr class="even">
<td><code>task_completed_without_help</code></td>
<td>binary</td>
<td>Task was completed without any help requests to the robot.</td>
</tr>
<tr class="odd">
<td><code>task_required_robot_help</code></td>
<td>binary</td>
<td>At least one robot help interaction was required for task completion.</td>
</tr>
</tbody>
</table>
</section>
<section id="dialogue-interaction-layer-turn-level" class="level3">
<h3 class="anchored" data-anchor-id="dialogue-interaction-layer-turn-level">Dialogue Interaction Layer (Turn-Level)</h3>
<section id="human-turn-codes" class="level4">
<h4 class="anchored" data-anchor-id="human-turn-codes">Human Turn Codes</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>human_help_request</code></td>
<td>binary</td>
<td>Participant explicitly or implicitly asks the robot for help or guidance.</td>
</tr>
<tr class="even">
<td><code>human_reasoning_self</code></td>
<td>binary</td>
<td>Participant articulates their own reasoning or problem-solving independent of the robot.</td>
</tr>
<tr class="odd">
<td><code>human_confusion</code></td>
<td>binary</td>
<td>Participant expresses confusion or uncertainty.</td>
</tr>
<tr class="even">
<td><code>human_confirmation_seeking</code></td>
<td>binary</td>
<td>Participant seeks confirmation of a tentative belief or solution.</td>
</tr>
<tr class="odd">
<td><code>human_ignores_robot</code></td>
<td>binary</td>
<td>Participant proceeds without engaging with the robot’s prior input.</td>
</tr>
</tbody>
</table>
</section>
<section id="robot-turn-codes" class="level4">
<h4 class="anchored" data-anchor-id="robot-turn-codes">Robot Turn Codes</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>robot_helpful_guidance</code></td>
<td>binary</td>
<td>Robot provides accurate, task-relevant guidance.</td>
</tr>
<tr class="even">
<td><code>robot_misleading_guidance</code></td>
<td>binary</td>
<td>Robot provides misleading or incorrect guidance.</td>
</tr>
<tr class="odd">
<td><code>robot_factually_incorrect</code></td>
<td>binary</td>
<td>Robot states information that is objectively incorrect.</td>
</tr>
<tr class="even">
<td><code>robot_policy_violation</code></td>
<td>binary</td>
<td>Robot violates stated system or task constraints.</td>
</tr>
<tr class="odd">
<td><code>robot_on_policy_unhelpful</code></td>
<td>binary</td>
<td>Robot adheres to policy but provides vague or non-actionable assistance.</td>
</tr>
<tr class="even">
<td><code>robot_stt_failure</code></td>
<td>binary</td>
<td>Robot response reflects a speech-to-text or input understanding failure.</td>
</tr>
<tr class="odd">
<td><code>robot_clarification_request</code></td>
<td>binary</td>
<td>Robot asks the participant to repeat or clarify their input.</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="affective-interaction-layer-turn-level" class="level3">
<h3 class="anchored" data-anchor-id="affective-interaction-layer-turn-level">Affective Interaction Layer (Turn-Level)</h3>
<section id="robot-affective-behaviour" class="level4">
<h4 class="anchored" data-anchor-id="robot-affective-behaviour">Robot Affective behaviour</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>robot_empathy_expression</code></td>
<td>binary</td>
<td>Robot expresses empathy, encouragement, or reassurance.</td>
</tr>
<tr class="even">
<td><code>robot_emotion_acknowledgement</code></td>
<td>binary</td>
<td>Robot explicitly references an inferred participant emotional state.</td>
</tr>
<tr class="odd">
<td><code>robot_affect_task_aligned</code></td>
<td>binary</td>
<td>Robot’s affective response is appropriate and supportive in context.</td>
</tr>
<tr class="even">
<td><code>robot_affect_misaligned</code></td>
<td>binary</td>
<td>Robot’s affective response is mistimed or disruptive to the task.</td>
</tr>
</tbody>
</table>
</section>
<section id="human-affective-response" class="level4">
<h4 class="anchored" data-anchor-id="human-affective-response">Human Affective Response</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>human_affective_engagement</code></td>
<td>binary</td>
<td>Participant responds in a socially warm or emotionally engaged manner.</td>
</tr>
<tr class="even">
<td><code>human_social_reciprocity</code></td>
<td>binary</td>
<td>Participant mirrors or responds to the robot’s affective expression.</td>
</tr>
<tr class="odd">
<td><code>human_anthropomorphic_language</code></td>
<td>binary</td>
<td>Participant treats the robot as a social agent.</td>
</tr>
<tr class="even">
<td><code>human_emotional_disengagement</code></td>
<td>binary</td>
<td>Participant responds in a curt, dismissive, or withdrawn manner.</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="notes" class="level3">
<h3 class="anchored" data-anchor-id="notes">Notes</h3>
<ul>
<li>Turn-level variables are coded per dialogue turn.</li>
<li>Task outcome variables are coded once per <code>session_id × stage</code>.</li>
<li>Raw dialogue text was retained during coding and removed prior to aggregation.</li>
<li>Multiple turn-level codes may co-occur unless otherwise specified.</li>
</ul>
</section>
</section>
<section id="key-components-of-the-system" class="level2">
<h2 class="anchored" data-anchor-id="key-components-of-the-system">Key Components of the System</h2>
<p>This study implemented a multi-stage collaborative task system where participants collaborate with the Misty II social robot to solve a who-dunnit type task. The system utilizes an autonomous, mixed-initiative dialogue architecture via langchain with affect-responsive capabilities.</p>
<ol type="1">
<li><p>Misty-II Robot: A programmable robot platform equipped with sensors and actuators for interaction.</p></li>
<li><p>Automated Speech Recognition (ASR): A speech-to-speech pipeline that processes spoken input from users and converts it into text for LLM processing then back to speech for output on the robot.</p>
<ul>
<li>STT: Deepgram API for real-time speech-to-text conversion.</li>
<li>DistilRoBERTa-base fine-tuned on emotion classification for emotion detection from user utterances</li>
<li>LLM: Gemini API for processing text input and generating contextually relevant responses in JSON format</li>
<li>TTS: Misty-II text-to-speech (TTS) engine on 820 processor.</li>
</ul></li>
<li><p>Langchain Dialogue Management: A system that manages the flow of conversation, ensuring coherent and contextually appropriate dialogue within a two-part collaborative task.</p></li>
<li><p>Collaborative-Tasks</p>
<ul>
<li>Task 1: Whodunnit style task where human and robot collaborate to find a missing robot via the human asking Yes/No questions (process of elimination in 6x4 suspect grid) to the robot. Robot knows ground truth but can only answer Yes/No questions about suspect features. Can not directly describe the suspect or name them. (human can choose a random suspect to solve on their own but only 1 in 24 chance of being correct without robot help)</li>
<li>Task 2: Where is Atlas? Robot collaborates with human to find Atlas by deciphering cryptic system and sensor logs. Robot does not know the answer here and can only guide the human usinng its expertise and knowledge of computer systems and basic logical reasoning. (human can solve on their own but very difficult without robot help depending on participants technical background).</li>
</ul></li>
<li><p>Flask-gui dashboard interface: A web-based interface/dashboard that allowed participants to interact with the tasks, view task-related information and input their answers to the questions. Responses were sent to the robot to signal task progression.</p>
<ul>
<li>Task 1 dashboard: Displays the suspect grid and allows the user to select suspects and view their features.</li>
<li>Task 2 dashboard: Displays system logs and allows the user to input their findings.</li>
</ul></li>
<li><p>Pre and post tests:</p>
<ul>
<li>PRE-TESTS: Need for Cognition Scale (short); Negative Attitudes to Robots Scale (NARS);</li>
<li>POST-TESTS: Trust Perception Scale-HRI; 9 custom questions adapted from Charalambous et al.&nbsp;(2020) on trust in industrial human-robot collaboration;</li>
</ul></li>
</ol>
</section>
</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-campagna2025" class="csl-entry" role="listitem">
Campagna, Giulio, and Matthias Rehm. 2025. <span>“A Systematic Review of Trust Assessments in Human<span></span>robot Interaction.”</span> <em>J. Hum.-Robot Interact.</em> 14 (2): 30:130:35. <a href="https://doi.org/10.1145/3706123">https://doi.org/10.1145/3706123</a>.
</div>
<div id="ref-emaminejad2022" class="csl-entry" role="listitem">
Emaminejad, Newsha, and Reza Akhavian. 2022. <span>“Trustworthy AI and Robotics: Implications for the AEC Industry.”</span> <em>Automation in Construction</em> 139 (July): 104298. <a href="https://doi.org/10.1016/j.autcon.2022.104298">https://doi.org/10.1016/j.autcon.2022.104298</a>.
</div>
<div id="ref-fartook2025" class="csl-entry" role="listitem">
Fartook, Ori, Zachary McKendrick, Tal Oron-Gilad, and Jessica R. Cauchard. 2025. <span>“Enhancing Emotional Support in Human-Robot Interaction: Implementing Emotion Regulation Mechanisms in a Personal Drone.”</span> <em>Computers in Human Behavior: Artificial Humans</em> 4 (May): 100146. <a href="https://doi.org/10.1016/j.chbah.2025.100146">https://doi.org/10.1016/j.chbah.2025.100146</a>.
</div>
<div id="ref-fu2021" class="csl-entry" role="listitem">
Fu, Wenjun, Ying Xu, Liangping Liu, and Liang Zhang. 2021. <span>“Design and Research of Intelligent Safety Monitoring Robot for Coal Mine Shaft Construction.”</span> Edited by Sang-Bing Tsai. <em>Advances in Civil Engineering</em> 2021 (1): 6897767. <a href="https://doi.org/10.1155/2021/6897767">https://doi.org/10.1155/2021/6897767</a>.
</div>
<div id="ref-lin2022" class="csl-entry" role="listitem">
Lin, Ting-Han, Spencer Ng, and Sarah Sebo. 2022. <span>“2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN).”</span> In, 37–44. <a href="https://doi.org/10.1109/RO-MAN53752.2022.9900828">https://doi.org/10.1109/RO-MAN53752.2022.9900828</a>.
</div>
<div id="ref-racette2024" class="csl-entry" role="listitem">
Racette, John. 2024. <span>“Design of a Hybrid Robotic System for Underground Mine Rescue Applications.”</span> PhD thesis. <a href="https://search.proquest.com/openview/ba52de2b7c6d03f61111afbe990e6371/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y">https://search.proquest.com/openview/ba52de2b7c6d03f61111afbe990e6371/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y</a>.
</div>
<div id="ref-shayganfar2019" class="csl-entry" role="listitem">
Shayganfar, Mahni, Charles Rich, Candace Sidner, and Benjamin Hylák. 2019. <span>“2019 IEEE International Conference on Humanized Computing and Communication (HCC).”</span> In, 7–15. <a href="https://doi.org/10.1109/HCC46620.2019.00010">https://doi.org/10.1109/HCC46620.2019.00010</a>.
</div>
<div id="ref-devisser2020" class="csl-entry" role="listitem">
Visser, Ewart J. de, Marieke M. M. Peeters, Malte F. Jung, Spencer Kohn, Tyler H. Shaw, Richard Pak, and Mark A. Neerincx. 2020. <span>“Towards a Theory of Longitudinal Trust Calibration in Human<span></span>Robot Teams.”</span> <em>International Journal of Social Robotics</em> 12 (2): 459–78. <a href="https://doi.org/10.1007/s12369-019-00596-x">https://doi.org/10.1007/s12369-019-00596-x</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>