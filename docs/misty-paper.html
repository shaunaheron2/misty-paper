<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Trust in Autonomous Human–Robot Collaboration: Effects of Responsive Interaction Policies</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-1d4f84655f446305ac42a8f1abcf7405.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods"><span class="header-section-number">2</span> Methods</a>
  <ul>
  <li><a href="#interaction-policies" id="toc-interaction-policies" class="nav-link" data-scroll-target="#interaction-policies"><span class="header-section-number">2.1</span> Interaction Policies</a></li>
  <li><a href="#collaborative-task-design" id="toc-collaborative-task-design" class="nav-link" data-scroll-target="#collaborative-task-design"><span class="header-section-number">2.2</span> Collaborative Task Design</a>
  <ul class="collapse">
  <li><a href="#task-1-robot-dependent-collaborative-reasoning" id="toc-task-1-robot-dependent-collaborative-reasoning" class="nav-link" data-scroll-target="#task-1-robot-dependent-collaborative-reasoning"><span class="header-section-number">2.2.1</span> Task 1: Robot-Dependent Collaborative Reasoning</a></li>
  <li><a href="#task-2-open-ended-collaborative-problem-solving" id="toc-task-2-open-ended-collaborative-problem-solving" class="nav-link" data-scroll-target="#task-2-open-ended-collaborative-problem-solving"><span class="header-section-number">2.2.2</span> Task 2: Open-Ended Collaborative Problem Solving</a></li>
  </ul></li>
  <li><a href="#study-protocol" id="toc-study-protocol" class="nav-link" data-scroll-target="#study-protocol"><span class="header-section-number">2.3</span> Study Protocol</a></li>
  <li><a href="#measures" id="toc-measures" class="nav-link" data-scroll-target="#measures"><span class="header-section-number">2.4</span> Measures</a>
  <ul class="collapse">
  <li><a href="#self-report-measures" id="toc-self-report-measures" class="nav-link" data-scroll-target="#self-report-measures"><span class="header-section-number">2.4.1</span> Self-Report Measures</a></li>
  <li><a href="#objective-measures" id="toc-objective-measures" class="nav-link" data-scroll-target="#objective-measures"><span class="header-section-number">2.4.2</span> Objective Measures</a></li>
  </ul></li>
  <li><a href="#participants" id="toc-participants" class="nav-link" data-scroll-target="#participants"><span class="header-section-number">2.5</span> Participants</a>
  <ul class="collapse">
  <li><a href="#randomization-check" id="toc-randomization-check" class="nav-link" data-scroll-target="#randomization-check"><span class="header-section-number">2.5.1</span> Randomization Check</a></li>
  </ul></li>
  <li><a href="#analytic-strategy" id="toc-analytic-strategy" class="nav-link" data-scroll-target="#analytic-strategy"><span class="header-section-number">2.6</span> Analytic Strategy</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">3</span> Results</a>
  <ul>
  <li><a href="#primary-analysis-eligible-sample" id="toc-primary-analysis-eligible-sample" class="nav-link" data-scroll-target="#primary-analysis-eligible-sample"><span class="header-section-number">3.1</span> Primary Analysis: Eligible Sample</a></li>
  <li><a href="#hierarchical-models" id="toc-hierarchical-models" class="nav-link" data-scroll-target="#hierarchical-models"><span class="header-section-number">3.2</span> Hierarchical Models</a>
  <ul class="collapse">
  <li><a href="#primary-analysis-eligible-sample-n-24" id="toc-primary-analysis-eligible-sample-n-24" class="nav-link" data-scroll-target="#primary-analysis-eligible-sample-n-24"><span class="header-section-number">3.2.1</span> Primary Analysis: Eligible Sample (n = 24)</a></li>
  <li><a href="#sensitivity-analysis-full-sample-n-29" id="toc-sensitivity-analysis-full-sample-n-29" class="nav-link" data-scroll-target="#sensitivity-analysis-full-sample-n-29"><span class="header-section-number">3.2.2</span> Sensitivity Analysis: Full Sample (n = 29)</a></li>
  <li><a href="#mechanism-analysis-communication-breakdown-n29" id="toc-mechanism-analysis-communication-breakdown-n29" class="nav-link" data-scroll-target="#mechanism-analysis-communication-breakdown-n29"><span class="header-section-number">3.2.3</span> Mechanism Analysis: Communication Breakdown (n=29)</a></li>
  </ul></li>
  <li><a href="#task-performance" id="toc-task-performance" class="nav-link" data-scroll-target="#task-performance"><span class="header-section-number">3.3</span> Task performance</a></li>
  <li><a href="#individual-differences-and-correlational-patterns" id="toc-individual-differences-and-correlational-patterns" class="nav-link" data-scroll-target="#individual-differences-and-correlational-patterns"><span class="header-section-number">3.4</span> Individual differences and correlational patterns</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="header-section-number">4</span> Discussion</a></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations"><span class="header-section-number">5</span> Limitations</a></li>
  <li><a href="#conclusion-and-future-work" id="toc-conclusion-and-future-work" class="nav-link" data-scroll-target="#conclusion-and-future-work"><span class="header-section-number">6</span> Conclusion and Future Work</a></li>
  
  
  
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="misty-paper.pdf"><i class="bi bi-file-pdf"></i>Typst (ieee)</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Trust in Autonomous Human–Robot Collaboration: Effects of Responsive Interaction Policies</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">M.C. Lau <a href="mailto:mclau@laurentian.ca" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Shauna Heron <a href="mailto:sheron@laurentian.ca" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>Trust plays a central role in human–robot collaboration, yet its formation is rarely studied under the constraints of fully autonomous interaction. This pilot study examines how interaction policy influences trust during in-person collaboration with a social robot operating without Wizard-of-Oz control or scripted recovery. Participants completed a multi-stage collaborative task with a mobile robot that autonomously managed spoken-language dialogue, affect inference, and task progression. Two interaction policies were compared: a responsive policy, in which the robot proactively adapted its dialogue, assistance, and repair strategies based on user input and inferred interaction state, and a neutral, reactive policy, in which the robot provided only direct, task-relevant responses when explicitly prompted. Responsive interaction was associated with significantly higher post-interaction trust, despite no reliable differences in overall task accuracy. Sensitivity analyses revealed that experienced trust was highly sensitive to communication breakdown, whereas evaluative trust judgments were more robust. When language-mediated interaction collapsed, trust was no longer meaningfully instantiated. These findings suggest that trust formation in human–robot interaction differs under fully autonomous conditions, where communication failures cannot be repaired by human intervention.</p>
  </div>
</div>


</header>


<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>As artificial intelligence (AI) technologies advance, they are increasingly integrated into robotic systems, enabling more adaptive, autonomous, and context-sensitive behaviour in real-world environments. This convergence has accelerated the deployment of robots across safety-critical domains such as manufacturing <span class="citation" data-cites="choi2025"><a href="#ref-choi2025" role="doc-biblioref">[1]</a></span>, mining <span class="citation" data-cites="fu2021"><a href="#ref-fu2021" role="doc-biblioref">[2]</a></span>, and healthcare <span class="citation" data-cites="ciuffreda2025"><a href="#ref-ciuffreda2025" role="doc-biblioref">[3]</a></span>, where robots are now expected to operate alongside humans rather than in isolation <span class="citation" data-cites="diab2025 spitale2023"><a href="#ref-diab2025" role="doc-biblioref">[4]</a>, <a href="#ref-spitale2023" role="doc-biblioref">[5]</a></span>. In these collaborative settings, successful deployment depends not only on technical performance and safety guarantees, but also on whether human users are willing to rely on, communicate with, and coordinate their actions around robotic partners <span class="citation" data-cites="campagna2025 emaminejad2022"><a href="#ref-campagna2025" role="doc-biblioref">[6]</a>, <a href="#ref-emaminejad2022" role="doc-biblioref">[7]</a></span>. Trust has therefore emerged as a central determinant of adoption and effective use in human–robot collaboration (HRC) <span class="citation" data-cites="wischnewski2023 campagna2025"><a href="#ref-campagna2025" role="doc-biblioref">[6]</a>, <a href="#ref-wischnewski2023" role="doc-biblioref">[8]</a></span>. Insufficient trust can lead to disuse or rejection of automation, while excessive trust risks overreliance and accidents—particularly in environments characterized by uncertainty or incomplete information <span class="citation" data-cites="devisser2020"><a href="#ref-devisser2020" role="doc-biblioref">[9]</a></span>.</p>
<p>In HRC, trust is commonly understood as a willingness to rely on an agent under conditions of uncertainty and risk <span class="citation" data-cites="muir1994 hancock2011"><a href="#ref-muir1994" role="doc-biblioref">[10]</a>, <a href="#ref-hancock2011" role="doc-biblioref">[11]</a></span>. This reliance is dynamically calibrated, shaping how closely users monitor a robot, when they intervene, and whether they defer or override its actions <span class="citation" data-cites="devisser2020"><a href="#ref-devisser2020" role="doc-biblioref">[9]</a></span>. Appropriately calibrated trust supports effective coordination, whereas under-trust may result in disengagement or redundant oversight, and over-trust can lead to inappropriate reliance and unsafe outcomes. These dynamics are especially pronounced in dialogue-driven collaborative tasks, where misunderstandings, delays, or ambiguous responses may directly influence users’ ongoing assessments of a robot’s competence and reliability.</p>
<p>A substantial body of human–robot interaction (HRI) research has examined how robot behaviour influences user trust, perceived reliability, and cooperation across industrial and social contexts <span class="citation" data-cites="shayganfar2019 fartook2025"><a href="#ref-shayganfar2019" role="doc-biblioref">[12]</a>, <a href="#ref-fartook2025" role="doc-biblioref">[13]</a></span>. Trust is typically conceptualized as a multidimensional construct encompassing cognitive evaluations of competence, predictability, and reliability, alongside a behavioural willingness to collaborate toward shared goals under conditions of risk or uncertainty <span class="citation" data-cites="muir1994 hancock2011 devisser2020"><a href="#ref-devisser2020" role="doc-biblioref">[9]</a>, <a href="#ref-muir1994" role="doc-biblioref">[10]</a>, <a href="#ref-hancock2011" role="doc-biblioref">[11]</a></span>. Despite this multidimensional framing, empirical studies have most often operationalized trust using post-interaction self-report questionnaires collected following short, highly controlled, and often scripted interactions. While such measures provide valuable global assessments of user attitudes, they offer limited insight into how trust is negotiated, disrupted, and repaired during ongoing interaction—particularly in autonomous systems where errors and ambiguities are unavoidable <span class="citation" data-cites="maure"><a href="#ref-maure" role="doc-biblioref">[14]</a></span>.</p>
<p>Studying trust as an interactional process therefore requires experimental settings in which users engage with robots that exhibit both adaptive behaviour and realistic system limitations <span class="citation" data-cites="campagna2025"><a href="#ref-campagna2025" role="doc-biblioref">[6]</a></span>. In such settings, trust is shaped not only by task success but by how robots handle uncertainty, errors, and misalignment during interaction. Fully autonomous systems, where dialogue management and response generation occur without human intervention, provide a critical testbed for examining these dynamics, as they expose users to the same constraints and breakdowns encountered in real-world deployment <span class="citation" data-cites="campagna2025"><a href="#ref-campagna2025" role="doc-biblioref">[6]</a></span>.</p>
<p>In practice, much of the existing HRI trust literature has relied on scripted behaviours, simulated environments, or Wizard-of-Oz paradigms in which a human operator covertly manages the robot’s behaviour <span class="citation" data-cites="bettencourt2025 campagna2025"><a href="#ref-campagna2025" role="doc-biblioref">[6]</a>, <a href="#ref-bettencourt2025" role="doc-biblioref">[15]</a></span>. While these approaches are valuable for isolating specific design factors, they obscure the interaction breakdowns and system imperfections that characterize deployed autonomous robots. Limitations such as speech recognition errors, delayed responses, misinterpretations of user intent, and incomplete affect sensing are not peripheral issues but defining features of real-world interaction. These failures are likely to play a decisive role in shaping trust and collaboration, yet remain underrepresented in empirical evaluations <span class="citation" data-cites="campagna2025"><a href="#ref-campagna2025" role="doc-biblioref">[6]</a></span>.</p>
<p>Within HRI, a range of design strategies have been proposed to support appropriate trust calibration during collaboration, including robot appearance <span class="citation" data-cites="nicolas2021"><a href="#ref-nicolas2021" role="doc-biblioref">[16]</a></span>, transparency cues <span class="citation" data-cites="zhang2019"><a href="#ref-zhang2019" role="doc-biblioref">[17]</a></span>, explanations of system intent, adaptive feedback, and interaction pacing <span class="citation" data-cites="kok2020"><a href="#ref-kok2020" role="doc-biblioref">[18]</a></span>. Many of these approaches aim to help users form accurate expectations about a robot’s capabilities and limitations, particularly in contexts involving uncertainty or partial observability. Among these strategies, adaptive interaction behaviour—how and when a robot responds to user state and task context—has been identified as a particularly influential factor in shaping perceptions of competence, reliability, and collaboration <span class="citation" data-cites="fartook2025"><a href="#ref-fartook2025" role="doc-biblioref">[13]</a></span>.</p>
<p>Recent advances in AI have expanded the range of interaction strategies available to autonomous robotic platforms in practice, enabling systems to move beyond fixed, scripted behaviours toward adaptive interaction policies that respond to user state and task context in real time <span class="citation" data-cites="atone2022"><a href="#ref-atone2022" role="doc-biblioref">[19]</a></span>. Improvements in spoken-language processing, dialogue state tracking, and large language model–based reasoning now allow robots to adjust not only what they say, but when and how assistance is provided during collaboration <span class="citation" data-cites="wei"><a href="#ref-wei" role="doc-biblioref">[20]</a></span>. In parallel, advances in affect inference from language and interaction cues have made it increasingly feasible for robots to incorporate estimates of user emotional state into interaction management <span class="citation" data-cites="mcduff spitale2023"><a href="#ref-spitale2023" role="doc-biblioref">[5]</a>, <a href="#ref-mcduff" role="doc-biblioref">[21]</a></span>. As a result, responsiveness in contemporary HRI is increasingly understood as a property of an underlying interaction policy, governing how a robot interprets cues, initiates support, and manages uncertainty, rather than as a surface-level social behaviour <span class="citation" data-cites="birnbaum2016 shayganfar2019 fartook2025"><a href="#ref-shayganfar2019" role="doc-biblioref">[12]</a>, <a href="#ref-fartook2025" role="doc-biblioref">[13]</a>, <a href="#ref-birnbaum2016" role="doc-biblioref">[22]</a></span>.</p>
<p>From an engineering perspective, responsiveness is defined by an interaction policy rather than a superficial social cue <span class="citation" data-cites="arkin2003"><a href="#ref-arkin2003" role="doc-biblioref">[23]</a></span>. Proactive assistance based on interaction context—including inferred task progress, uncertainty, and human affective state—differs fundamentally from reactive, request-based behaviour. For example, a responsive robot may offer clarification or encouragement when confusion, hesitation, or frustration is inferred rather than waiting for an explicit request for help <span class="citation" data-cites="birnbaum2016"><a href="#ref-birnbaum2016" role="doc-biblioref">[22]</a></span>. Implementing such policies requires robots to manage spoken-language dialogue, track interaction and user state over time, and coordinate verbal and nonverbal responses in real time, all while operating under noise, latency, and sensing uncertainty <span class="citation" data-cites="campagna2025"><a href="#ref-campagna2025" role="doc-biblioref">[6]</a></span>.</p>
<p>The present work addresses these gaps through a pilot study examining trust and collaboration during in-person interaction with a fully autonomous social robot. Participants collaborated with one of two versions of the same robot platform during a dialogue-driven puzzle task requiring shared problem solving. In both conditions, all interaction management, including speech recognition, dialogue state tracking, task progression, and response generation, was handled and logged autonomously by a centralized dialogue agent without human intervention. In the responsive condition, the robot employed a proactive, affect-aware interaction policy, adapting its assistance based on conversational cues and inferred user affect (e.g., frustration or engagement). In the neutral condition, the robot followed a reactive policy, providing basic guidance and otherwise assistance only when explicitly requested.</p>
<p>This pilot study had three primary objectives: (1) to design and evaluate the feasibility of an autonomous spoken-language interaction system with affect-responsive behaviour on a mobile robot platform; (2) to assess whether a responsive interaction policy influences post-interaction trust and collaborative experience under realistic autonomous conditions; and (3) to explore how behavioural and interaction-level indicators align with subjective trust evaluations. Rather than optimizing for flawless interaction, the system was intentionally designed to reflect the capabilities and limitations of contemporary social robots, allowing interaction breakdowns to surface naturally.</p>
<p>By combining post-interaction trust measures with task-level and behavioural observations, this study aims to contribute empirical evidence on how trust in human–robot collaboration emerges and is enacted during fully autonomous interaction. The findings are intended to inform the design of a larger subsequent study by evaluating feasibility and identifying technical, interactional, and methodological challenges that must be addressed when evaluating affect-responsive robots in real-world contexts.</p>
</section>
<section id="methods" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="methods"><span class="header-section-number">2</span> Methods</h2>
<p>This study employed a between-subjects experimental design to examine how robot interaction policy influences trust and collaboration during fully autonomous, in-person human–robot interaction. The sole experimental factor was the robot’s interaction policy, with participants randomly assigned to interact with either a responsive or neutral version of the same robot system.</p>
<p>Throughout this paper, references to “the robot” denote a fully autonomous interactive system comprising the Misty-II hardware platform and an integrated offboard software pipeline. Spoken-language understanding, dialogue management, task logic, and interaction policy execution were handled on an external edge device which interfaced with the robot via application programming interfaces. The Misty-II platform was responsible for audio capture, speech synthesis, and the execution of embodied behaviours including facial expressions, body movement, and LED signalling. Despite this distributed execution, all interaction decisions were generated autonomously by a centralized dialogue agent responsible for coordinating spoken-language understanding, task state, and verbal and nonverbal behaviour based on the interaction policy, without human intervention. More detail on the robot platform and software architecture is provided in Appendix A.</p>
<section id="interaction-policies" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="interaction-policies"><span class="header-section-number">2.1</span> Interaction Policies</h3>
<p>Across both tasks, interaction behaviour was governed by one of two interaction policies that differed in how the robot was intended to initiate, framed, and adapt its contributions during collaboration. Under both policies, the robot continuously monitored interaction timing and issued brief check-ins following extended periods of participant silence in order to preserve interaction continuity. In the neutral, reactive condition, these check-ins were minimal and task-focused, serving only to signal availability (e.g., indicating readiness to answer questions) without providing guidance, encouragement, or additional framing (e.g., “I’m ready for your next question.”).</p>
<p>In contrast, under the responsive policy, the robot’s utterances were affectively framed and context-sensitive. In addition to answering questions, the robot proactively adapted its dialogue, assistance, and repair strategies based on inferred interaction state, such as hesitation, frustration, or apparent difficulty. This included acknowledging task difficulty, offering encouragement, and proposing collaborative reasoning (e.g., “I can tell you’re frustrated, don’t worry! we can reason through this together”), rather than waiting for an explicit request for help. Beyond differences in check-in style, the responsive robot also initiated guidance or clarification when interaction stagnated, whereas the neutral robot limited its contributions to basic task guidance and direct queries.</p>
<p>Both conditions operated fully autonomously, without human intervention or scripted recovery, and used the same underlying task logic and sensing infrastructure.</p>
</section>
<section id="collaborative-task-design" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="collaborative-task-design"><span class="header-section-number">2.2</span> Collaborative Task Design</h3>
<p>The task structure was modelled after <span class="citation" data-cites="lin2022b"><a href="#ref-lin2022b" role="doc-biblioref">[24]</a></span> and designed to elicit collaboration under two distinct dependency conditions: enforced collaboration, in which successful task completion required the robot’s involvement, and optional collaboration, in which participants could choose whether and how to engage the robot. To operationalize these conditions, participants completed an immersive, narrative-driven puzzle game consisting of five sequential stages and two timed reasoning tasks. The game positioned participants as investigators searching for a missing robot colleague, with the Misty-II robot serving as a diegetic guide and collaborative partner throughout the interaction. The full session lasted approximately 25 minutes.</p>
<p>Interactions took place in a shared physical workspace that included the Misty-II social robot and a participant-facing computer interface <span class="citation" data-cites="mistyrobotics"><a href="#ref-mistyrobotics" role="doc-biblioref">[25]</a></span>. The interface was used to display task materials, collect participant responses, and support progression through the game (see <a href="#fig-task1" class="quarto-xref">Figure&nbsp;2</a>). Importantly, the interface did not function as a control mechanism for the robot. Instead, the robot autonomously monitored task progression and participant inputs via the interface and adapted its dialogue and behaviour accordingly.</p>
<div id="fig-setup" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-setup-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/misty-pullback.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="482">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-setup-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Experimental setup showing the autonomous robot and participant-facing task interface used during in-person sessions. Participants entered task responses and navigated between task stages using the interface, while the robot autonomously tracked task state and adapted its interaction based on participant input.
</figcaption>
</figure>
</div>
<p>The interaction began with a brief greeting phase, during which the robot introduced itself and engaged in rapport-building dialogue. This was followed by a mission briefing in which the robot explained the narrative context and overall objectives. Participants then completed two core task stages: a robot-dependent collaborative reasoning task, in which the robot’s participation was required to solve the problem, and a more open-ended problem-solving task in which robot assistance was optional. The interaction concluded with a wrap-up stage in which the robot provided closing feedback and formally ended the session.</p>
<p>Participants advanced between stages using the interface, either in response to the robot’s prompts or at their own discretion. All spoken dialogue and interaction events were managed autonomously by the robot and logged automatically for analysis.</p>
<section id="task-1-robot-dependent-collaborative-reasoning" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="task-1-robot-dependent-collaborative-reasoning"><span class="header-section-number">2.2.1</span> Task 1: Robot-Dependent Collaborative Reasoning</h4>
<p>In the first task, participants were asked to identify a perpetrator from a 6 × 4 grid of 24 ‘suspects’ by asking the robot a series of yes/no questions about the suspect’s features (e.g., “was the suspect wearing a hat?”). The grid was displayed on the interface, while questions were posed verbally.</p>
<div id="fig-task1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-task1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/task1-whodunnit2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="482">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-task1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <em>Task 1 interface including the 6 × 4 grid of 24 candidates. Participants could track those eliminated by clicking on subjects which would grey them out. A box was provided to input their final answer and a button included to move to the next task.</em>
</figcaption>
</figure>
</div>
<p>The robot possessed the ground-truth information necessary to answer each question correctly. Successful task completion was therefore dependent on interaction with the robot, creating an enforced collaborative dynamic. Participants were required to coordinate a questioning strategy that progressively eliminated candidates based on shared features, narrowing the search space within a five-minute time limit. Efficient performance depended on selecting informative questions (e.g., features that divided the remaining candidates), tracking eliminations, and adapting subsequent questions based on prior answers.</p>
<p>This structure made the task sensitive to interaction quality. Inefficient questioning, repeated queries, or uncertainty about next steps could slow progress and increase cognitive load, whereas effective coordination supported rapid elimination and convergence on a solution. The structured nature of the task ensured consistent interaction demands across participants and conditions, while still allowing meaningful variation in collaboration style.</p>
</section>
<section id="task-2-open-ended-collaborative-problem-solving" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="task-2-open-ended-collaborative-problem-solving"><span class="header-section-number">2.2.2</span> Task 2: Open-Ended Collaborative Problem Solving</h4>
<p>The second task involved a more open-ended reasoning challenge. Participants were presented with multiple technical logs through a simulated terminal interface that could be used to infer the location of the missing robot (see <a href="#fig-task2" class="quarto-xref">Figure&nbsp;3</a>). The task was intentionally cryptic and difficult to solve within the allotted ten minutes without synthesizing information across multiple sources.</p>
<div id="fig-task2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-task2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/task2-cryptic-puzzle.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="482">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-task2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: The task 2 interface presented multiple technical logs through a simulated terminal interface that could be used to determine the location of the missing robot.
</figcaption>
</figure>
</div>
<p>The logs contained partial and indirect clues related to the robot’s activity, such as wireless connectivity records, sensor readings, and timestamped system events. Solving the task required participants to identify which logs were relevant, extract spatial or temporal cues, and integrate these signals to progressively narrow down plausible locations. As in Task 1, successful performance depended on managing uncertainty and iteratively refining hypotheses rather than on recognizing a single explicit cue.</p>
<p>Unlike Task 1, the robot did not have access to ground-truth information or the contents of the logs. Its assistance was therefore limited to general reasoning support derived from its language model, such as explaining how to interpret log formats, suggesting strategies for cross-referencing timestamps, or prompting participants to reconcile inconsistencies across sources. Participants could complete the task independently or solicit assistance from the robot at their discretion <span class="citation" data-cites="lin2022"><a href="#ref-lin2022" role="doc-biblioref">[26]</a></span>, allowing collaboration to emerge voluntarily rather than being enforced by task structure.</p>
</section>
</section>
<section id="study-protocol" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="study-protocol"><span class="header-section-number">2.3</span> Study Protocol</h3>
<p>Participants signed up for the study and completed a pre-session questionnaire before their in-person session via Qualtrics. The pre-session questionnaire colleced basic demographics information and assessed baseline characteristics, including the Negative Attitudes Toward Robots Scale (NARS) and the short form of the Need for Cognition scale (NFC-s) <span class="citation" data-cites="nomura cacioppo1982"><a href="#ref-nomura" role="doc-biblioref">[27]</a>, <a href="#ref-cacioppo1982" role="doc-biblioref">[28]</a></span>. These measures were used to capture individual differences that may moderate responses to robot interaction.</p>
<p>In-person sessions were conducted in a quiet, private room at Laurentian University between November and December 2025. Prior to each session, the robot’s interaction policy was configured to the assigned experimental condition.</p>
<p>Upon arrival, participants were greeted by the researcher, provided with a brief overview of the session, and given instructions for effective communication with the robot, including waiting for a visual indicator before speaking. Once participants indicated readiness, the researcher exited the room, leaving the participant and robot to complete the interaction without human presence or observation. Participants initiated the interaction by clicking a start button on the interface and were informed that they could terminate the session at any time without penalty.</p>
<p>Following task completion, participants completed a 21-item post-interaction questionnaire assessing trust. Participants then engaged in a brief debrief with the researcher and were awarded a $15 gift card. Total session duration averaged approximately 30 minutes.</p>
</section>
<section id="measures" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="measures"><span class="header-section-number">2.4</span> Measures</h3>
<p>A combination of self-report and objective measures was used to assess trust, engagement, and task performance.</p>
<section id="self-report-measures" class="level4" data-number="2.4.1">
<h4 data-number="2.4.1" class="anchored" data-anchor-id="self-report-measures"><span class="header-section-number">2.4.1</span> Self-Report Measures</h4>
<p>Participants completed a pre-session questionnaire assessing baseline characteristics, including the Negative Attitudes Toward Robots Scale (NARS) and the short form of the Need for Cognition scale (NFC-s) <span class="citation" data-cites="cacioppo1982 nomura"><a href="#ref-nomura" role="doc-biblioref">[27]</a>, <a href="#ref-cacioppo1982" role="doc-biblioref">[28]</a></span>. These measures were used to capture individual differences that may moderate responses to robot interaction.</p>
<p>Trust was assessed using two established self-report instruments commonly used in human–robot interaction research: the Trust Perception Scale–HRI (TPS-HRI) and the Trust in Industrial Human–Robot Collaboration scale (TI-HRC) <span class="citation" data-cites="schaefer2016 charalambous2016"><a href="#ref-schaefer2016" role="doc-biblioref">[29]</a>, <a href="#ref-charalambous2016" role="doc-biblioref">[30]</a></span>. Both measures were adapted to reflect the specific dialogue-driven task context and interaction modality of the present study, while preserving the original constructs and response intent of each scale. 9 items were retained from the TI-HRC and 12 items from the TPS-HRI. Item wording was modified to reference the robot’s behaviour during the dialogue-driven collaborative tasks, and response formats were adjusted to ensure interpretability for participants without prior robotics experience (see Appendix B for a full item list).</p>
<p>Together, these instruments capture complementary dimensions of trust, including perceived reliability, task competence, and affective comfort. However, they differ in their conceptual emphasis: the TPS-HRI primarily operationalizes trust as a reflective judgement of system performance (i.e., “What percent of the time was the robot reliable”), whereas the TI-HRC scale emphasizes trust as an experienced, embodied response arising during interaction (i.e., “The way the robot moved made me feel uneasy”). Despite this complementarity, both measures rely on retrospective self-report and may be insensitive to moment-to-moment trust dynamics as collaboration unfolds. For this reason, questionnaire data were interpreted alongside behavioural and interaction-level measures.</p>
</section>
<section id="objective-measures" class="level4" data-number="2.4.2">
<h4 data-number="2.4.2" class="anchored" data-anchor-id="objective-measures"><span class="header-section-number">2.4.2</span> Objective Measures</h4>
<p>Objective task metrics included task completion, task accuracy, time to completion, and the number of assistance requests made to the robot. behavioural engagement metrics were derived from interaction logs and manually coded dialogue transcripts, including number of dialogue turns, frequency of communication breakdowns, response timing, and task-relevant robot contributions.</p>
</section>
</section>
<section id="participants" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="participants"><span class="header-section-number">2.5</span> Participants</h3>
<p>A total of 29 participants were recruited from the Laurentian University community via word of mouth and the SONA recruitment system. Eligibility criteria required being 18 years or older, fluent in spoken and written English, and having normal or corrected-to-normal hearing and vision. Participants received a $15 gift card as compensation for their time; some students additionally received partial credit for participating depending on their program of study. All procedures were approved by the Laurentian University Research Ethics Board (REB #6021966).</p>
<p>Although English fluency was an eligibility requirement, in-person observation during data collection revealed meaningful variability in participants’ functional spoken-language proficiency. The researcher therefore documented observed language proficiency and monitored interaction quality during each session in anticipation of potential speech-based system limitations.</p>
<p>Subsequent review of interaction transcripts and system logs indicated that a subset of sessions exhibited severe and sustained communication failure, characterized by fragmented or unintelligible automatic speech recognition (ASR) output and stalled dialogue. In these sessions, the robot was unable to extract sufficient linguistic content to maintain conversation, respond meaningfully to participant input, or support task progression. These interaction failures are described in detail in the Analytic Strategy and Results sections.</p>
<section id="randomization-check" class="level4" data-number="2.5.1">
<h4 data-number="2.5.1" class="anchored" data-anchor-id="randomization-check"><span class="header-section-number">2.5.1</span> Randomization Check</h4>
<p>Across analyses, participants in the responsive and control conditions were comparable with respect to demographic characteristics, prior experience with robots, and baseline attitudes toward robots, including Negative Attitudes Toward Robots (NARS) and Need for Cognition scores (see <a href="#tbl-pre" class="quarto-xref">Table&nbsp;1</a>) <span class="citation" data-cites="cacioppo1982"><a href="#ref-cacioppo1982" role="doc-biblioref">[28]</a></span>. These patterns were consistent across both eligible and full samples, indicating successful random assignment.</p>
<div class="cell">
<div id="tbl-pre" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-pre-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Participant Demographics and Baseline Characteristics by Group
</figcaption>
<div aria-describedby="tbl-pre-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="obmphfmoaj" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#obmphfmoaj table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#obmphfmoaj thead, #obmphfmoaj tbody, #obmphfmoaj tfoot, #obmphfmoaj tr, #obmphfmoaj td, #obmphfmoaj th {
  border-style: none;
}

#obmphfmoaj p {
  margin: 0;
  padding: 0;
}

#obmphfmoaj .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 13px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#obmphfmoaj .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#obmphfmoaj .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#obmphfmoaj .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#obmphfmoaj .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#obmphfmoaj .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#obmphfmoaj .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#obmphfmoaj .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#obmphfmoaj .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#obmphfmoaj .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#obmphfmoaj .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#obmphfmoaj .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#obmphfmoaj .gt_spanner_row {
  border-bottom-style: hidden;
}

#obmphfmoaj .gt_group_heading {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#obmphfmoaj .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#obmphfmoaj .gt_from_md > :first-child {
  margin-top: 0;
}

#obmphfmoaj .gt_from_md > :last-child {
  margin-bottom: 0;
}

#obmphfmoaj .gt_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#obmphfmoaj .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#obmphfmoaj .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#obmphfmoaj .gt_row_group_first td {
  border-top-width: 2px;
}

#obmphfmoaj .gt_row_group_first th {
  border-top-width: 2px;
}

#obmphfmoaj .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#obmphfmoaj .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#obmphfmoaj .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#obmphfmoaj .gt_last_summary_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#obmphfmoaj .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#obmphfmoaj .gt_first_grand_summary_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#obmphfmoaj .gt_last_grand_summary_row_top {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#obmphfmoaj .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#obmphfmoaj .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#obmphfmoaj .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#obmphfmoaj .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#obmphfmoaj .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#obmphfmoaj .gt_sourcenote {
  font-size: 90%;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#obmphfmoaj .gt_left {
  text-align: left;
}

#obmphfmoaj .gt_center {
  text-align: center;
}

#obmphfmoaj .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#obmphfmoaj .gt_font_normal {
  font-weight: normal;
}

#obmphfmoaj .gt_font_bold {
  font-weight: bold;
}

#obmphfmoaj .gt_font_italic {
  font-style: italic;
}

#obmphfmoaj .gt_super {
  font-size: 65%;
}

#obmphfmoaj .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#obmphfmoaj .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#obmphfmoaj .gt_indent_1 {
  text-indent: 5px;
}

#obmphfmoaj .gt_indent_2 {
  text-indent: 10px;
}

#obmphfmoaj .gt_indent_3 {
  text-indent: 15px;
}

#obmphfmoaj .gt_indent_4 {
  text-indent: 20px;
}

#obmphfmoaj .gt_indent_5 {
  text-indent: 25px;
}

#obmphfmoaj .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#obmphfmoaj div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>

<table class="gt_table do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-bootstrap="false">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="gt_col_headings header">
<th id="label" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col"><strong>Characteristic</strong></th>
<th id="n" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>N</strong></th>
<th id="stat_1" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>CONTROL</strong><br>
N = 13<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span></th>
<th id="stat_2" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>RESPONSIVE</strong><br>
N = 16<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span></th>
<th id="p.value" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>p-value</strong><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>2</sup></span></th>
</tr>
</thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_left" headers="label" style="font-weight: bold">Gender</td>
<td class="gt_row gt_center" headers="n">27</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value">0.84</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Woman</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">6 / 13 (46%)</td>
<td class="gt_row gt_center" headers="stat_2">7 / 14 (50%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Man</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">7 / 13 (54%)</td>
<td class="gt_row gt_center" headers="stat_2">7 / 14 (50%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label" style="font-weight: bold">Age Group</td>
<td class="gt_row gt_center" headers="n">27</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value">0.35</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;18-24</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">5 / 13 (38%)</td>
<td class="gt_row gt_center" headers="stat_2">7 / 14 (50%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;25-34</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">4 / 13 (31%)</td>
<td class="gt_row gt_center" headers="stat_2">2 / 14 (14%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;34-44</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">1 / 13 (7.7%)</td>
<td class="gt_row gt_center" headers="stat_2">4 / 14 (29%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;45+</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">3 / 13 (23%)</td>
<td class="gt_row gt_center" headers="stat_2">1 / 14 (7.1%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label" style="font-weight: bold">Program</td>
<td class="gt_row gt_center" headers="n">25</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value">&gt;0.99</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Psychology</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">1 / 13 (7.7%)</td>
<td class="gt_row gt_center" headers="stat_2">1 / 12 (8.3%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Engineering</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">2 / 13 (15%)</td>
<td class="gt_row gt_center" headers="stat_2">1 / 12 (8.3%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Computer Science</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">7 / 13 (54%)</td>
<td class="gt_row gt_center" headers="stat_2">6 / 12 (50%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Earth Sciences</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">0 / 13 (0%)</td>
<td class="gt_row gt_center" headers="stat_2">1 / 12 (8.3%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Other</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">3 / 13 (23%)</td>
<td class="gt_row gt_center" headers="stat_2">3 / 12 (25%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label" style="font-weight: bold">Experience with Robots</td>
<td class="gt_row gt_center" headers="n">29</td>
<td class="gt_row gt_center" headers="stat_1">7 / 13 (54%)</td>
<td class="gt_row gt_center" headers="stat_2">4 / 16 (25%)</td>
<td class="gt_row gt_center" headers="p.value">0.14</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label" style="font-weight: bold">Native English Speaker</td>
<td class="gt_row gt_center" headers="n">29</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value">0.53</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Native English</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">5 / 13 (38%)</td>
<td class="gt_row gt_center" headers="stat_2">8 / 16 (50%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Non-Native English</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">8 / 13 (62%)</td>
<td class="gt_row gt_center" headers="stat_2">8 / 16 (50%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label" style="font-weight: bold">NARS Overall</td>
<td class="gt_row gt_center" headers="n">29</td>
<td class="gt_row gt_center" headers="stat_1">38 (8)</td>
<td class="gt_row gt_center" headers="stat_2">38 (7)</td>
<td class="gt_row gt_center" headers="p.value">0.79</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label" style="font-weight: bold">Need for Cognition</td>
<td class="gt_row gt_center" headers="n">29</td>
<td class="gt_row gt_center" headers="stat_1">3.62 (0.78)</td>
<td class="gt_row gt_center" headers="stat_2">3.74 (0.74)</td>
<td class="gt_row gt_center" headers="p.value">0.55</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label" style="font-weight: bold">Dialogue Viability</td>
<td class="gt_row gt_center" headers="n">29</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value">0.63</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;exclude</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">3 / 13 (23%)</td>
<td class="gt_row gt_center" headers="stat_2">2 / 16 (13%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;include</td>
<td class="gt_row gt_center" headers="n"><br>
</td>
<td class="gt_row gt_center" headers="stat_1">10 / 13 (77%)</td>
<td class="gt_row gt_center" headers="stat_2">14 / 16 (88%)</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
</tbody><tfoot>
<tr class="gt_footnotes odd">
<td colspan="5" class="gt_footnote"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span> n / N (%); Mean (SD)</td>
</tr>
<tr class="gt_footnotes even">
<td colspan="5" class="gt_footnote"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>2</sup></span> Pearson’s Chi-squared test; Fisher’s exact test; Wilcoxon rank sum test</td>
</tr>
</tfoot>

</table>

</div>
</div>
</div>
</figure>
</div>
</div>
</section>
</section>
<section id="analytic-strategy" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="analytic-strategy"><span class="header-section-number">2.6</span> Analytic Strategy</h3>
<p>Because the study relied fundamentally on spoken-language collaboration, analyses were structured to explicitly account for interaction viability. Sessions were classified as non-viable when severe communication breakdown prevented sustained dialogue between the participant and the robot, rendering the experimental manipulation inoperative.</p>
<p>Communication viability was operationalized using a dialogue-level metric derived from manual coding of system logs. For each session, the proportion of dialogue turns affected by speech-recognition failure (i.e., fragmented or unintelligible utterances) was computed. Sessions in which more than 60% of dialogue turns were affected were classified as non-viable, reflecting cases in which spoken-language interaction could not be meaningfully sustained. This criterion closely aligned with sessions independently flagged by the researcher during administration.</p>
<p>Analyses were conducted using three complementary approaches. Primary analyses were performed on an eligible sample excluding non-viable sessions, reflecting interactions in which the spoken-language protocol and experimental manipulation operated as intended. Full-sample analyses including all sessions were conducted as sensitivity checks. Finally, mechanism-focused analyses compared viable and non-viable sessions on interaction-process metrics (e.g., ASR failure rates, dialogue turn completion, task abandonment) to characterize how severe communication breakdown alters interaction dynamics. Trust measures from non-viable sessions were not interpreted as valid estimates of human–robot trust under functional interaction, as the robot was unable to sustain dialogue or collaborative behavior in these cases.</p>
<p>All analyses were conducted using R (version 4.5.1) within the Quarto framework. Data manipulation and visualization utilized the tidyverse suite of packages <span class="citation" data-cites="wickham2019"><a href="#ref-wickham2019" role="doc-biblioref">[31]</a></span>, with mixed-effects models fitted using the lme4 and lmerTest packages <span class="citation" data-cites="bates2015 kuznetsova2017"><a href="#ref-bates2015" role="doc-biblioref">[32]</a>, <a href="#ref-kuznetsova2017" role="doc-biblioref">[33]</a></span> while Bayesian hierarchical models were fitted using the brms package <span class="citation" data-cites="burkner2018"><a href="#ref-burkner2018" role="doc-biblioref">[34]</a></span>. Summary tables were generated using the gtsummary package <span class="citation" data-cites="sjoberg2021"><a href="#ref-sjoberg2021" role="doc-biblioref">[35]</a></span>. All code used for data processing and analysis is available at: <a href="">GitHub Repository</a></p>
</section>
</section>
<section id="results" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="results"><span class="header-section-number">3</span> Results</h2>
<p>Of the 29 completed sessions, 5 met the pre-specified criterion for non-viable interaction due to severe and persistent communication failure. These sessions were characterized by high rates of ASR failure, incomplete dialogue sequences, and skipped task stages.</p>
<p>Primary results are therefore reported for the eligible sample (n = 24), with full-sample and mechanism-focused analyses reported as sensitivity and exploratory analyses, respectively.</p>
<section id="primary-analysis-eligible-sample" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="primary-analysis-eligible-sample"><span class="header-section-number">3.1</span> Primary Analysis: Eligible Sample</h3>
<p>Simple descriptive comparisons of post-interaction trust measures indicated higher trust ratings in the responsive condition relative to the control condition across both trust scales (see <a href="#tbl-post-eligible" class="quarto-xref">Table&nbsp;2</a>). As indicated in <a href="#fig-post-eligible2" class="quarto-xref">Figure&nbsp;4</a>, average post-interaction scores on the Trust in Industrial Human–Robot Collaboration scale (TI-HRC) differed by approximately 26 points (Likert 1-5 converted to 0-100 scale for easier comparison across scales), while differences on the Trust Perception Scale–HRI (TPS-HRI) were approximately 15 points higher in the responsive condition compared to the control (<a href="#fig-post-eligible" class="quarto-xref">Figure&nbsp;5</a>).</p>
<p>Analysis of dialogue interaction patterns confirmed successful manipulation of the interaction policies. Manual coding of dialogue transcripts revealed substantial and significant differences in robot behaviour across conditions (see <a href="#tbl-post-eligible" class="quarto-xref">Table&nbsp;2</a>). In the responsive condition, the robot employed encouragement in 36% of dialogue turns compared to 0% in the control condition (p &lt; .001), expressed empathy or acknowledged participant affect in 13% of turns versus 0% in the control (p &lt; .001), and used collaborative language (e.g., “we,” “let’s”) in 42% of turns compared to 5% in control (p &lt; .001). Interestingly, while both conditions included proactive check-ins following periods of participant silence, the control robot engaged in such check-ins at a higher rate (21% vs.&nbsp;13% of turns, p = .033), consistent with its reactive policy that limited proactive assistance to structured silence monitoring. Critically, proportions of communication breakdown did not differ between conditions (25% vs.&nbsp;22%, p = .70), indicating that interaction policy did not systematically impact technical speech recognition viability among eligible participants. Participants in the responsive condition also exhibited higher levels of AI-detected engagement during interaction, with an average of 3.50 engaged responses (SD = 1.95) compared to 2.00 (SD = 2.21) in the control condition. Consistent with these dialogue differences, interactions in the responsive condition were characterized by longer session durations and slower robot response times, reflecting the additional dialogue and affective support behaviours. Together, these patterns confirm that the responsive robot implemented affect-adaptive, proactive support behaviours while maintaining comparable interaction viability to the neutral control condition.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-post-eligible2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post-eligible2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="misty-paper_files/figure-html/fig-post-eligible2-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post-eligible2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Distribution of Trust in Industrial Human–Robot Collaboration scores by interaction policy. Points represent individual observations; violins depict score distributions. Red points indicate group means with 95% confidence intervals. Statistical comparisons are reported in the Results section.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Importantly, overall task accuracy did not differ significantly between conditions (detailed task performance results are reported below), suggesting that observed differences in trust were not driven by differential task success but rather by the quality of the interaction process itself.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-post-eligible" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post-eligible-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="misty-paper_files/figure-html/fig-post-eligible-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post-eligible-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Distribution of Trust Perception in HRI by interaction policy. Points represent individual observations; violins depict score distributions. Red points indicate group means with 95% confidence intervals. Statistical comparisons are reported in the Results section.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div id="tbl-post-eligible" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-post-eligible-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Post-Interaction Raw Outcome Measures by Group
</figcaption>
<div aria-describedby="tbl-post-eligible-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="ovrocinzwb" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#ovrocinzwb table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#ovrocinzwb thead, #ovrocinzwb tbody, #ovrocinzwb tfoot, #ovrocinzwb tr, #ovrocinzwb td, #ovrocinzwb th {
  border-style: none;
}

#ovrocinzwb p {
  margin: 0;
  padding: 0;
}

#ovrocinzwb .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 13px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#ovrocinzwb .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#ovrocinzwb .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#ovrocinzwb .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#ovrocinzwb .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ovrocinzwb .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ovrocinzwb .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ovrocinzwb .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#ovrocinzwb .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#ovrocinzwb .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#ovrocinzwb .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#ovrocinzwb .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#ovrocinzwb .gt_spanner_row {
  border-bottom-style: hidden;
}

#ovrocinzwb .gt_group_heading {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#ovrocinzwb .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#ovrocinzwb .gt_from_md > :first-child {
  margin-top: 0;
}

#ovrocinzwb .gt_from_md > :last-child {
  margin-bottom: 0;
}

#ovrocinzwb .gt_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#ovrocinzwb .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#ovrocinzwb .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#ovrocinzwb .gt_row_group_first td {
  border-top-width: 2px;
}

#ovrocinzwb .gt_row_group_first th {
  border-top-width: 2px;
}

#ovrocinzwb .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#ovrocinzwb .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#ovrocinzwb .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#ovrocinzwb .gt_last_summary_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ovrocinzwb .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#ovrocinzwb .gt_first_grand_summary_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#ovrocinzwb .gt_last_grand_summary_row_top {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#ovrocinzwb .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#ovrocinzwb .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ovrocinzwb .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ovrocinzwb .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#ovrocinzwb .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ovrocinzwb .gt_sourcenote {
  font-size: 90%;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#ovrocinzwb .gt_left {
  text-align: left;
}

#ovrocinzwb .gt_center {
  text-align: center;
}

#ovrocinzwb .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#ovrocinzwb .gt_font_normal {
  font-weight: normal;
}

#ovrocinzwb .gt_font_bold {
  font-weight: bold;
}

#ovrocinzwb .gt_font_italic {
  font-style: italic;
}

#ovrocinzwb .gt_super {
  font-size: 65%;
}

#ovrocinzwb .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#ovrocinzwb .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#ovrocinzwb .gt_indent_1 {
  text-indent: 5px;
}

#ovrocinzwb .gt_indent_2 {
  text-indent: 10px;
}

#ovrocinzwb .gt_indent_3 {
  text-indent: 15px;
}

#ovrocinzwb .gt_indent_4 {
  text-indent: 20px;
}

#ovrocinzwb .gt_indent_5 {
  text-indent: 25px;
}

#ovrocinzwb .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#ovrocinzwb div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>

<table class="gt_table do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-bootstrap="false">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="gt_col_headings header">
<th id="label" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col"><strong>Characteristic</strong></th>
<th id="stat_1" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>CONTROL</strong><br>
N = 10<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span></th>
<th id="stat_2" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>RESPONSIVE</strong><br>
N = 14<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span></th>
<th id="p.value" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>p-value</strong><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>2</sup></span></th>
</tr>
</thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_left" headers="label">Trust in Industrial HRI Collaboration</td>
<td class="gt_row gt_center" headers="stat_1">39 (22)</td>
<td class="gt_row gt_center" headers="stat_2">67 (21)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.004</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">Subscales</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Reliability subscale</td>
<td class="gt_row gt_center" headers="stat_1">40 (24)</td>
<td class="gt_row gt_center" headers="stat_2">65 (18)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.012</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Trust Perception subscale</td>
<td class="gt_row gt_center" headers="stat_1">42 (23)</td>
<td class="gt_row gt_center" headers="stat_2">60 (22)</td>
<td class="gt_row gt_center" headers="p.value">0.075</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Affective Trust subscale</td>
<td class="gt_row gt_center" headers="stat_1">50 (31)</td>
<td class="gt_row gt_center" headers="stat_2">79 (22)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.018</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">Trust Perception Scale HRI</td>
<td class="gt_row gt_center" headers="stat_1">59 (17)</td>
<td class="gt_row gt_center" headers="stat_2">77 (18)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.022</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">Overall Task Accuracy</td>
<td class="gt_row gt_center" headers="stat_1">0.60 (0.21)</td>
<td class="gt_row gt_center" headers="stat_2">0.66 (0.23)</td>
<td class="gt_row gt_center" headers="p.value">0.47</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">Objective Measures</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Dialogue Turns</td>
<td class="gt_row gt_center" headers="stat_1">34 (9)</td>
<td class="gt_row gt_center" headers="stat_2">33 (5)</td>
<td class="gt_row gt_center" headers="p.value">0.45</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Avg Session Duration (mins)</td>
<td class="gt_row gt_center" headers="stat_1">13.24 (3.06)</td>
<td class="gt_row gt_center" headers="stat_2">15.26 (2.12)</td>
<td class="gt_row gt_center" headers="p.value">0.084</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Avg Robot Response Time (ms)</td>
<td class="gt_row gt_center" headers="stat_1">14.37 (3.76)</td>
<td class="gt_row gt_center" headers="stat_2">17.24 (2.52)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">&lt;0.001</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Silent Periods</td>
<td class="gt_row gt_center" headers="stat_1">5.60 (1.96)</td>
<td class="gt_row gt_center" headers="stat_2">4.71 (2.05)</td>
<td class="gt_row gt_center" headers="p.value">0.29</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Engaged Responses</td>
<td class="gt_row gt_center" headers="stat_1">2.00 (2.21)</td>
<td class="gt_row gt_center" headers="stat_2">3.50 (1.95)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.040</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Frustrated Responses</td>
<td class="gt_row gt_center" headers="stat_1">0.60 (0.70)</td>
<td class="gt_row gt_center" headers="stat_2">0.93 (1.21)</td>
<td class="gt_row gt_center" headers="p.value">0.68</td>
</tr>
</tbody><tfoot>
<tr class="gt_footnotes odd">
<td colspan="4" class="gt_footnote"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span> Mean (SD)</td>
</tr>
<tr class="gt_footnotes even">
<td colspan="4" class="gt_footnote"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>2</sup></span> Wilcoxon rank sum test; Wilcoxon rank sum exact test</td>
</tr>
</tfoot>

</table>

</div>
</div>
</div>
</figure>
</div>
</div>
<p>Beyond descriptive comparisons, hierarchical models were fitted to evaluate the robustness of interaction policy effects while controlling for baseline covariates and accounting for measurement structure.</p>
</section>
<section id="hierarchical-models" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="hierarchical-models"><span class="header-section-number">3.2</span> Hierarchical Models</h3>
<p>To evaluate the robustness of interaction policy effects while controlling for baseline covariates, mixed-effects models were fitted using both frequentist and Bayesian estimation approaches. Frequentist models provided hypothesis tests against null effects, while Bayesian models enabled direct quantification of uncertainty and facilitated evaluation across varying levels of communication viability. All models included interaction policy (responsive vs.&nbsp;control) as the primary fixed effect, with baseline negative attitudes toward robots (NARS) and native English fluency as covariates. Random intercepts for sessions and trust items accounted for repeated measurement structure. Model building proceeded by comparing a baseline model containing interaction policy alone against models incorporating theoretically motivated covariates. Adding NARS scores significantly improved model fit (χ² = 4.82, p = .028), whereas prior experience with robots did not. Native English fluency did not significantly improve model fit but was retained due to its relevance for spoken-language interaction viability. Bayesian models used weakly informative priors and showed satisfactory convergence across all analyses (R̂ ≤ 1.01; effective sample sizes &gt; 1000).</p>
<section id="primary-analysis-eligible-sample-n-24" class="level4" data-number="3.2.1">
<h4 data-number="3.2.1" class="anchored" data-anchor-id="primary-analysis-eligible-sample-n-24"><span class="header-section-number">3.2.1</span> Primary Analysis: Eligible Sample (n = 24)</h4>
<p>Frequentist linear mixed-effects models revealed consistent positive effects of responsive interaction on both trust measures. For experienced trust (TI-HRC), participants who interacted with the responsive robot reported significantly higher post-interaction trust than those in the control condition (β = 16.28, SE = 5.14, t = 3.17, p = .005). Higher baseline negative attitudes toward robots were associated with lower trust scores (β = −7.43, SE = 2.81, p = .016), while native English fluency was not significantly associated with trust. Inclusion of random intercepts for individual trust items significantly improved model fit, indicating meaningful item-level variability beyond session-level differences. For perceived trust (TPS-HRI), a comparable pattern emerged, with responsive interaction associated with higher trust scores (β = 14.17, SE = 6.50, t = 2.00, p = .046). However, random intercepts for trust items did not improve model fit for this scale, likely reflecting differences in scale format and response interface: TPS-HRI was administered using a continuous slider input via touchpad, whereas TI-HRC employed discrete Likert-style response options. The slider-based interface may have reduced response precision and item-level variability, though meaningful between-condition differences remained detectable at the aggregate level.</p>
<p>Bayesian estimation converged on similar effect magnitudes with high posterior certainty. For TI-HRC, the responsive condition showed a posterior median effect of β = 14.86 (95% credible interval [7.20, 22.09]), with near-unity probability of a positive effect. For TPS-HRI, the posterior median was β = 12.73 (95% credible interval [2.93, 22.17]), with posterior probability exceeding 99% that the effect was positive. Baseline NARS showed credible negative associations with both outcomes, while native English fluency showed negative associations that were credible for TPS-HRI but uncertain for TI-HRC. Model fit was substantial for TPS-HRI (conditional R² = 0.64) and moderate for TI-HRC (conditional R² = 0.42), with fixed effects explaining 16% and 21% of variance, respectively. The smaller item-level variance for TI-HRC suggests greater coherence among affective trust items under functional interaction conditions.</p>
</section>
<section id="sensitivity-analysis-full-sample-n-29" class="level4" data-number="3.2.2">
<h4 data-number="3.2.2" class="anchored" data-anchor-id="sensitivity-analysis-full-sample-n-29"><span class="header-section-number">3.2.2</span> Sensitivity Analysis: Full Sample (n = 29)</h4>
<p>To assess robustness, Bayesian models were refitted including all sessions regardless of communication viability. The responsive interaction effect remained positive for both trust measures but showed substantial attenuation compared to the eligible sample. For TPS-HRI, the posterior median effect was β = 7.04 (95% credible interval [−1.83, 15.67]). Although uncertainty increased and the credible interval included zero, the posterior probability of a positive effect remained high (&gt;94%). Model fit decreased relative to the eligible sample (conditional R² = 0.44), indicating increased unexplained variability when sessions with severe communication breakdown were included. For TI-HRC, attenuation was more pronounced. The posterior median effect decreased to β = 7.17 (95% credible interval [−1.97, 16.70]), with reduced probability of a large effect. Model fit remained moderate (conditional R² = 0.60), but residual variance increased, consistent with the inclusion of interactions in which collaborative behaviour could not be sustained. These results indicate that experienced trust is particularly sensitive to interaction breakdown, and that trust ratings obtained under non-functional interaction conditions do not reflect graded variation in collaborative experience but rather reflect the collapse of the interaction itself.</p>
</section>
<section id="mechanism-analysis-communication-breakdown-n29" class="level4" data-number="3.2.3">
<h4 data-number="3.2.3" class="anchored" data-anchor-id="mechanism-analysis-communication-breakdown-n29"><span class="header-section-number">3.2.3</span> Mechanism Analysis: Communication Breakdown (n=29)</h4>
<p>To examine whether communication quality moderated the effect of interaction policy, Bayesian models were fitted in the full sample with proportional communication breakdown included as an interaction term. For TPS-HRI, the interaction between policy and communication breakdown was weak and centered near zero (posterior median β = −1.14, 95% credible interval [−18.87, 16.28]), with only 55% probability of being negative and 50% probability of being significant. This broad, unstable posterior distribution suggests that evaluative trust judgments were relatively insensitive to graded variation in communication quality once a basic threshold of viability was established. In contrast, TI-HRC showed a more consistent pattern. The interaction term showed a negative tendency (posterior median β = −5.97, 95% credible interval [−23.01, 10.92]), with 76% probability of being negative, 71% probability of being significant, and 45% probability of being large. While responsive behaviour was associated with higher experienced trust under low levels of breakdown, this advantage diminished as communication failures accumulated. This differential sensitivity suggests that experienced trust depends critically on the robot’s sustained ability to engage in responsive interaction, whereas evaluative trust judgments may rely more on discrete moments of successful collaboration that can occur even within partially degraded interactions.</p>
<div class="cell">
<div id="tbl-post-nonviable" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-post-nonviable-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Post-Interaction Objective Measures by Group
</figcaption>
<div aria-describedby="tbl-post-nonviable-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="iiirporkin" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#iiirporkin table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#iiirporkin thead, #iiirporkin tbody, #iiirporkin tfoot, #iiirporkin tr, #iiirporkin td, #iiirporkin th {
  border-style: none;
}

#iiirporkin p {
  margin: 0;
  padding: 0;
}

#iiirporkin .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 13px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#iiirporkin .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#iiirporkin .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#iiirporkin .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#iiirporkin .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#iiirporkin .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#iiirporkin .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#iiirporkin .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#iiirporkin .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#iiirporkin .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#iiirporkin .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#iiirporkin .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#iiirporkin .gt_spanner_row {
  border-bottom-style: hidden;
}

#iiirporkin .gt_group_heading {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#iiirporkin .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#iiirporkin .gt_from_md > :first-child {
  margin-top: 0;
}

#iiirporkin .gt_from_md > :last-child {
  margin-bottom: 0;
}

#iiirporkin .gt_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#iiirporkin .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#iiirporkin .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#iiirporkin .gt_row_group_first td {
  border-top-width: 2px;
}

#iiirporkin .gt_row_group_first th {
  border-top-width: 2px;
}

#iiirporkin .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#iiirporkin .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#iiirporkin .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#iiirporkin .gt_last_summary_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#iiirporkin .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#iiirporkin .gt_first_grand_summary_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#iiirporkin .gt_last_grand_summary_row_top {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#iiirporkin .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#iiirporkin .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#iiirporkin .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#iiirporkin .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#iiirporkin .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#iiirporkin .gt_sourcenote {
  font-size: 90%;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#iiirporkin .gt_left {
  text-align: left;
}

#iiirporkin .gt_center {
  text-align: center;
}

#iiirporkin .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#iiirporkin .gt_font_normal {
  font-weight: normal;
}

#iiirporkin .gt_font_bold {
  font-weight: bold;
}

#iiirporkin .gt_font_italic {
  font-style: italic;
}

#iiirporkin .gt_super {
  font-size: 65%;
}

#iiirporkin .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#iiirporkin .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#iiirporkin .gt_indent_1 {
  text-indent: 5px;
}

#iiirporkin .gt_indent_2 {
  text-indent: 10px;
}

#iiirporkin .gt_indent_3 {
  text-indent: 15px;
}

#iiirporkin .gt_indent_4 {
  text-indent: 20px;
}

#iiirporkin .gt_indent_5 {
  text-indent: 25px;
}

#iiirporkin .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#iiirporkin div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>

<table class="gt_table do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-bootstrap="false">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="gt_col_headings header">
<th id="label" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col"><strong>Characteristic</strong></th>
<th id="stat_1" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>CONTROL</strong><br>
N = 10<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span></th>
<th id="stat_2" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>RESPONSIVE</strong><br>
N = 14<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span></th>
<th id="p.value" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>p-value</strong><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>2</sup></span></th>
</tr>
</thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_left" headers="label">Objective Measures</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Dialogue Turns</td>
<td class="gt_row gt_center" headers="stat_1">34 (9)</td>
<td class="gt_row gt_center" headers="stat_2">33 (5)</td>
<td class="gt_row gt_center" headers="p.value">0.45</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Avg Session Duration (mins)</td>
<td class="gt_row gt_center" headers="stat_1">13.24 (3.06)</td>
<td class="gt_row gt_center" headers="stat_2">15.26 (2.12)</td>
<td class="gt_row gt_center" headers="p.value">0.084</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Avg Robot Response Time (ms)</td>
<td class="gt_row gt_center" headers="stat_1">14.37 (3.76)</td>
<td class="gt_row gt_center" headers="stat_2">17.24 (2.52)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">&lt;0.001</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Silent Periods</td>
<td class="gt_row gt_center" headers="stat_1">5.60 (1.96)</td>
<td class="gt_row gt_center" headers="stat_2">4.71 (2.05)</td>
<td class="gt_row gt_center" headers="p.value">0.29</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Engaged Responses</td>
<td class="gt_row gt_center" headers="stat_1">2.00 (2.21)</td>
<td class="gt_row gt_center" headers="stat_2">3.50 (1.95)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.040</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Frustrated Responses</td>
<td class="gt_row gt_center" headers="stat_1">0.60 (0.70)</td>
<td class="gt_row gt_center" headers="stat_2">0.93 (1.21)</td>
<td class="gt_row gt_center" headers="p.value">0.68</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">% of Dialogue Turns Characterized by...</td>
<td class="gt_row gt_center" headers="stat_1"><br>
</td>
<td class="gt_row gt_center" headers="stat_2"><br>
</td>
<td class="gt_row gt_center" headers="p.value"><br>
</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Communication Breakdowns</td>
<td class="gt_row gt_center" headers="stat_1">0.25 (0.17)</td>
<td class="gt_row gt_center" headers="stat_2">0.22 (0.16)</td>
<td class="gt_row gt_center" headers="p.value">0.70</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Human Reasoning</td>
<td class="gt_row gt_center" headers="stat_1">0.26 (0.12)</td>
<td class="gt_row gt_center" headers="stat_2">0.35 (0.16)</td>
<td class="gt_row gt_center" headers="p.value">0.21</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Robot Reasoning</td>
<td class="gt_row gt_center" headers="stat_1">0.12 (0.07)</td>
<td class="gt_row gt_center" headers="stat_2">0.37 (0.14)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">&lt;0.001</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Robot Helpful Guidance</td>
<td class="gt_row gt_center" headers="stat_1">0.68 (0.08)</td>
<td class="gt_row gt_center" headers="stat_2">0.84 (0.08)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">&lt;0.001</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Robot Unhelpful Contributions</td>
<td class="gt_row gt_center" headers="stat_1">0.08 (0.04)</td>
<td class="gt_row gt_center" headers="stat_2">0.02 (0.03)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.003</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Robot Encouragement</td>
<td class="gt_row gt_center" headers="stat_1">0.00 (0.00)</td>
<td class="gt_row gt_center" headers="stat_2">0.36 (0.11)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">&lt;0.001</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Robot Empathy Expression</td>
<td class="gt_row gt_center" headers="stat_1">0.00 (0.00)</td>
<td class="gt_row gt_center" headers="stat_2">0.13 (0.09)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">&lt;0.001</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Robot Collaborative Language</td>
<td class="gt_row gt_center" headers="stat_1">0.05 (0.05)</td>
<td class="gt_row gt_center" headers="stat_2">0.42 (0.16)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">&lt;0.001</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">&nbsp;&nbsp;&nbsp;&nbsp;Robot Proactive Check-ins</td>
<td class="gt_row gt_center" headers="stat_1">0.21 (0.08)</td>
<td class="gt_row gt_center" headers="stat_2">0.13 (0.08)</td>
<td class="gt_row gt_center" headers="p.value" style="font-weight: bold">0.033</td>
</tr>
</tbody><tfoot>
<tr class="gt_footnotes odd">
<td colspan="4" class="gt_footnote"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span> Mean (SD)</td>
</tr>
<tr class="gt_footnotes even">
<td colspan="4" class="gt_footnote"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>2</sup></span> Wilcoxon rank sum test; Wilcoxon rank sum exact test</td>
</tr>
</tfoot>

</table>

</div>
</div>
</div>
</figure>
</div>
</div>
</section>
</section>
<section id="task-performance" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="task-performance"><span class="header-section-number">3.3</span> Task performance</h3>
<p>Overall task accuracy did not differ significantly between conditions (60% vs.&nbsp;66%, p = .534), nor did performance on individual task components reach statistical significance (see <a href="#tbl-post-tasks" class="quarto-xref">Table&nbsp;4</a>). However, a notable pattern emerged when examining task structure. For the suspect identification task (Task 1), which required robot collaboration to complete accurately, participants in the responsive condition achieved more than double the accuracy of those in the control condition (64% vs.&nbsp;30%, p = .106), representing a large effect that approached but did not reach conventional significance thresholds given the pilot sample size. In contrast, performance on the location identification task components (building, zone, and floor identification), where robot assistance was optional, showed no consistent directional advantage for either condition. These patterns suggest that responsive robot behaviour may particularly benefit collaborative performance on tasks requiring sustained interaction and mutual grounding, though larger samples are needed to establish statistical reliability. Critically, the absence of significant overall accuracy differences indicates that observed trust differences cannot be attributed simply to differential task success, but rather reflect distinct responses to the interaction process itself.</p>
<div class="cell">
<div id="tbl-post-tasks" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-post-tasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: Post-Interaction Raw Outcome Measures by Group
</figcaption>
<div aria-describedby="tbl-post-tasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="qmlbmfoyqp" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#qmlbmfoyqp table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#qmlbmfoyqp thead, #qmlbmfoyqp tbody, #qmlbmfoyqp tfoot, #qmlbmfoyqp tr, #qmlbmfoyqp td, #qmlbmfoyqp th {
  border-style: none;
}

#qmlbmfoyqp p {
  margin: 0;
  padding: 0;
}

#qmlbmfoyqp .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 13px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#qmlbmfoyqp .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#qmlbmfoyqp .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#qmlbmfoyqp .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#qmlbmfoyqp .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#qmlbmfoyqp .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#qmlbmfoyqp .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#qmlbmfoyqp .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#qmlbmfoyqp .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#qmlbmfoyqp .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#qmlbmfoyqp .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#qmlbmfoyqp .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#qmlbmfoyqp .gt_spanner_row {
  border-bottom-style: hidden;
}

#qmlbmfoyqp .gt_group_heading {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#qmlbmfoyqp .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#qmlbmfoyqp .gt_from_md > :first-child {
  margin-top: 0;
}

#qmlbmfoyqp .gt_from_md > :last-child {
  margin-bottom: 0;
}

#qmlbmfoyqp .gt_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#qmlbmfoyqp .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#qmlbmfoyqp .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#qmlbmfoyqp .gt_row_group_first td {
  border-top-width: 2px;
}

#qmlbmfoyqp .gt_row_group_first th {
  border-top-width: 2px;
}

#qmlbmfoyqp .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#qmlbmfoyqp .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#qmlbmfoyqp .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#qmlbmfoyqp .gt_last_summary_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#qmlbmfoyqp .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#qmlbmfoyqp .gt_first_grand_summary_row {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#qmlbmfoyqp .gt_last_grand_summary_row_top {
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#qmlbmfoyqp .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#qmlbmfoyqp .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#qmlbmfoyqp .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#qmlbmfoyqp .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#qmlbmfoyqp .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#qmlbmfoyqp .gt_sourcenote {
  font-size: 90%;
  padding-top: 1px;
  padding-bottom: 1px;
  padding-left: 5px;
  padding-right: 5px;
}

#qmlbmfoyqp .gt_left {
  text-align: left;
}

#qmlbmfoyqp .gt_center {
  text-align: center;
}

#qmlbmfoyqp .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#qmlbmfoyqp .gt_font_normal {
  font-weight: normal;
}

#qmlbmfoyqp .gt_font_bold {
  font-weight: bold;
}

#qmlbmfoyqp .gt_font_italic {
  font-style: italic;
}

#qmlbmfoyqp .gt_super {
  font-size: 65%;
}

#qmlbmfoyqp .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#qmlbmfoyqp .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#qmlbmfoyqp .gt_indent_1 {
  text-indent: 5px;
}

#qmlbmfoyqp .gt_indent_2 {
  text-indent: 10px;
}

#qmlbmfoyqp .gt_indent_3 {
  text-indent: 15px;
}

#qmlbmfoyqp .gt_indent_4 {
  text-indent: 20px;
}

#qmlbmfoyqp .gt_indent_5 {
  text-indent: 25px;
}

#qmlbmfoyqp .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#qmlbmfoyqp div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>

<table class="gt_table do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-bootstrap="false">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="gt_col_headings header">
<th id="label" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col"><strong>Characteristic</strong></th>
<th id="stat_1" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>CONTROL</strong><br>
N = 10<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span></th>
<th id="stat_2" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>RESPONSIVE</strong><br>
N = 14<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span></th>
<th id="p.value" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col"><strong>p-value</strong><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>2</sup></span></th>
</tr>
</thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_left" headers="label">% Task Accuracy</td>
<td class="gt_row gt_center" headers="stat_1">0.60 (0.21)</td>
<td class="gt_row gt_center" headers="stat_2">0.66 (0.23)</td>
<td class="gt_row gt_center" headers="p.value">0.47</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">Suspect ID Accuracy (robot dependent)</td>
<td class="gt_row gt_center" headers="stat_1">3 / 10 (30%)</td>
<td class="gt_row gt_center" headers="stat_2">9 / 14 (64%)</td>
<td class="gt_row gt_center" headers="p.value">0.10</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">Building ID Accuracy</td>
<td class="gt_row gt_center" headers="stat_1">7 / 10 (70%)</td>
<td class="gt_row gt_center" headers="stat_2">11 / 14 (79%)</td>
<td class="gt_row gt_center" headers="p.value">0.67</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="label">Zone ID Accuracy</td>
<td class="gt_row gt_center" headers="stat_1">5 / 10 (50%)</td>
<td class="gt_row gt_center" headers="stat_2">4 / 14 (29%)</td>
<td class="gt_row gt_center" headers="p.value">0.40</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="label">Floor ID Accuracy</td>
<td class="gt_row gt_center" headers="stat_1">7 / 10 (70%)</td>
<td class="gt_row gt_center" headers="stat_2">13 / 14 (93%)</td>
<td class="gt_row gt_center" headers="p.value">0.27</td>
</tr>
</tbody><tfoot>
<tr class="gt_footnotes odd">
<td colspan="4" class="gt_footnote"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>1</sup></span> Mean (SD); n / N (%)</td>
</tr>
<tr class="gt_footnotes even">
<td colspan="4" class="gt_footnote"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;line-height:0;"><sup>2</sup></span> Wilcoxon rank sum test; Pearson’s Chi-squared test; Fisher’s exact test</td>
</tr>
</tfoot>

</table>

</div>
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="individual-differences-and-correlational-patterns" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="individual-differences-and-correlational-patterns"><span class="header-section-number">3.4</span> Individual differences and correlational patterns</h3>
<p>Correlational analyses revealed patterns consistent with the interpretation that trust was shaped by interaction quality rather than individual predispositions or task outcomes. As expected, higher Need for Cognition (NFC) scores were negatively associated with baseline Negative Attitudes Towards Robots (NARS; r = −.48, p = .01), indicating that individuals who enjoy effortful thinking tend to hold more positive attitudes toward robots prior to interaction. However, neither NFC nor NARS showed significant associations with post-interaction trust outcomes (r = −.17 to −.33, p &gt; .11), suggesting that the impact of responsive robot behaviour was relatively independent of participants’ baseline dispositions toward robots or cognitive engagement preferences.</p>
<p>Consistent with the experimental manipulation, specific robot dialogue behaviours showed substantial correlations with trust outcomes. Robot empathy expression—quantified as the proportion of dialogue turns in which the robot acknowledged participant affect or expressed understanding—was strongly correlated with both experienced trust (r = .53, p = .008) and perceived trust (r = .65, p = .001). Similarly, robot use of collaborative language (e.g., “we,” “let’s”) was positively associated with experienced trust (r = .50, p = .012), as was robot encouragement (experienced trust: r = .45, p = .027; perceived trust: r = .42, p = .040). These associations provide direct evidence that the specific affective and collaborative behaviours implemented in the responsive condition were linked to participants’ trust evaluations beyond simple condition assignment.</p>
<p>Participant engagement during interaction, operationalized as the frequency of AI-detected positive affective responses, was positively correlated with perceived trust (r = .52, p = .009) and showed a trending association with experienced trust (r = .30, p = .156). Engaged responses were also associated with longer interaction duration (r = .54, p = .007), greater use of collaborative language by the robot (r = .50, p = .013), and fewer communication breakdowns (r = −.49, p = .016). This pattern suggests that responsive robot behaviour may have fostered a reciprocal dynamic in which robot affective adaptation elicited participant engagement, which in turn supported smoother interaction and higher trust.</p>
<p>Notably, objective task performance showed no significant association with either trust measure (suspect accuracy: r = .18–.20, p &gt; .35; overall accuracy: r = −.05 to .19, p &gt; .37). This dissociation between task outcomes and trust ratings indicates that participants’ trust judgments reflected the quality and affective tone of the interaction process rather than instrumental success on the collaborative tasks. Task performance was not included as a covariate in primary models to avoid conditioning on a potential mediator of interaction policy effects.</p>
</section>
</section>
<section id="discussion" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="discussion"><span class="header-section-number">4</span> Discussion</h2>
<p>This study examined how a responsive interaction policy can influence trust during fully autonomous, spoken-language human–robot collaboration. Unlike much prior work in HRI trust research, all dialogue management, affect inference, task progression, and response generation in the present system were executed autonomously and in real time, without Wizard-of-Oz control, scripted recovery, or human intervention. As a result, participants were exposed not only to adaptive interaction behaviour, but also to the unavoidable limitations, delays, and failures characteristic of deployed autonomous systems.</p>
<p>Across analytic approaches, the responsive interaction policy was consistently associated with higher post-interaction trust compared to a neutral, reactive control policy when interaction functioned as intended. Crucially, these trust differences emerged without reliable differences in overall task accuracy, indicating that trust was shaped by the interaction process itself rather than by instrumental task success–despite participants learning whether they were correct or not during wrapup. This dissociation was particularly clear in the robot-dependent task, where responsive behaviour showed a large (though underpowered) advantage in accuracy, and in the open-ended task, where trust varied independently of performance.</p>
<p>The results further suggest that autonomous interactions are critical to learning how trust is formed and evaluated. Sensitivity and mechanism analyses showed that task-oriented trust judgments (e.g., perceived reliability) were relatively robust to moderate communication degradation, whereas experienced trust, capturing affective comfort, engagement, and embodied collaboration, was highly sensitive to interaction-level failures. As communication breakdown accumulated, the trust advantage conferred by responsive behaviour diminished, suggesting that affect-adaptive policies require a minimum threshold of interaction viability to operate effectively.</p>
<p>Importantly, communication failure did not merely reduce trust uniformly. In sessions characterized by severe breakdown, the responsive robot continued to generate proactive assistance, encouragement, and meta-communication aimed at repairing the interaction. However, when spoken-language grounding could not be re-established, these behaviours may have increased participant confusion and cognitive distress. In contrast, the neutral robot’s reactive policy resulted in fewer unsolicited interventions, which—while less supportive under functional conditions—reduced interaction complexity when collaboration was no longer viable. Under these conditions, trust ratings no longer systematically reflected the intended policy manipulation.</p>
<p>This pattern highlights a key insight specific to autonomous human–robot interaction: when language-mediated collaboration collapses, higher-level constructs such as trust are not simply attenuated, but may cease to be meaningfully instantiated <span class="citation" data-cites="devisser2020 lee2004"><a href="#ref-devisser2020" role="doc-biblioref">[9]</a>, <a href="#ref-lee2004" role="doc-biblioref">[36]</a></span>. In such cases, trust measures may not reflect calibrated judgments of reliability or competence, but rather the breakdown of the interaction itself. This distinction is often obscured in scripted or Wizard-of-Oz paradigms, where failures can be covertly repaired and autonomy constraints are masked.</p>
<p>The present findings therefore support a process-oriented view of examining trust in autonomous HRI. Trust emerges not only from what a robot does, but from how it manages uncertainty, error, and interactional misalignment without human intervention. Responsive interaction policies can enhance trust under viable conditions, but they also amplify the consequences of failure when the system lacks sufficient grounding to adapt effectively.</p>
</section>
<section id="limitations" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="limitations"><span class="header-section-number">5</span> Limitations</h2>
<p>Several limitations should be considered when interpreting the present findings.</p>
<p>This study was conducted as a pilot with a modest sample size, which limits statistical power for detecting interaction effects involving task structure, communication quality, and individual differences. Although effect sizes were often large and consistent across analytic approaches, uncertainty remains high. The primary aim of this work was therefore not definitive hypothesis testing, but feasibility assessment and mechanism exploration under fully autonomous conditions. In this respect, the study successfully identified both promising effects and critical boundary conditions for trust formation.</p>
<p>Spoken-language interaction viability emerged as a central constraint of fully autonomous collaboration, and one that revealed an important direction for future research. Although English fluency was an eligibility requirement, substantial variability in functional spoken-language proficiency was observed during in-person sessions. In a subset of interactions, persistent speech recognition failure prevented the experimental manipulation from operating as intended, leading to exclusion from primary analyses on methodological grounds. Rather than reflecting a shortcoming of the experimental design, these cases highlight a fundamental challenge for autonomous spoken-language HRI: trust and collaboration presuppose a minimum level of linguistic grounding, and when that grounding fails, higher-level interaction constructs are no longer meaningfully instantiated.</p>
<p>The visibility of this constraint is itself a consequence of autonomy. In scripted or Wizard-of-Oz paradigms, language breakdown can be covertly repaired or masked by human intervention. In fully autonomous systems, communication viability becomes an explicit property of the interaction that must be detected, managed, and responded to by the robot itself. This points to a clear research direction focused on interaction policies that can recognize emerging language mismatch and adapt accordingly, rather than assuming linguistic competence as a fixed prerequisite.</p>
<p>Natural language understanding was also constrained by the task-specific policy design used in the robot-dependent task. Fixed mappings between participant questions and predefined task features meant that semantically valid but unexpected phrasing, synonym use, or multi-attribute queries occasionally led to misinterpretation or incorrect responses (e.g., treating “orange hair” as distinct from the ground-truth feature “red”). These failures reflect limitations in prompt design and NLU robustness rather than participant reasoning, and likely contributed to some interaction breakdowns.</p>
<p>Measurement-related factors may have introduced additional noise. One trust instrument relied on continuous slider-based responses administered via a laptop touchpad, whereas the other used discrete, clickable Likert-style responses. Touchpad-based slider interaction can be awkward and imprecise for some users, which may have attenuated effects on the continuous scale relative to the Likert-based measure.</p>
<p>Finally, affect inference in the deployed system relied primarily on speech-based signals and conversational context. This design choice prioritized real-time stability and robustness under autonomous constraints, but necessarily limited the richness of affect sensing. Incorporating facial expression or prosodic features could improve responsiveness, though such approaches introduce additional latency, orchestration complexity, and failure modes that were beyond the scope of this pilot study.</p>
</section>
<section id="conclusion-and-future-work" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="conclusion-and-future-work"><span class="header-section-number">6</span> Conclusion and Future Work</h2>
<p>This pilot study demonstrates that affect-responsive interaction policies can meaningfully increase trust in fully autonomous, in-person human–robot collaboration, even under realistic conditions that include latency, misrecognition, and interaction breakdown. Trust differences emerged independently of task success, underscoring the central role of interaction quality and affective responsiveness in shaping collaborative experience.</p>
<p>At the same time, the results identify clear boundary conditions for trust evaluation in spoken-language HRI. When communication viability collapses, trust is not merely reduced—it becomes undefined. Recognizing and modeling this distinction is essential for both experimental design and the deployment of autonomous social robots in real-world settings.</p>
<p>An important direction for future work concerns adaptive language management under autonomous conditions. The present findings indicate that communication viability constitutes a hard boundary condition for trust formation in spoken-language HRI. While interaction policies could be improved to better recognize early signs of language mismatch—such as repeated clarification requests, fragmented utterances, or prolonged turn failures—more ambitious approaches could involve explicit modeling of language proficiency and accent-related variability.</p>
<p>One possible extension would involve a modeling approach in which lightweight classifiers estimate the likelihood of a language barrier based on acoustic features (e.g., accent, speech rate, phoneme-level uncertainty), dialogue-level signals (e.g., repeated speech recognition failures, repair loops), and interaction dynamics. These estimates could inform downstream interaction policy decisions, such as simplifying linguistic structure, slowing response pacing, increasing redundancy, switching to more constrained phrasing, or offering alternative interaction strategies.</p>
<p>More advanced implementations could involve accent-aware or language-adaptive speech recognition and response generation pipelines that dynamically adjust recognition models or response language. Such approaches introduce substantial technical and ethical complexity, including increased latency, model orchestration challenges, and risks of misclassification or inappropriate adaptation <span class="citation" data-cites="piercy2025"><a href="#ref-piercy2025" role="doc-biblioref">[37]</a></span>. Future work should therefore prioritize conservative, transparency-oriented adaptations that improve interaction robustness without over-ascribing user attributes or undermining user agency.</p>
<p>Critically, these adaptations would need to operate fully autonomously and be evaluated not only for performance gains, but for their impact on trust calibration, user comfort, and perceived competence. The present findings suggest that the ability of a robot to recognize <em>when</em> collaboration is breaking down—and to respond in ways that reduce cognitive load rather than amplify it—may be as important for trust as affective responsiveness itself.</p>
<p>A second systems-focused direction concerns dialogue and stage-control architecture. The present implementation used a LangChain-based dialogue manager with stage state injected into prompts at each turn <span class="citation" data-cites="Chase_LangChain_2022"><a href="#ref-Chase_LangChain_2022" role="doc-biblioref">[38]</a></span>. While effective for establishing end-to-end autonomy, this approach proved relatively brittle in the face of off-nominal interaction trajectories (e.g., participants advancing stages early via the interface, skipping wrap-up, or producing fragmented utterances that triggered repetitive recovery prompts). Future iterations would benefit from a more explicit state-machine representation of the interaction, with guardrails around stage transitions, better defined recovery pathways, and tighter coupling between task state, dialogue policy, and tool use.</p>
<p>One practical route is to migrate from a linear chain design to a graph-based orchestration framework (e.g., LangGraph), in which stage handling is represented as explicit nodes and conditional edges rather than implicit prompt-following <span class="citation" data-cites="Chase_LangChain_2022"><a href="#ref-Chase_LangChain_2022" role="doc-biblioref">[38]</a></span>. This would support more flexible adaptation under autonomy constraints, including formalized handling of stage skips and partial completions, more granular repair strategies when communication degrades, and safer integration of additional tools. In particular, a graph-based design would make it easier to incorporate multimodal sensing (e.g., facial affect, prosody, interaction timing features) as tool calls invoked conditionally when uncertainty is high, rather than running continuously and imposing latency <span class="citation" data-cites="a.v.2024"><a href="#ref-a.v.2024" role="doc-biblioref">[39]</a></span>. This architectural shift would allow future work to evaluate not only <em>whether</em> responsiveness increases trust, but <em>which components of an autonomous policy</em>—state tracking, tool selection, repair strategy, or affect inference—drive improvements in collaboration.</p>
<p>Future work will also extend this research empirically. Larger samples will enable formal tests of mediation pathways linking responsiveness, interaction fluency, affective engagement, and trust outcomes. Planned studies will compare embodied robot interaction with functionally equivalent virtual agents to isolate the contribution of physical embodiment under autonomous control. On the systems side, improvements to NLU robustness, turn-taking management, participant instruction, and multimodal affect inference will be pursued to reduce avoidable interaction failures while preserving real-time autonomy.</p>
<p>More broadly, this work underscores the importance of evaluating trust in autonomous robots under conditions that expose, rather than conceal, system limitations. Understanding how trust is negotiated, disrupted, and repaired in the absence of human intervention is essential for the responsible deployment of autonomous robots in real-world collaborative settings.</p>
</section>




<div id="quarto-appendix" class="default"><section id="sec-appendix-a" class="level2 appendix" data-number="7"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">7</span> Appendix A</h2><div class="quarto-appendix-contents">

<section id="technical-implementation-details" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="technical-implementation-details"><span class="header-section-number">7.1</span> Technical Implementation Details</h3>
<p>The experimental system comprised a fully autonomous, multi-stage collaborative task in which participants interacted with the Misty II social robot to solve a two-part investigative scenario. Interaction was mediated through spoken dialogue and a companion web interface, allowing the robot and participant to jointly reason about task information. The system was designed as a mixed-initiative dialogue architecture with optional affect-responsive behaviour, implemented without human intervention during experimental sessions.</p>
<section id="hardware-platform" class="level4" data-number="7.1.1">
<h4 data-number="7.1.1" class="anchored" data-anchor-id="hardware-platform"><span class="header-section-number">7.1.1</span> Hardware Platform</h4>
<p>The robot platform used in this study was the Misty II social robot. Misty II is a mobile social robot equipped with an expressive display, articulated head and arms, and programmable RGB LEDs. These components were used to produce synchronized verbal and nonverbal behaviour, including eye expressions, head movements, arm gestures, and colour-based state indicators. Audio input was captured via the robot’s RTSP video stream, which provided real-time access to the microphone signal for downstream speech processing.</p>
</section>
<section id="software-architecture" class="level4" data-number="7.1.2">
<h4 data-number="7.1.2" class="anchored" data-anchor-id="software-architecture"><span class="header-section-number">7.1.2</span> Software Architecture</h4>
<p>All system components were implemented in Python (version 3.10) <span class="citation" data-cites="python"><a href="#ref-python" role="doc-biblioref">[40]</a></span>. The software architecture integrated robot control, speech processing, dialogue management, task logic, and data logging into a single autonomous pipeline. Core dependencies included the Misty Robotics Python SDK for robot control, the Deepgram SDK for speech recognition <span class="citation" data-cites="misty deepgram"><a href="#ref-misty" role="doc-biblioref">[41]</a>, <a href="#ref-deepgram" role="doc-biblioref">[42]</a></span>, FFmpeg for audio stream processing, Flask and Flask-SocketIO for the web-based task interface, and DuckDB for structured data logging <span class="citation" data-cites="duckdb2026"><a href="#ref-duckdb2026" role="doc-biblioref">[43]</a></span>.</p>
</section>
<section id="dialogue-management-and-large-language-model-integration" class="level4" data-number="7.1.3">
<h4 data-number="7.1.3" class="anchored" data-anchor-id="dialogue-management-and-large-language-model-integration"><span class="header-section-number">7.1.3</span> Dialogue Management and Large Language Model Integration</h4>
<p>Dialogue was managed using the LangChain framework, which provided abstraction over message handling, memory persistence, and large language model integration <span class="citation" data-cites="Chase_LangChain_2022"><a href="#ref-Chase_LangChain_2022" role="doc-biblioref">[38]</a></span>. The system used Google’s Gemini API as the underlying language model, configured to produce strictly JSON-formatted outputs to ensure reliable downstream parsing and execution on the robot <span class="citation" data-cites="gemini"><a href="#ref-gemini" role="doc-biblioref">[44]</a></span>.</p>
<p>The deployed model was <code>gemini-2.5-flash-lite</code>, selected for its low-latency response characteristics. Generation temperature was set to 0.7 to balance coherence and variability. Conversation history was maintained using a buffer-based memory mechanism, allowing the robot to reference prior exchanges within a session while resetting memory between participants. Conversation histories were stored as session-specific JSON files to enable post-hoc analysis and recovery.</p>
</section>
<section id="prompt-structure-and-context-injection" class="level4" data-number="7.1.4">
<h4 data-number="7.1.4" class="anchored" data-anchor-id="prompt-structure-and-context-injection"><span class="header-section-number">7.1.4</span> Prompt Structure and Context Injection</h4>
<p>System prompts were constructed dynamically at each dialogue turn. Each prompt consisted of a system message defining task rules, role constraints, and output format requirements, followed by the accumulated conversation history and the current participant utterance. In addition to transcribed speech, structured contextual variables were injected into the prompt as JSON fields, including the current task stage, detected emotion labels, timer expiration flags, and task submission status. This approach allowed the language model to access environmental state without embedding control information directly into conversational text.</p>
</section>
</section>
<section id="speech-processing" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="speech-processing"><span class="header-section-number">7.2</span> Speech Processing</h3>
<p>Speech-to-text processing was handled by Deepgram’s Nova-2 model using real-time WebSocket streaming. The system employed adaptive endpointing and voice activity detection to support conversational turn-taking. Endpointing thresholds differed across task stages, with shorter timeouts during dialogue-driven stages and longer timeouts during log-reading phases.</p>
<p>Text-to-speech output was generated using Misty II’s onboard TTS engine, which produces a synthetic robotic voice. Although external TTS options (including OpenAI and Deepgram Aura voices) were implemented and tested, the onboard voice was selected to avoid introducing human-like vocal qualities that could independently influence trust perceptions.</p>
</section>
<section id="emotion-detection-and-affective-state-mapping" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="emotion-detection-and-affective-state-mapping"><span class="header-section-number">7.3</span> Emotion Detection and Affective State Mapping</h3>
<p>Participant affect was inferred from transcribed utterances using a DistilRoBERTa-based emotion classification model fine-tuned for English-language emotion detection. The model produced categorical predictions (e.g., joy, frustration, anxiety, neutral), which were mapped to higher-level interaction states such as positive engagement, irritation, or confusion. In the responsive condition, these inferred states were used to guide dialogue strategy and nonverbal behaviour selection.</p>
</section>
<section id="multimodal-behaviour-generation" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="multimodal-behaviour-generation"><span class="header-section-number">7.4</span> Multimodal Behaviour Generation</h3>
<p>The robot’s nonverbal behaviour was implemented through a library of custom action scripts combining facial expressions, LED patterns, arm movements, and head motions. At each dialogue turn, the language model selected an expression label from a predefined set, which was then translated into a coordinated multimodal action. In the responsive condition, additional backchannel behaviours were triggered during participant speech, including listening cues and emotion-matched expressions.</p>
<p>LED colours were used to signal system state to participants. A blue LED indicated active listening, while a purple LED indicated processing or speaking.</p>
</section>
<section id="collaborative-tasks" class="level3" data-number="7.5">
<h3 data-number="7.5" class="anchored" data-anchor-id="collaborative-tasks"><span class="header-section-number">7.5</span> Collaborative Tasks</h3>
<p>The interaction consisted of two collaborative tasks inspired by the work of <span class="citation" data-cites="lin2022a"><a href="#ref-lin2022a" role="doc-biblioref">[45]</a></span>. In the first task, participants and the robot jointly solved a “who-dunnit” problem by eliminating suspects from a grid based on yes/no questions. The robot possessed ground-truth knowledge but was constrained to answering only feature-based yes/no queries. In the second task, participants and the robot attempted to locate a missing robot by interpreting cryptic system and sensor logs. In this task, the robot did not know the solution and instead provided guidance based on general technical knowledge and logical reasoning.</p>
<p>Task information and participant responses were presented through a web-based dashboard. The dashboard displayed suspect grids, system logs, and response input fields, and communicated task progression events back to the robot via REST API calls.</p>
</section>
<section id="data-collection-and-logging" class="level3" data-number="7.6">
<h3 data-number="7.6" class="anchored" data-anchor-id="data-collection-and-logging"><span class="header-section-number">7.6</span> Data Collection and Logging</h3>
<p>All interaction data were logged to a DuckDB relational database <span class="citation" data-cites="duckdb2026"><a href="#ref-duckdb2026" role="doc-biblioref">[43]</a></span>. Logged data included session metadata, turn-level dialogue transcripts, language model responses, nonverbal behaviour selections, response latencies, task submissions, detected emotions, and system events such as stage transitions and timer expirations. This structure enabled detailed post-hoc analysis of interaction dynamics, communication failures, and trust-related behaviours.</p>
</section>
<section id="interaction-dynamics-and-control-modes" class="level3" data-number="7.7">
<h3 data-number="7.7" class="anchored" data-anchor-id="interaction-dynamics-and-control-modes"><span class="header-section-number">7.7</span> Interaction Dynamics and Control Modes</h3>
<p>Two interaction policies were implemented and toggled programmatically at runtime: a responsive mode and a control mode. In the responsive mode, the robot proactively offered assistance, adjusted its dialogue based on inferred affect, and produced supportive backchannel behaviours. In the control mode, the robot provided information only when explicitly prompted and did not adapt its behaviour based on affective cues. The active mode was set prior to each session and remained fixed throughout the interaction.</p>
<p>Silence handling was implemented using a fixed threshold, after which the robot issued a check-in prompt. The phrasing of these prompts differed across conditions to reflect proactive versus reactive interaction strategies.</p>
</section>
<section id="inter-process-communication" class="level3" data-number="7.8">
<h3 data-number="7.8" class="anchored" data-anchor-id="inter-process-communication"><span class="header-section-number">7.8</span> Inter-process Communication</h3>
<p>System components communicated via a set of Flask-based REST endpoints. These endpoints synchronized task stage state, detected participant submissions, managed timer events, and allowed limited facilitator override when necessary. All communication between the web interface and the robot occurred locally to ensure low latency and experimental reliability.</p>
</section>
</div></section><section id="sec-appendix-b" class="level2 appendix" data-number="8"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">8</span> Appendix B</h2><div class="quarto-appendix-contents">

<section id="trust-perception-scale-hri-tps-hri" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="trust-perception-scale-hri-tps-hri"><span class="header-section-number">8.1</span> Trust Perception Scale HRI (TPS-HRI)</h3>
<p>Participants rated the following items on a percentage scale (0–100%), indicating the proportion of time each statement applied to the robot during the interaction.</p>
<ul>
<li>What percent of the time was the robot dependable?</li>
<li>What percent of the time was the robot reliable?</li>
<li>What percent of the time was the robot responsive?</li>
<li>What percent of the time was the robot trustworthy?</li>
<li>What percent of the time was the robot supportive?</li>
<li>What percent of the time did this robot act consistently?</li>
<li>What percent of the time did this robot provide feedback?</li>
<li>What percent of the time did this robot meet the needs of the mission task?</li>
<li>What percent of the time did this robot provide appropriate information?</li>
<li>What percent of the time did this robot communicate appropriately?</li>
<li>What percent of the time did this robot follow directions?</li>
<li>What percent of the time did this robot answer the questions asked?</li>
</ul>
</section>
<section id="trust-in-industrial-humanrobot-collaboration-ti-hrc" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="trust-in-industrial-humanrobot-collaboration-ti-hrc"><span class="header-section-number">8.2</span> Trust in Industrial Human–Robot Collaboration (TI-HRC)</h3>
<p>Participants indicated their agreement with the following statements using a 5-point Likert-type scale (Strongly Disagree to Strongly Agree). Negatively worded items were reverse-scored prior to analysis.</p>
<p><strong><em>Reliability</em></strong></p>
<ul>
<li>I trusted that the robot would give me accurate answers.</li>
<li>The robot’s responses seemed reliable.</li>
<li>I felt I could rely on the robot to do what it was supposed to do.</li>
</ul>
<p><strong><em>Perceptual / Affective Trust</em></strong></p>
<ul>
<li>The robot seemed to enjoy helping me.</li>
<li>The robot was responsive to my needs.</li>
<li>The robot seemed to care about helping me.</li>
</ul>
<p><strong><em>Discomfort / Unease</em></strong></p>
<ul>
<li>The way the robot moved made me uncomfortable. (R)</li>
<li>The way the robot spoke made me uncomfortable. (R)</li>
<li>Talking to the robot made me uneasy. (R)</li>
</ul>
</section>
</div></section><section id="sec-appendix-c" class="level2 appendix" data-number="9"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">9</span> Appendix C</h2><div class="quarto-appendix-contents">

<section id="dialogue-coding-scheme" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="dialogue-coding-scheme"><span class="header-section-number">9.1</span> Dialogue Coding Scheme</h3>
<section id="task-outcome-layer-stage-level" class="level4" data-number="9.1.1">
<h4 data-number="9.1.1" class="anchored" data-anchor-id="task-outcome-layer-stage-level"><span class="header-section-number">9.1.1</span> Task Outcome Layer (Stage-Level)</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 36%">
<col style="width: 26%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>task_outcome</code></td>
<td>categorical</td>
<td>Final task status (<code>completed</code>, <code>timeout</code>, <code>skipped</code>, <code>partial</code>, <code>abandoned</code>).</td>
</tr>
<tr class="even">
<td><code>task_completed</code></td>
<td>binary</td>
<td>Task goal was fully completed.</td>
</tr>
<tr class="odd">
<td><code>task_timed_out</code></td>
<td>binary</td>
<td>Task ended due to expiration of the time limit.</td>
</tr>
<tr class="even">
<td><code>task_skipped</code></td>
<td>binary</td>
<td>Participant explicitly skipped or advanced past the stage.</td>
</tr>
<tr class="odd">
<td><code>task_partially_completed</code></td>
<td>binary</td>
<td>Task progress was made, but the full solution was not reached.</td>
</tr>
<tr class="even">
<td><code>task_abandoned</code></td>
<td>binary</td>
<td>Participant disengaged or stopped attempting the task before timeout.</td>
</tr>
<tr class="odd">
<td><code>task_completed_without_help</code></td>
<td>binary</td>
<td>Task was completed without any help requests to the robot.</td>
</tr>
<tr class="even">
<td><code>task_required_robot_help</code></td>
<td>binary</td>
<td>At least one robot help interaction was required for task completion.</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="dialogue-interaction-layer-turn-level" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="dialogue-interaction-layer-turn-level"><span class="header-section-number">9.2</span> Dialogue Interaction Layer (Turn-Level)</h3>
<section id="human-turn-codes" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="human-turn-codes">Human Turn Codes</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 36%">
<col style="width: 26%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>human_help_request</code></td>
<td>binary</td>
<td>Participant explicitly or implicitly asks the robot for help or guidance.</td>
</tr>
<tr class="even">
<td><code>human_reasoning</code></td>
<td>binary</td>
<td>Participant reasons out loud with the robot toward problem-solving.</td>
</tr>
<tr class="odd">
<td><code>human_confusion</code></td>
<td>binary</td>
<td>Participant expresses confusion or uncertainty.</td>
</tr>
<tr class="even">
<td><code>human_confirmation_seeking</code></td>
<td>binary</td>
<td>Participant seeks confirmation of a tentative belief or solution.</td>
</tr>
</tbody>
</table>
</section>
<section id="robot-turn-codes" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="robot-turn-codes">Robot Turn Codes</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 26%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>robot_helpful_guidance</code></td>
<td>binary</td>
<td>Robot provides accurate, task-relevant information or guidance.</td>
</tr>
<tr class="even">
<td><code>robot_misleading_guidance</code></td>
<td>binary</td>
<td>Robot provides misleading or incorrect guidance.</td>
</tr>
<tr class="odd">
<td><code>robot_factually_incorrect</code></td>
<td>binary</td>
<td>Robot states information that is objectively incorrect (though it may not know it is incorrect).</td>
</tr>
<tr class="even">
<td><code>robot_policy_violation</code></td>
<td>binary</td>
<td>Robot violates stated system or task constraints.</td>
</tr>
<tr class="odd">
<td><code>robot_on_policy_unhelpful</code></td>
<td>binary</td>
<td>Robot adheres to policy but provides vague or non-actionable assistance.</td>
</tr>
<tr class="even">
<td><code>robot_stt_failure</code></td>
<td>binary</td>
<td>Robot response reflects a speech-to-text or input understanding failure.</td>
</tr>
<tr class="odd">
<td><code>robot_clarification_request</code></td>
<td>binary</td>
<td>Robot asks the participant for information or to repeat or clarify their input.</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="affective-interaction-layer-turn-level" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="affective-interaction-layer-turn-level"><span class="header-section-number">9.3</span> Affective Interaction Layer (Turn-Level)</h3>
<section id="robot-affective-behaviour" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="robot-affective-behaviour">Robot Affective behaviour</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 26%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>robot_empathy_expression</code></td>
<td>binary</td>
<td>Robot expresses empathy, encouragement, or reassurance.</td>
</tr>
<tr class="even">
<td><code>robot_emotion_acknowledgement</code></td>
<td>binary</td>
<td>Robot explicitly references an inferred participant emotional state.</td>
</tr>
</tbody>
</table>
</section>
<section id="human-affective-response" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="human-affective-response">Human Affective Response</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 26%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>human_affective_engagement</code></td>
<td>binary</td>
<td>Participant responds in a socially warm or engaged manner.</td>
</tr>
<tr class="even">
<td><code>human_social_reciprocity</code></td>
<td>binary</td>
<td>Participant mirrors or responds to the robot’s affective expression.</td>
</tr>
<tr class="odd">
<td><code>human_anthropomorphic_language</code></td>
<td>binary</td>
<td>Participant treats the robot as a social agent.</td>
</tr>
<tr class="even">
<td><code>human_emotional_disengagement</code></td>
<td>binary</td>
<td>Participant responds in a curt, dismissive, or withdrawn manner.</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="notes" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="notes"><span class="header-section-number">9.4</span> Notes</h3>
<ul>
<li>Turn-level variables are coded per dialogue turn.</li>
<li>Task outcome variables are coded once per <code>session_id × stage</code>.</li>
<li>Raw dialogue text was retained during coding and removed prior to aggregation.</li>
<li>Multiple turn-level codes may co-occur unless otherwise specified.</li>
</ul>



</section>
</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list">
<div id="ref-choi2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">J. Y. Choi <em>et al.</em>, <span>“Exploring Challenges and Opportunities in Manufacturing and Intelligence for Future Robotics,”</span> <em>International Journal of Precision Engineering and Manufacturing</em>, vol. 26, no. 9, pp. 2203–2222, Sep. 2025, doi: <a href="https://doi.org/10.1007/s12541-025-01318-2">10.1007/s12541-025-01318-2</a>. Available: <a href="https://doi.org/10.1007/s12541-025-01318-2">https://doi.org/10.1007/s12541-025-01318-2</a></div>
</div>
<div id="ref-fu2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">W. Fu, Y. Xu, L. Liu, and L. Zhang, <span>“Design and Research of Intelligent Safety Monitoring Robot for Coal Mine Shaft Construction,”</span> <em>Advances in Civil Engineering</em>, vol. 2021, no. 1, p. 6897767, Jan. 2021, doi: <a href="https://doi.org/10.1155/2021/6897767">10.1155/2021/6897767</a>. Available: <a href="https://onlinelibrary.wiley.com/doi/10.1155/2021/6897767">https://onlinelibrary.wiley.com/doi/10.1155/2021/6897767</a></div>
</div>
<div id="ref-ciuffreda2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">I. Ciuffreda <em>et al.</em>, <span>“Design and Development of a Technological Platform Based on a Sensorized Social Robot for Supporting Older Adults and Caregivers: GUARDIAN Ecosystem,”</span> <em>International Journal of Social Robotics</em>, vol. 17, no. 5, pp. 803–822, May 2025, doi: <a href="https://doi.org/10.1007/s12369-023-01038-5">10.1007/s12369-023-01038-5</a>. Available: <a href="https://doi.org/10.1007/s12369-023-01038-5">https://doi.org/10.1007/s12369-023-01038-5</a></div>
</div>
<div id="ref-diab2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">M. Diab and Y. Demiris, <span>“TICK: A Knowledge Processing Infrastructure for Cognitive Trust in Human<span></span>Robot Interaction,”</span> <em>International Journal of Social Robotics</em>, pp. 1–33, Jan. 2025, doi: <a href="https://doi.org/10.1007/s12369-024-01206-1">10.1007/s12369-024-01206-1</a>. Available: <a href="https://link.springer.com/article/10.1007/s12369-024-01206-1">https://link.springer.com/article/10.1007/s12369-024-01206-1</a></div>
</div>
<div id="ref-spitale2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">M. Spitale, M. Axelsson, and H. Gunes, <span>“Robotic mental well-being coaches for the workplace: An in-the-wild study on form,”</span> in HRI ’23. New York, NY, USA: Association for Computing Machinery, Mar. 2023, p. 301310. doi: <a href="https://doi.org/10.1145/3568162.3577003">10.1145/3568162.3577003</a>. Available: <a href="https://dl.acm.org/doi/10.1145/3568162.3577003">https://dl.acm.org/doi/10.1145/3568162.3577003</a></div>
</div>
<div id="ref-campagna2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">G. Campagna and M. Rehm, <span>“A systematic review of trust assessments in human<span></span>robot interaction,”</span> <em>J. Hum.-Robot Interact.</em>, vol. 14, no. 2, p. 30:130:35, Jan. 2025, doi: <a href="https://doi.org/10.1145/3706123">10.1145/3706123</a>. Available: <a href="https://doi.org/10.1145/3706123">https://doi.org/10.1145/3706123</a></div>
</div>
<div id="ref-emaminejad2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">N. Emaminejad and R. Akhavian, <span>“Trustworthy AI and robotics: Implications for the AEC industry,”</span> <em>Automation in Construction</em>, vol. 139, p. 104298, Jul. 2022, doi: <a href="https://doi.org/10.1016/j.autcon.2022.104298">10.1016/j.autcon.2022.104298</a>. Available: <a href="https://www.sciencedirect.com/science/article/pii/S0926580522001716">https://www.sciencedirect.com/science/article/pii/S0926580522001716</a></div>
</div>
<div id="ref-wischnewski2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">M. Wischnewski, N. Krämer, and E. Müller, <span>“Measuring and understanding trust calibrations for automated systems: A survey of the state-of-the-art and future directions,”</span> in CHI ’23. New York, NY, USA: Association for Computing Machinery, Apr. 2023, p. 116. doi: <a href="https://doi.org/10.1145/3544548.3581197">10.1145/3544548.3581197</a>. Available: <a href="https://doi.org/10.1145/3544548.3581197">https://doi.org/10.1145/3544548.3581197</a></div>
</div>
<div id="ref-devisser2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">E. J. de Visser <em>et al.</em>, <span>“Towards a Theory of Longitudinal Trust Calibration in Human<span></span>Robot Teams,”</span> <em>International Journal of Social Robotics</em>, vol. 12, no. 2, pp. 459–478, May 2020, doi: <a href="https://doi.org/10.1007/s12369-019-00596-x">10.1007/s12369-019-00596-x</a>. Available: <a href="https://doi.org/10.1007/s12369-019-00596-x">https://doi.org/10.1007/s12369-019-00596-x</a></div>
</div>
<div id="ref-muir1994" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">B. M. MUIR, <span>“Trust in automation: Part i. Theoretical issues in the study of trust and human intervention in automated systems,”</span> <em>Ergonomics</em>, vol. 37, no. 11, pp. 1905–1922, Nov. 1994, doi: <a href="https://doi.org/10.1080/00140139408964957">10.1080/00140139408964957</a>. Available: <a href="https://doi.org/10.1080/00140139408964957">https://doi.org/10.1080/00140139408964957</a></div>
</div>
<div id="ref-hancock2011" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">P. A. Hancock, D. R. Billings, K. E. Schaefer, J. Y. C. Chen, E. J. de Visser, and R. Parasuraman, <span>“A meta-analysis of factors affecting trust in human-robot interaction,”</span> <em>Human Factors</em>, vol. 53, no. 5, pp. 517–527, Oct. 2011, doi: <a href="https://doi.org/10.1177/0018720811417254">10.1177/0018720811417254</a></div>
</div>
<div id="ref-shayganfar2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">M. Shayganfar, C. Rich, C. Sidner, and B. Hylák, <span>“2019 IEEE international conference on humanized computing and communication (HCC),”</span> Sep. 2019, pp. 7–15. doi: <a href="https://doi.org/10.1109/HCC46620.2019.00010">10.1109/HCC46620.2019.00010</a>. Available: <a href="https://ieeexplore.ieee.org/document/8940829">https://ieeexplore.ieee.org/document/8940829</a></div>
</div>
<div id="ref-fartook2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">O. Fartook, Z. McKendrick, T. Oron-Gilad, and J. R. Cauchard, <span>“Enhancing emotional support in human-robot interaction: Implementing emotion regulation mechanisms in a personal drone,”</span> <em>Computers in Human Behavior: Artificial Humans</em>, vol. 4, p. 100146, May 2025, doi: <a href="https://doi.org/10.1016/j.chbah.2025.100146">10.1016/j.chbah.2025.100146</a>. Available: <a href="https://www.sciencedirect.com/science/article/pii/S2949882125000301">https://www.sciencedirect.com/science/article/pii/S2949882125000301</a></div>
</div>
<div id="ref-maure" class="csl-entry" role="listitem">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">R. Maure and B. Bruno, <span>“Autonomy in socially assistive robotics: A systematic review,”</span> <em>Frontiers in Robotics and AI</em>, vol. 12, p. 1586473, 2025, doi: <a href="https://doi.org/10.3389/frobt.2025.1586473">10.3389/frobt.2025.1586473</a>. Available: <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12491022/">https://pmc.ncbi.nlm.nih.gov/articles/PMC12491022/</a></div>
</div>
<div id="ref-bettencourt2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">C. Bettencourt <em>et al.</em>, <span>“Investigating the Feasibility of a Wizard-of-Oz Robotic Interface (R2C3) in a Social Skills Group for Children with Autism Spectrum Disorder,”</span> <em>International Journal of Social Robotics</em>, vol. 17, no. 7, pp. 1395–1411, Jul. 2025, doi: <a href="https://doi.org/10.1007/s12369-025-01243-4">10.1007/s12369-025-01243-4</a>. Available: <a href="https://doi.org/10.1007/s12369-025-01243-4">https://doi.org/10.1007/s12369-025-01243-4</a></div>
</div>
<div id="ref-nicolas2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">S. Nicolas and W. Agnieszka, <span>“The personality of anthropomorphism: How the need for cognition and the need for closure define attitudes and anthropomorphic attributions toward robots,”</span> <em>Computers in Human Behavior</em>, vol. 122, p. 106841, Sep. 2021, doi: <a href="https://doi.org/10.1016/j.chb.2021.106841">10.1016/j.chb.2021.106841</a>. Available: <a href="https://www.sciencedirect.com/science/article/pii/S0747563221001643">https://www.sciencedirect.com/science/article/pii/S0747563221001643</a></div>
</div>
<div id="ref-zhang2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">Y. Zhang <em>et al.</em>, <span>“Theory of robot mind: False belief attribution to social robots in children with and without autism,”</span> <em>Frontiers in Psychology</em>, vol. 10, Aug. 2019, doi: <a href="https://doi.org/10.3389/fpsyg.2019.01732">10.3389/fpsyg.2019.01732</a>. Available: <a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2019.01732/full">https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2019.01732/full</a></div>
</div>
<div id="ref-kok2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">B. C. Kok and H. Soh, <span>“Trust in Robots: Challenges and Opportunities,”</span> <em>Current Robotics Reports</em>, vol. 1, no. 4, pp. 297–309, Dec. 2020, doi: <a href="https://doi.org/10.1007/s43154-020-00029-y">10.1007/s43154-020-00029-y</a>. Available: <a href="https://doi.org/10.1007/s43154-020-00029-y">https://doi.org/10.1007/s43154-020-00029-y</a></div>
</div>
<div id="ref-atone2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">S. A. Atone and A. S. Bhalchandra, <span>“International Conference on Image Processing and Capsule Networks,”</span> Springer, Cham, 2022, pp. 498–522. doi: <a href="https://doi.org/10.1007/978-3-030-84760-9_43">10.1007/978-3-030-84760-9_43</a>. Available: <a href="https://link.springer.com/chapter/10.1007/978-3-030-84760-9_43">https://link.springer.com/chapter/10.1007/978-3-030-84760-9_43</a></div>
</div>
<div id="ref-wei" class="csl-entry" role="listitem">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">J. Wei <em>et al.</em>, <span>“Chain-of-thought prompting elicits reasoning in large language models,”</span> doi: <a href="https://doi.org/10.48550/arXiv.2201.11903">10.48550/arXiv.2201.11903</a></div>
</div>
<div id="ref-mcduff" class="csl-entry" role="listitem">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">D. McDuff and J. Berger, <span>“Do facial expressions predict ad sharing? A large-scale observational study,”</span> doi: <a href="https://doi.org/10.48550/arXiv.1912.10311">10.48550/arXiv.1912.10311</a></div>
</div>
<div id="ref-birnbaum2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">G. E. Birnbaum, M. Mizrahi, G. Hoffman, H. T. Reis, E. J. Finkel, and O. Sass, <span>“What robots can teach us about intimacy: The reassuring effects of robot responsiveness to human disclosure,”</span> <em>Computers in Human Behavior</em>, vol. 63, pp. 416–423, Oct. 2016, doi: <a href="https://doi.org/10.1016/j.chb.2016.05.064">10.1016/j.chb.2016.05.064</a>. Available: <a href="https://www.sciencedirect.com/science/article/pii/S0747563216303910">https://www.sciencedirect.com/science/article/pii/S0747563216303910</a></div>
</div>
<div id="ref-arkin2003" class="csl-entry" role="listitem">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">R. C. Arkin, M. Fujita, T. Takagi, and R. Hasegawa, <span>“An ethological and emotional basis for human<span></span>robot interaction,”</span> <em>Robotics and Autonomous Systems</em>, vol. 42, no. 3–4, pp. 191–201, Mar. 2003, doi: <a href="https://doi.org/10.1016/S0921-8890(02)00375-5">10.1016/S0921-8890(02)00375-5</a>. Available: <a href="https://linkinghub.elsevier.com/retrieve/pii/S0921889002003755">https://linkinghub.elsevier.com/retrieve/pii/S0921889002003755</a></div>
</div>
<div id="ref-lin2022b" class="csl-entry" role="listitem">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">T.-H. Lin, S. Ng, and S. Sebo, <span>“2022 31st IEEE international conference on robot and human interactive communication (RO-MAN),”</span> Aug. 2022, pp. 37–44. doi: <a href="https://doi.org/10.1109/RO-MAN53752.2022.9900828">10.1109/RO-MAN53752.2022.9900828</a>. Available: <a href="https://ieeexplore.ieee.org/document/9900828">https://ieeexplore.ieee.org/document/9900828</a></div>
</div>
<div id="ref-mistyrobotics" class="csl-entry" role="listitem">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">Furhat Robotics, <span>“Misty-II robot platform.”</span> 2023. Available: <a href="https://www.mistyrobotics.com">https://www.mistyrobotics.com</a></div>
</div>
<div id="ref-lin2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">T.-H. Lin, S. Ng, and S. Sebo, <span>“2022 31st IEEE international conference on robot and human interactive communication (RO-MAN),”</span> Aug. 2022, pp. 37–44. doi: <a href="https://doi.org/10.1109/RO-MAN53752.2022.9900828">10.1109/RO-MAN53752.2022.9900828</a>. Available: <a href="https://ieeexplore.ieee.org/document/9900828">https://ieeexplore.ieee.org/document/9900828</a></div>
</div>
<div id="ref-nomura" class="csl-entry" role="listitem">
<div class="csl-left-margin">[27] </div><div class="csl-right-inline">T. Nomura, T. Suzuki, T. Kanda, and K. Kato, <span>“Negative Attitudes toward Robots Scale.”</span> doi: <a href="https://doi.org/10.1037/t57930-000">10.1037/t57930-000</a></div>
</div>
<div id="ref-cacioppo1982" class="csl-entry" role="listitem">
<div class="csl-left-margin">[28] </div><div class="csl-right-inline">J. T. Cacioppo and R. E. Petty, <span>“The need for cognition,”</span> <em>Journal of Personality and Social Psychology</em>, vol. 42, no. 1, pp. 116–131, 1982, doi: <a href="https://doi.org/10.1037/0022-3514.42.1.116">10.1037/0022-3514.42.1.116</a></div>
</div>
<div id="ref-schaefer2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[29] </div><div class="csl-right-inline">K. E. Schaefer, <span>“Measuring Trust in Human Robot Interactions: Development of the <span>“</span>Trust Perception Scale-HRI<span>”</span>,”</span> R. Mittu, D. Sofge, A. Wagner, and W. F. Lawless, Eds., Boston, MA: Springer US, 2016, pp. 191–218. Available: <a href="https://doi.org/10.1007/978-1-4899-7668-0_10">https://doi.org/10.1007/978-1-4899-7668-0_10</a></div>
</div>
<div id="ref-charalambous2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[30] </div><div class="csl-right-inline">G. Charalambous, S. Fletcher, and P. Webb, <span>“The Development of a Scale to Evaluate Trust in Industrial Human-robot Collaboration,”</span> <em>International Journal of Social Robotics</em>, vol. 8, no. 2, pp. 193–209, Apr. 2016, doi: <a href="https://doi.org/10.1007/s12369-015-0333-8">10.1007/s12369-015-0333-8</a>. Available: <a href="https://doi.org/10.1007/s12369-015-0333-8">https://doi.org/10.1007/s12369-015-0333-8</a></div>
</div>
<div id="ref-wickham2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[31] </div><div class="csl-right-inline">H. Wickham <em>et al.</em>, <span>“Welcome to the Tidyverse,”</span> <em>Journal of Open Source Software</em>, vol. 4, no. 43, p. 1686, Nov. 2019, doi: <a href="https://doi.org/10.21105/joss.01686">10.21105/joss.01686</a>. Available: <a href="https://joss.theoj.org/papers/10.21105/joss.01686">https://joss.theoj.org/papers/10.21105/joss.01686</a></div>
</div>
<div id="ref-bates2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">[32] </div><div class="csl-right-inline">D. Bates, M. Mächler, B. Bolker, and S. Walker, <span>“Fitting Linear Mixed-Effects Models Using lme4,”</span> <em>Journal of Statistical Software</em>, vol. 67, pp. 1–48, Oct. 2015, doi: <a href="https://doi.org/10.18637/jss.v067.i01">10.18637/jss.v067.i01</a>. Available: <a href="https://doi.org/10.18637/jss.v067.i01">https://doi.org/10.18637/jss.v067.i01</a></div>
</div>
<div id="ref-kuznetsova2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[33] </div><div class="csl-right-inline">A. Kuznetsova, P. B. Brockhoff, and R. H. B. Christensen, <span>“<span class="nocase">lmerTest</span> package: Tests in linear mixed effects models,”</span> <em>Journal of Statistical Software</em>, vol. 82, no. 13, pp. 1–26, 2017, doi: <a href="https://doi.org/10.18637/jss.v082.i13">10.18637/jss.v082.i13</a></div>
</div>
<div id="ref-burkner2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[34] </div><div class="csl-right-inline">P.-C. Bürkner, <span>“Advanced <span>Bayesian</span> multilevel modeling with the <span>R</span> package <span class="nocase">brms</span>,”</span> <em>The R Journal</em>, vol. 10, no. 1, pp. 395–411, 2018, doi: <a href="https://doi.org/10.32614/RJ-2018-017">10.32614/RJ-2018-017</a></div>
</div>
<div id="ref-sjoberg2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[35] </div><div class="csl-right-inline">D. D. Sjoberg, K. Whiting, M. Curry, J. A. Lavery, and J. Larmarange, <span>“Reproducible summary tables with the gtsummary package,”</span> <em>The R Journal</em>, vol. 13, no. 1, pp. 570–580, Jun. 2021, doi: <a href="https://doi.org/10.32614/RJ-2021-053">10.32614/RJ-2021-053</a>. Available: <a href="https://doi.org/10.32614/RJ-2021-053/">https://doi.org/10.32614/RJ-2021-053/</a></div>
</div>
<div id="ref-lee2004" class="csl-entry" role="listitem">
<div class="csl-left-margin">[36] </div><div class="csl-right-inline">J. D. Lee and K. A. See, <span>“Trust in automation: designing for appropriate reliance,”</span> <em>Human Factors</em>, vol. 46, no. 1, pp. 50–80, 2004, doi: <a href="https://doi.org/10.1518/hfes.46.1.50_30392">10.1518/hfes.46.1.50_30392</a></div>
</div>
<div id="ref-piercy2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">[37] </div><div class="csl-right-inline">C. W. Piercy, G. Montgomery-Vestecka, and S. K. Lee, <span>“Gender and accent stereotypes in communication with an intelligent virtual assistant,”</span> <em>International Journal of Human-Computer Studies</em>, vol. 195, p. 103407, Jan. 2025, doi: <a href="https://doi.org/10.1016/j.ijhcs.2024.103407">10.1016/j.ijhcs.2024.103407</a>. Available: <a href="https://www.sciencedirect.com/science/article/pii/S1071581924001903">https://www.sciencedirect.com/science/article/pii/S1071581924001903</a></div>
</div>
<div id="ref-Chase_LangChain_2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">[38] </div><div class="csl-right-inline">H. Chase, <span>“<span>LangChain</span>.”</span> 2022. Available: <a href="https://langchain.com/">https://langchain.com/</a></div>
</div>
<div id="ref-a.v.2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">[39] </div><div class="csl-right-inline">G. A.v., M. T., P. D., and U. E., <span>“Multimodal emotion recognition with deep learning: Advancements, challenges, and future directions,”</span> <em>Information Fusion</em>, vol. 105, p. 102218, May 2024, doi: <a href="https://doi.org/10.1016/j.inffus.2023.102218">10.1016/j.inffus.2023.102218</a>. Available: <a href="https://www.sciencedirect.com/science/article/pii/S1566253523005341">https://www.sciencedirect.com/science/article/pii/S1566253523005341</a></div>
</div>
<div id="ref-python" class="csl-entry" role="listitem">
<div class="csl-left-margin">[40] </div><div class="csl-right-inline">Python Software Foundation, <span>“Python language reference, version 3.10.”</span> <a href="https://www.python.org" class="uri">https://www.python.org</a>, 2023.</div>
</div>
<div id="ref-misty" class="csl-entry" role="listitem">
<div class="csl-left-margin">[41] </div><div class="csl-right-inline"><span>“Misty.”</span> Available: <a href="https://docs.mistyrobotics.com/">https://docs.mistyrobotics.com/</a></div>
</div>
<div id="ref-deepgram" class="csl-entry" role="listitem">
<div class="csl-left-margin">[42] </div><div class="csl-right-inline">Deepgram, <span>“Deepgram speech recognition API.”</span> <a href="https://deepgram.com" class="uri">https://deepgram.com</a>, 2024.</div>
</div>
<div id="ref-duckdb2026" class="csl-entry" role="listitem">
<div class="csl-left-margin">[43] </div><div class="csl-right-inline">H. Mühleisen and M. Raasveldt, <em>Duckdb: DBI package for the DuckDB database management system</em>. 2026. Available: <a href="https://r.duckdb.org/">https://r.duckdb.org/</a></div>
</div>
<div id="ref-gemini" class="csl-entry" role="listitem">
<div class="csl-left-margin">[44] </div><div class="csl-right-inline">Google, <span>“Gemini API.”</span> <a href="https://ai.google.dev" class="uri">https://ai.google.dev</a>, 2024.</div>
</div>
<div id="ref-lin2022a" class="csl-entry" role="listitem">
<div class="csl-left-margin">[45] </div><div class="csl-right-inline">T.-H. Lin, S. Ng, and S. Sebo, <span>“2022 31st IEEE international conference on robot and human interactive communication (RO-MAN),”</span> Aug. 2022, pp. 37–44. doi: <a href="https://doi.org/10.1109/RO-MAN53752.2022.9900828">10.1109/RO-MAN53752.2022.9900828</a>. Available: <a href="https://ieeexplore.ieee.org/document/9900828">https://ieeexplore.ieee.org/document/9900828</a></div>
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>