<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>old_text</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="old_text_files/libs/clipboard/clipboard.min.js"></script>
<script src="old_text_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="old_text_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="old_text_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="old_text_files/libs/quarto-html/popper.min.js"></script>
<script src="old_text_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="old_text_files/libs/quarto-html/anchor.min.js"></script>
<link href="old_text_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="old_text_files/libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="old_text_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="old_text_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="old_text_files/libs/bootstrap/bootstrap-9e3ffae467580fdb927a41352e75a2e0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="project-description" class="level2">
<h2 class="anchored" data-anchor-id="project-description">Project Description</h2>
<p>As many industries move toward greater automation (e.g., mining, manufacturing), robotic assistants are becoming increasingly common in operational workflows from manufacturing to mining <span class="citation" data-cites="fu2021 racette2024">[@fu2021; @racette2024]</span>. These systems can help reduce human exposure to hazards, but trust remains a critical barrier to adoption <span class="citation" data-cites="campagna2025 emaminejad2022">[@campagna2025; @emaminejad2022]</span>. Low trust can lead to rejection of automation, while excessive trust (automation bias) risks overreliance <span class="citation" data-cites="devisser2020">[@devisser2020]</span>. While most industrial robots rely on fixed, rule-based behaviour, there is growing interest in AI-enhanced systems that support more adaptive and socially intelligent interactions.</p>
<p>Research shows that emotional responsiveness in robots—expressed through tone, facial expression, or supportive dialogue—can improve perceptions of trustworthiness and social intelligence <span class="citation" data-cites="shayganfar2019 fartook2025">[@shayganfar2019; @fartook2025]</span>. Moreover, a robot’s ability to detect early signs of stress, fatigue, or cognitive overload may have practical value in high-risk environments such as underground mines, where attentional lapses can pose serious safety risks <span class="citation" data-cites="racette2024 murphy2009">[@racette2024; @murphy2009]</span>. Responsive adaptation in this context could enhance both trust and real-time situational awareness <span class="citation" data-cites="fu2021 murphy2009 racette2024">[@fu2021; @murphy2009; @racette2024]</span>.</p>
<p>This project aims to develop and evaluate a multi-sensor Emotion-Adaptive AI System (EAI) for the Misty II robot <span class="citation" data-cites="mistya">[@mistya]</span>. The EAI will use AI-driven affect-inference models including facial emotion recognition models, vocal tone analysis, combined with a generative language model to coordinate task interaction monitoring to estimate user affective states such as frustration, confusion, or disengagement. These inferences will guide Misty’s real-time responses, including encouragement, clarification prompts, or praise. We will compare this adaptive mode against a neutral, rule-based version of Misty to assess effects on user trust, collaboration, and task performance. We will also explore how individual cognitive traits may moderate these interactions <span class="citation" data-cites="nicolas2021">[@nicolas2021]</span>.</p>
</section>
<section id="pilot-study" class="level1">
<h1>Pilot Study</h1>
<p>Human–robot collaboration (HRC) has become a central topic across engineering, computer science, and the social sciences as robots increasingly move from controlled laboratory settings into everyday collaborative roles. In many emerging applications, collaboration depends not only on physical coordination but also on shared problem-solving through dialogue, where AI-driven robots must reason, communicate, and adapt in real time. Understanding how humans perceive and respond to such systems is therefore critical for designing robots that can function as effective collaborators rather than passive tools.</p>
<p>A key factor shaping successful collaboration in human–robot interaction (HRI) is trust <span class="citation" data-cites="waytz2014 emaminejad2022">[@waytz2014; @emaminejad2022]</span>. Trust influences whether users are willing to adopt robotic systems, accept their guidance, and remain engaged during joint tasks, particularly in situations characterized by uncertainty or incomplete information <span class="citation" data-cites="waytz2014 emaminejad2022">[@waytz2014; @emaminejad2022]</span>. Prior work has shown that trust affects both subjective perceptions such as perceived reliability or intent, and objective outcomes including task performance, compliance, and cooperation. As a result, a substantial body of research has focused on measuring trust in HRI, leading to the development of standardized instruments for assessing users’ evaluations of robot behaviour across industrial, medical, and social contexts.</p>
<p>Despite this progress, much of the existing literature on trust in HRI is based on interactions conducted under highly controlled or simulated conditions. In many studies, robot behaviour is scripted, partially simulated, or mediated through Wizard-of-Oz paradigms, where a human operator covertly controls aspects of the robot’s behaviour. While these approaches are valuable for isolating specific design factors and testing early hypotheses, they also mask many of the failures and inconsistencies that characterize autonomous systems in real-world use. Speech recognition errors, delayed or inappropriate responses, misinterpretations of user intent, and limitations of affect sensing are not peripheral issues but central features of deployed autonomous robots. These imperfections are likely to play a decisive role in shaping trust, yet they remain underexplored in empirical HRI research.</p>
<p>Introduce the concept of responsiveness in robot/AI systems as a moderator of trust. Prior work has suggested that robots that can adapt their behaviour based on user affect and contextual cues may foster greater trust and engagement. However, most studies examining responsiveness have relied on simulated or semi-autonomous systems, leaving open questions about how these effects manifest in fully autonomous, in-person interactions. While most industrial robots rely on fixed, rule-based behaviour, there is growing interest in AI-enhanced systems that support more adaptive and socially intelligent interactions <span class="citation" data-cites="fu2021 murphy2009 racette2024">[@fu2021; @murphy2009; @racette2024]</span>.</p>
<p>The present pilot study addresses this gap by examining trust and collaboration in an in-person interaction with two versions of a fully autonomous social robot operating within predefined behavioural constraints. Using a between-subjects design, participants collaborated with either a responsive or neutral robot during an immersive, dialogue-driven puzzle game in which the robot acted as a diegetic game guide and partner. The task required shared problem-solving through conversation, with participants seeking hints, advice, and support from the robot while navigating a game environment displayed on a computer screen. Crucially, all interaction management—including speech-based dialogue, task progression, and affect-responsive behaviour was handled autonomously by the robot without human intervention.</p>
<p>In the experimental condition, the robot was designed to be proactive and responsive, adapting its behaviour based on participant affect–as estimated from outputs of an affect detection model–as well as conversational cues. In the other condition, the robot provided assistance only when explicitly requested, offering a more reactive interaction style. This manipulation allowed us to examine how differences in autonomy and responsiveness to human states influence trust perceptions and collaborative performance under otherwise identical task demands.</p>
<p>To support this interaction, we developed an autonomous spoken-language system integrated with automatic speech recognition (ASR) and affect detection on the Misty-II robot platform. The system we developed enables the robot to recognize speech, manage dialogue state, maintain conversational context, and generate coordinated verbal responses alongside LED facial expressions and head and arm movements. Rather than optimizing for flawless performance, the system was designed to reflect realistic capabilities and limitations of contemporary social robots.</p>
<p>By combining post-interaction trust measures with behavioural and task-level outcomes, this study aims to contribute empirical evidence on how trust is shaped in fully autonomous HRI scenarios. The focus is not on demonstrating idealized interaction under perfect conditions, but on examining trust as it emerges through realistic human–robot collaboration, where uncertainty, interactional breakdowns, and adaptive behaviour are unavoidable. In doing so, this work seeks to inform the design and evaluation of affect-responsive autonomous robots intended for real-world collaborative settings.</p>
<p>This project aimed to develop and evaluate a multi-sensor Emotion-Adaptive AI System (EAI) for the Misty II robot <span class="citation" data-cites="mistya">[@mistya]</span>. The EAI leveraged pretrained LLMs for syntaxical affect inference and task interaction monitoring to estimate user affective states such as frustration, engagement, or anxiousness. These inferences guided Misty’s real-time responses, including encouragement, clarification prompts, and praise. We compared an adaptive version of Misty against a neutral, rule-based version of Misty to assess effects on user trust, collaboration, and perceived intelligence. We also explored how individual differences in Need for Cognition could moderate these interactions <span class="citation" data-cites="nicolas2021">[@nicolas2021]</span>.</p>
<p>[also mention the pre-session questionnaire with NARS and NFC to capture baseline attitudes and individual differences that may moderate trust responses] [also mention that this is a pilot study to inform a larger planned study with more participants and refined methods based on lessons learned here and touch on some of the lessons we learned (i.e., language issues, ASR issues, interaction design issues, etc.)]</p>
<section id="related-work" class="level2">
<h2 class="anchored" data-anchor-id="related-work">Related work</h2>
<section id="spoken-language-interaction-in-hri" class="level3">
<h3 class="anchored" data-anchor-id="spoken-language-interaction-in-hri">Spoken language interaction in HRI</h3>
<p>Discuss prior work on spoken language interaction in HRI, including the challenges of ASR, dialogue management, and affect detection in real-world settings. Summarize key findings from studies that have examined how spoken language capabilities influence user perceptions of robots, particularly in collaborative tasks. Highlight any gaps in the literature regarding fully autonomous systems operating in-person without human intervention.</p>
<p>While most industrial robots rely on fixed, rule-based behaviour, there is growing interest in AI-enhanced systems that support more adaptive and socially intelligent interactions <span class="citation" data-cites="fu2021 murphy2009 racette2024">[@fu2021; @murphy2009; @racette2024]</span>. Discuss the technical challenges of implementing spoken language interaction on mobile robot platforms, including issues related to speech recognition accuracy, latency, and contextual understanding. Explain how these challenges impact user perceptions and trust in HRI.</p>
</section>
<section id="trust-in-hri-research" class="level3">
<h3 class="anchored" data-anchor-id="trust-in-hri-research">Trust in HRI research</h3>
<p>Research shows that responsiveness to human-affect in robots—expressed through tone, movement, facial expression, or supportive dialogue—can improve perceptions of trustworthiness and social intelligence <span class="citation" data-cites="shayganfar2019 fartook2025">[@shayganfar2019; @fartook2025]</span>. These anthropomorphic cues may serve as transparency signals, helping users infer robotic intent and fostering cooperation <span class="citation" data-cites="birnbaum2016">[@birnbaum2016]</span>.</p>
<p>A robot’s ability to detect and respond to early signs of stress, fatigue, confusion or cognitive overload could have practical value in high-risk environments like medical settings or underground mines where attentional lapses pose serious safety risks. Affecctive adaptation in these contexts could enhance both trust as well as real-time responsiveness and situational awareness <span class="citation" data-cites="fu2021 murphy2009 racette2024">[@fu2021; @murphy2009; @racette2024]</span>.</p>
<p>Introduce the two scales here we used to measure trust in HRI: adapted 14-item Trust Perception Scale-HRI <span class="citation" data-cites="bartneck2009">[@bartneck2009]</span> and the Trust in Industrial Human–Robot Collaboration scale <span class="citation" data-cites="charalambous">[@charalambous]</span>. Discuss prior work that has used these scales and their relevance to our study. The differences between these two scales in terms of what aspects of trust they measure (e.g., affective vs.&nbsp;cognitive trust, reliability vs.&nbsp;collaboration). In addition, discuss how these scales have been validated in prior research and their psychometric properties.</p>
<p>Also talk about using the Negative Attitudes Toward Robots Scale (NARS) <span class="citation" data-cites="nomura2006">[@nomura2006]</span> and the Need for Cognition Scale (NFC-s) <span class="citation" data-cites="cacioppo1984">[@cacioppo1984, @cacioppo1996]</span> as baseline measures to account for individual differences in attitudes toward robots and cognitive engagement that may influence trust perceptions.</p>
<p>Explain why we chose to use both scales in our study to capture a comprehensive picture of trust in HRI. The reasons for selecting these scales should be linked to our research questions and hypotheses about how robot responsiveness and affective adaptation might influence different dimensions of trust.</p>
</section>
<section id="responsiveness-and-affective-adaptation-in-hri" class="level3">
<h3 class="anchored" data-anchor-id="responsiveness-and-affective-adaptation-in-hri">Responsiveness and affective adaptation in HRI</h3>
<p>Discuss prior research on the role of robot responsiveness and affective adaptation in shaping trust and engagement in HRI. Summarize key findings from studies that have examined how robots that can perceive and respond to human affective states influence user perceptions and behaviours. Highlight any gaps in the literature, particularly regarding fully autonomous, in-person interactions, which our study aims to address.</p>
<p>Discuss theoretical frameworks that explain why responsiveness and affective adaptation might enhance trust, such as social presence theory or the CASA paradigm. Explain how these frameworks inform our hypotheses about the expected effects of robot interaction policy on trust and collaboration.</p>
<p>Discuss the reason for conducting the study in a fully autonomous, in-person setting rather than using Wizard-of-Oz or simulated paradigms. Emphasize the importance of examining trust in realistic conditions where interaction imperfections are present.</p>
</section>
<section id="why-a-pilot-study" class="level3">
<h3 class="anchored" data-anchor-id="why-a-pilot-study">Why a pilot study?</h3>
<p>Why did we run a pilot study? what future work will this inform? Focus on testing feasibility of the autonomous system, interaction design, and measurement approach.</p>
<p>Our initial objectives: 1. Develop and implement an autonomous Emotion-Adaptive AI System (EAI) using Misty’s onboard sensor data and AI on an edge device. 2. Evaluate the impact of EAI on trust, perceived social intelligence, and task collaboration in two types of human-robot tasks. 3. Assess whether individual differences in human cognitive style (i.e., Need for Cognition) influences responses to adaptive robotic behaviour and trust in robots (before and after interaction)</p>
<p>What we did</p>
<p>We developed an autonomous spoken-language interaction system integrated with a prompted ASR pipeline and the Misty-II robot platform that can engage in natural conversations with users. The system is capable of recognizing speech, managing dialogue, and generating spoken responses.</p>
<p>This project aimed to develop and evaluate a multi-sensor Emotion-Adaptive AI System (EAI) for the Misty II robot <span class="citation" data-cites="mistya">[@mistya]</span>. The EAI leveraged pretrained LLMs for syntaxical affect inference and task interaction monitoring to estimate user affective states such as frustration, engagement, or anxiousness. These inferences guided Misty’s real-time responses, including encouragement, clarification prompts, and praise. We compared an adaptive version of Misty against a neutral, rule-based version of Misty to assess effects on user trust, collaboration, and perceived intelligence. We also explored how individual differences in Need for Cognition could moderate these interactions <span class="citation" data-cites="nicolas2021">[@nicolas2021]</span>.</p>
<p>Discuss pilot studies or preliminary work that informed the design of our robot interaction policies. This could include prior experiments with semi-autonomous systems, user feedback on robot behaviours, or technical evaluations of affect detection models.</p>
</section>
</section>
</section>
<section id="methods" class="level1">
<h1>Methods</h1>
<p>The primary objective of this study was to examine how differences in robot interaction policy influence trust and collaboration during fully autonomous, in-person human–robot interaction. Based on prior work linking robot responsiveness, affective behavior, and trust in HRI, we formulated the following hypotheses.</p>
<section id="hypotheses" class="level2">
<h2 class="anchored" data-anchor-id="hypotheses">Hypotheses</h2>
<p>H1: Participants interacting with a responsive, affect-adaptive robot will report higher post-interaction trust than participants interacting with a neutral, reactive robot.</p>
<p>H2: Participants in the responsive condition will demonstrate greater engagement with the robot during the collaborative tasks, reflected in increased voluntary interaction and reliance on robot input during problem solving.</p>
<p>Fix H3–we didn’t actually test this directly H3: Differences in trust and engagement will be most pronounced during the open-ended collaborative task, where assistance from the robot is optional rather than required.</p>
</section>
<section id="experimental-conditions" class="level2">
<h2 class="anchored" data-anchor-id="experimental-conditions">Experimental Conditions</h2>
<p>The study employed a between-subjects design with robot interaction policy as the sole experimental factor. Participants interacted with the same Misty-II robot in a shared physical workspace that included both the robot and a participant-facing computer interface. The interface was used to present brief task instructions, collect participant inputs, and manage transitions between task stages. Critically, the interface did not serve as a control mechanism for the robot. Instead, the robot autonomously monitored task state and participant inputs via the interface and input from the participant. Dialogue and behavior were adapted accordingly, without any real-time human intervention (see <a href="#fig-setup" class="quarto-xref">Figure&nbsp;1</a>).</p>
<div id="fig-setup" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-setup-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/misty-pullback.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="482">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-setup-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Experimental setup showing the autonomous robot and participant-facing task interface used during in-person sessions. Participants entered task responses and navigated between task stages using the interface, while the robot autonomously tracked task state and adapted its interaction based on participant input. No real-time human intervention occurred during the interaction.
</figcaption>
</figure>
</div>
<p>Participants collaborated with the robot during an immersive puzzle game in which the robot functioned as a diegetic game guide and collaborative partner. The interaction was fully autonomous in both conditions, and both versions of the robot were subject to the same sensory and interaction constraints inherent to real-world operation, including speech recognition variability and response timing delays. The only manipulation between conditions was the robot’s interaction policy.</p>
<p>During initial sign-up, participants were randomly assigned to one of two conditions by the Qualtrics software used for recruitment. Balanced random assignment was employed to ensure equal group sizes, however due to no-shows, last-minute cancellations and technical difficulties (see below), final group sizes were n = 14 in the RESPONSIVE condition and n = 9 in the CONTROL condition.:</p>
<p>RESPONSIVE (experimental): The robot adopted a warm, emotionally engaged, and proactive interaction style, adapting its responses based on detected participant affect, dialogue context, and task demands.</p>
<p>CONTROL (baseline): The robot followed a neutral, reactive interaction policy, providing information and assistance only when explicitly requested, without affect-responsive adaptation.</p>
</section>
<section id="game-design" class="level2">
<h2 class="anchored" data-anchor-id="game-design">Game Design</h2>
<p>The game contained two timed puzzles embedded within five sequential stages designed to elicit interaction with the robot under differing collaboration and dependency conditions, following established approaches in HRI task design <span class="citation" data-cites="lin2022">[@lin2022]</span>. To solve each puzzle, participants needed to engage in logical reasoning, deduction, pattern recognition, and information synthesis. The tasks provided an immersive narrative context in which participants assumed the role of a detective investigating the disappearance of a fellow robot colleague. The robot served as both a dietic guide and a collaborative partner, providing hints, clarifications, and encouragement that depended on experimental condition.</p>
<p>To create a smooth difficulty curve with respect to the time limits of 5 and 15 minutes, Task 2 was more difficult than Task 1, and the overall experience was designed such that it would be difficult to solve both tasks in time without the game guide’s help.</p>
<p>Stage 1: Greeting. The robot introduced itself and engaged in brief rapport-building interaction.</p>
<p>Stage 2: Mission brief. The robot explained the narrative context and overall objectives of the task.</p>
<p>Stage 3: Task 1 (robot-dependent reasoning). Participants completed a constrained “who-dunnit” task where participants could ask the robot .</p>
<p>Stage 4: Task 2 (open-ended collaborative problem solving). Participants worked to determine the location of the missing robot using technical logs.</p>
<p>Stage 5: Wrap-up. The robot provided feedback and concluded the interaction.</p>
<p>Mention that all dialogue was logged etc.Participants could move to the next stage by clicking buttons on their dashboard as directed by the robot or at their own volition. Total session duration was approximately 15 minutes.</p>
<section id="task-1-robot-dependent-collaborative-reasoning" class="level3">
<h3 class="anchored" data-anchor-id="task-1-robot-dependent-collaborative-reasoning">Task 1: Robot-dependent collaborative reasoning</h3>
<p>In the first task, participants were required to identify a suspect from a 6 × 4 grid of 24 candidates by asking the robot a series of yes/no questions about the suspect’s features (e.g., hair color, accessories, clothing). The grid was displayed on the interface, while questions were posed verbally to the robot. The robot possessed the ground-truth information necessary to evaluate each question and provide correct responses.</p>
<div id="fig-task1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-task1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/task1-whodunnit.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="482">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-task1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: In the first task, participants were required to identify a suspect from a 6 × 4 grid of 24 candidates by asking the robot a series of yes/no questions about the suspect’s features (e.g., hair color, accessories, clothing). The grid was displayed on the interface, while questions were posed verbally to the robot. Participants could track those eliminated here and input their final answer.
</figcaption>
</figure>
</div>
<p>Successful completion of this task was therefore dependent on interaction with the robot, creating a forced collaborative dynamic in which the robot served as an essential informational partner. Participants were required to coordinate questioning strategies with the robot to narrow down the correct suspect within a five-minute time limit. The structured nature of the task ensured consistent interaction demands across participants and conditions.</p>
</section>
<section id="task-2-open-ended-problem-solving-with-advisory-robot-support" class="level3">
<h3 class="anchored" data-anchor-id="task-2-open-ended-problem-solving-with-advisory-robot-support">Task 2: Open-ended problem solving with advisory robot support</h3>
<p>The second task involved a more open-ended problem-solving scenario. Participants were presented with multiple technical logs through a simulated terminal interface that could be used to determine the location of the missing robot. Unlike Task 1, the robot <em>did not</em> have access to ground-truth information or the contents of the logs. The robot’s assistance was limited to general problem-solving support derived from its language model, such as explaining how to interpret logs, suggesting reasoning strategies, or prompting participants to reflect on inconsistencies.</p>
<div id="fig-task2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-task2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/task2-cryptic-puzzle.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="482">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-task2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: The task 2 interface presented multiple technical logs through a simulated terminal interface that could be used to determine the location of the missing robot.
</figcaption>
</figure>
</div>
<p>Participants could complete this task independently or choose to solicit assistance from the robot. The robot could ask clarifying questions about what the participant observed in the logs, and participants could likewise ask the robot for guidance. This design positioned the robot as a collaborative reasoning partner rather than an authoritative source and allowed collaboration to emerge voluntarily rather than being enforced by task structure <span class="citation" data-cites="lin2022">[@lin2022]</span>.</p>
</section>
</section>
<section id="study-protocol" class="level2">
<h2 class="anchored" data-anchor-id="study-protocol">Study Protocol</h2>
<p>In person sessions were conducted between November __ and November __ at Laurentian, with a single supplementary session conducted on December 15. Before arrival for their scheduled session, the interaction policy was set to either RESPONSIVE or CONTROL condition to ensure the correct robot behaviour was produced during the session.</p>
<p>In-person sessions were conducted in a quiet, private room equipped with the Misty-II robot and a participant-facing computer interface. Upon arrival, participants were greeted by the researcher, who provided a brief overview of the session, confirmed consent and answered any questions.</p>
<p>At the start of the in-person session, the experimenter seated participants in front of the Misty-II robot, then gave brief guidance on effective communication with the robot, including waiting for a visual indicator on the robot before speaking (blue light). Once participants indicated readiness, participants were instructed they could begin the interaction by clicking a start button on the interface once the experimenter left the room and closed the door, leaving the participant and robot to complete the tasks without human presence or direct observation. Participants were informed that they could terminate the session at any time without penalty.</p>
<p>Following task completion, participants exited the room and completed a post-interaction survey assessing trust using the Trust Perception–HRI scale and the Trust in Industrial Human–Robot Collaboration scale. Participants then engaged in a written and verbal debrief with the researcher. All participants completed the full procedure, with total session duration averaging approximately 30 minutes, and received a $15.00 gift card as compensation for their time.</p>
</section>
<section id="measures" class="level2">
<h2 class="anchored" data-anchor-id="measures">Measures</h2>
<p>A combination of self-report questionnaires and behavioural metrics were used to assess trust, engagement, and task performance.</p>
<p>Self-report</p>
<ol type="1">
<li><p>Pre-session questionairenaire: Participants completed baseline measures including the Negative Attitudes Toward Robots Scale (NARS) <span class="citation" data-cites="nomura2006">[@nomura2006]</span> and the short form of the Need for Cognition scale (NFC-s) <span class="citation" data-cites="cacioppo1984">[@cacioppo1984, @cacioppo1996]</span> to assess individual differences that may moderate trust responses. The pre-session questionnaire included basic demographic items, the Negative Attitudes Toward Robots Scale (NARS), and the short form of the Need for Cognition scale (NFC-s). Eligibility criteria related to English fluency were applied to ensure that participants could reliably navigate the task interface and communicate with the robot using the automatic speech recognition (ASR) system during spoken interaction. Prior experience interacting with robots was not an exclusion criterion; instead, self-reported familiarity with robots was treated as a covariate in subsequent analyses. Participants’ personality traits, background experience with robots, and demographic data were recorded in the survey as potential covariates.</p></li>
<li><p>Post-interaction trust measures: Two validated trust scales were administered after the interaction: The 14-item Trust Perception Scale-HRI <span class="citation" data-cites="bartneck2009">[@bartneck2009]</span> and the Trust in Industrial Human–Robot Collaboration scale <span class="citation" data-cites="charalambous">[@charalambous]</span>. Both scales assess multiple dimensions of trust, including reliability, competence, and collaboration.</p></li>
</ol>
<p>Objective</p>
<ol start="4" type="1">
<li><p>Objective measures of task performance included the number of correctly solved puzzles, time taken to complete each task, and the number of assistance requests made to the robot.</p></li>
<li><p>Behavioural metrics: Interaction logs were manually coded and analyzed to extract objective measures of engagement and AI performance, task performance, including the number of dialogue turns, frequency of communication issues (e.g., sentence fragments or language issues), frequency of help requests, response times, whether the AI reemained on task and provided useful information etc.</p></li>
</ol>
</section>
<section id="sample-and-recruitment" class="level2">
<h2 class="anchored" data-anchor-id="sample-and-recruitment">Sample and Recruitment</h2>
<p>29 participants were recruited from the Laurentian University community through direct recruitment, word of mouth and the SONA participant recruitment system. Initial enrollment was conducted via a Qualtrics questionnaire. Interested individuals first completed an eligibility screening confirming that they were adults (18 years or older), fluent in spoken and written English, and had normal or corrected-to-normal hearing and vision. Eligible participants then provided informed consent, selected an in-person session time, and completed a pre-session questionnaire. Sample characteristics are reported in <strong>?@tbl-pre</strong>.</p>
<p>Participants received a $15.00 gift card as compensation for their time. All study procedures were approved by Laurentian University Research Ethics Board (REB #6021966). The Misty-II robot used in this study was purchased through grant funding from the IAMGOLD President’s Innovation Fund.</p>
<p><strong>Data exclusions and communication viability</strong></p>
<p>Although participants were required to be fluent in spoken English, in-person observation on meeting participants as well as post-hoc review of interaction transcripts revealed that a small subset of participants experienced persistent communication breakdowns caused by language barriers that prevented meaningful engagement with the robot. These breakdowns were characterized by repeated speech recognition failures, fragmented responses, and task abandonment.</p>
<p>Because the experimental manipulation relied on language-mediated collaboration, the most extreme sessions were considered non-compliant with the intended protocol and were excluded from task-level analyses. Importantly, exclusion criteria were based on interaction viability rather than outcome measures, and results were qualitatively unchanged when these sessions were retained.</p>
<p><strong>Sample eligibility and protocol adherence.</strong> Participants were required to be fluent in spoken English. During administration, the experimenter recorded contemporaneous notes when conversational fluency appeared insufficient to support the language-mediated protocol. Communication viability was later assessed using objective interaction indicators extracted from system logs and manually coded transcripts (e.g., repeated speech-recognition failures, fragmented utterances, and task abandonment). Sessions showing sustained communication breakdown were treated as protocol non-adherence and were excluded from task-level analyses (n=6). All analyses are also reported with these sessions retained as a sensitivity check.</p>
<p>Participant characteristics and baseline measures</p>
<p>Participants in the control and responsive conditions were comparable with respect to pre-interaction demographic characteristics, academic background, prior experience with robots, and baseline attitudes toward robots. Importantly, Negative Attitudes Towards Robots (NARS) and Need for Cognition scores were similar across groups, indicating that post-interaction differences are unlikely to reflect pre-existing attitudes (see <strong>?@tbl-pre</strong>). [add stats here]</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>