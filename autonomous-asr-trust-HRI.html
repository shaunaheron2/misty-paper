<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Autonomous spoken-language interaction and trust in robotic systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="autonomous-asr-trust-HRI_files/libs/clipboard/clipboard.min.js"></script>
<script src="autonomous-asr-trust-HRI_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="autonomous-asr-trust-HRI_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="autonomous-asr-trust-HRI_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="autonomous-asr-trust-HRI_files/libs/quarto-html/popper.min.js"></script>
<script src="autonomous-asr-trust-HRI_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="autonomous-asr-trust-HRI_files/libs/quarto-html/anchor.min.js"></script>
<link href="autonomous-asr-trust-HRI_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="autonomous-asr-trust-HRI_files/libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="autonomous-asr-trust-HRI_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="autonomous-asr-trust-HRI_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="autonomous-asr-trust-HRI_files/libs/bootstrap/bootstrap-9e3ffae467580fdb927a41352e75a2e0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Autonomous spoken-language interaction and trust in robotic systems</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="proponents" class="level1">
<h1>Proponents:</h1>
<ul>
<li><strong>Student:</strong> Shauna Heron</li>
<li><strong>Faculty/Supervisor:</strong> Dr.&nbsp;Meng Cheng Lau, School of Engineering and Computer Science / Director of Laurentian Intelligent Mobile Robotic Lab (LIMRL)</li>
</ul>
<p>Keywords: human-robot interaction, affect-detection, responsive robotics, socially assistive robotics, cobots, level of autonomy, artificial intelligence, autonomous robot systems, trust in automation, large language models, spoken language interaction</p>
<section id="project-description" class="level2">
<h2 class="anchored" data-anchor-id="project-description">Project Description</h2>
<p>Autonomy in socially-assistive robotics?</p>
<p>Need to stress autonomous interaction: While majority of related robotics work relies on simulation and the Wizard-of-Oz (WoZ) paradigm which gives the illusion of interactivity and intelligence, thanks to advancing technology, a transition toward more autonomous robots can be observed. We take the autonomousu approach. <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12491022/#B40" class="uri">https://pmc.ncbi.nlm.nih.gov/articles/PMC12491022/#B40</a></p>
<p>The authors use <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12491022/#B7">Beer et al.&nbsp;(2014)</a>’s definition of autonomy: “The extent to which a robot can sense its environment, plan based on that environment, and act upon that environment with the intent of reaching some task-specific goal without external control.”</p>
<p>As global industry moves toward greater automation, robotic assistants are becoming increasingly common in a variety of commercial, clinical and industrial settings and workflows. Though these systems have shown some success in… ADD MORE STUDIES AND EXAMPLES HERE reduce human exposure to hazards, trust remains a critical barrier to adoption <span class="citation" data-cites="waytz2014 emaminejad2022">[@waytz2014; @emaminejad2022]</span>. Research suggests that low trust can lead to rejection of automation, while excessive trust (automation bias) risks overreliance <span class="citation" data-cites="msosa2023">[@msosa2023]</span>. While most industrial robots rely on fixed, rule-based behaviour, there is growing interest in AI-enhanced systems that support more adaptive and socially intelligent interactions <span class="citation" data-cites="fu2021 murphy2009 racette2024">[@fu2021; @murphy2009; @racette2024]</span>.</p>
<p>Research shows that responsiveness to human-affect in robots—expressed through tone, movement, facial expression, or supportive dialogue—can improve perceptions of trustworthiness and social intelligence <span class="citation" data-cites="shayganfar2019 fartook2025">[@shayganfar2019; @fartook2025]</span>. These anthropomorphic cues may serve as transparency signals, helping users infer robotic intent and fostering cooperation <span class="citation" data-cites="birnbaum2016">[@birnbaum2016]</span>.</p>
<p>A robot’s ability to detect and respond to early signs of stress, fatigue, confusion or cognitive overload could have practical value in high-risk environments like medical settings or underground mines where attentional lapses pose serious safety risks. Affecctive adaptation in these contexts could enhance both trust as well as real-time responsiveness and situational awareness <span class="citation" data-cites="fu2021 murphy2009 racette2024">[@fu2021; @murphy2009; @racette2024]</span>.</p>
<p>This project aimed to develop and evaluate a multi-sensor Emotion-Adaptive AI System (EAI) for the Misty II robot <span class="citation" data-cites="mistya">[@mistya]</span>. The EAI leveraged pretrained LLMs for syntaxical affect inference and task interaction monitoring to estimate user affective states such as frustration, engagement, or anxiousness. These inferences guided Misty’s real-time responses, including encouragement, clarification prompts, and praise. We compared an adaptive version of Misty against a neutral, rule-based version of Misty to assess effects on user trust, collaboration, and perceived intelligence. We also explored how individual differences in Need for Cognition could moderate these interactions <span class="citation" data-cites="nicolas2021">[@nicolas2021]</span>.</p>
</section>
<section id="objectives" class="level2">
<h2 class="anchored" data-anchor-id="objectives">Objectives</h2>
<ol type="1">
<li>Develop and implement an autonomous Emotion-Adaptive AI System (EAI) using Misty’s onboard sensor data and AI on an edge device.</li>
<li>Evaluate the impact of EAI on trust, perceived social intelligence, and task collaboration in two types of human-robot tasks.</li>
<li>Assess whether individual differences in human cognitive style (i.e., Need for Cognition) influences responses to adaptive robotic behaviour and trust in robots (before and after interaction)</li>
</ol>
</section>
<section id="methodology" class="level2">
<h2 class="anchored" data-anchor-id="methodology">Methodology</h2>
<section id="participants" class="level3">
<h3 class="anchored" data-anchor-id="participants">Participants</h3>
<p>We recruited 29 adults from Laurentian University. Ethics approval was obtained prior to participant recruitment.</p>
</section>
<section id="experimental-design" class="level3">
<h3 class="anchored" data-anchor-id="experimental-design">Experimental Design</h3>
<p>A between-subjects design varied robot behaviour (EAI-enabled vs.&nbsp;EAI-disabled (control)) and task type (robot-dependent vs.&nbsp;robot-independent). Each participant completed two collaborative tasks with either EAI Misty or Neutral Misty:</p>
<ul>
<li><strong>Whodunnit Task (structured):</strong> human solves a who dunnit type task by asking Yes/No questions (process of elimination in 6x4 suspect grid) to the robot. Robot knows ground truth features of the suspect but can only answer Yes/No questions about suspect features. Can not directly describe the suspect or name them. (human can choose a random suspect to solve on their own but only 1 in 24 chance of being correct without robot help).
<ul>
<li>Designed to force information seeking from robot.</li>
</ul></li>
<li><strong>Where is? Task (unstructured):</strong> Human collaborates with robot to find a missing robot by deciphering cryptic system and sensor logs. Robot does not know the answer here and can only guide the human usinng its expertise and knowledge of computer systems and basic logical reasoning. (human can solve on their own but very difficult without robot help depending on participants technical background).
<ul>
<li>Designed to elicit moderate frustration, this scenario is ideal for assessing trust, help-seeking, and compliance.</li>
</ul></li>
</ul>
<p>Misty adapts its real-time behaviour based on dialogue, questions, user affect in the EAI condition. In the neutral condition, Misty follows a fixed interaction script with consistent, non-adaptive responses. Bring in the role modes here.</p>
</section>
<section id="measures" class="level3">
<h3 class="anchored" data-anchor-id="measures">Measures</h3>
<p>We collected behavioural, self-report, and performance data:</p>
<ul>
<li><p><strong>Trust and responsiveness was</strong> measured using:</p></li>
<li><p>The (adapted) 14-item Trust Perception Scale-HRI <span class="citation" data-cites="bartneck2009">[@bartneck2009]</span>.</p></li>
<li><p>Trust in Industrial Human-Robot Collaboration <span class="citation" data-cites="charalambous">[@charalambous]</span>.</p></li>
<li><p><strong>Engagement and behaviour:</strong> Help-seeking, adherence, affect</p></li>
<li><p><strong>Task Performance:</strong> Accuracy, fluency, task duration</p></li>
<li><p><strong>Potential Moderator variables:</strong> Need for Cognition (NFC) <span class="citation" data-cites="cacioppo1982">[@cacioppo1982]</span></p></li>
<li><p>Negative Attitudes to Robots Scale (NARS) <span class="citation" data-cites="nomura2006">[@nomura2006]</span></p></li>
</ul>
</section>
<section id="analysis" class="level3">
<h3 class="anchored" data-anchor-id="analysis">Analysis</h3>
<p>Linear mixed-effects models will be used to test the effects of robot behaviour and task type on outcomes. Moderation analysis will explore whether individual cognitive traits influence the impact of the EAI on trust and engagement.</p>
</section>
</section>
<section id="timeline" class="level2">
<h2 class="anchored" data-anchor-id="timeline">Timeline</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 40%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Phase</th>
<th>Activities</th>
<th>Timeline</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Setup &amp; Pilot</td>
<td>Task design, EAI development, ethics submission</td>
<td>May–June 2025</td>
</tr>
<tr class="even">
<td>Data Collection</td>
<td>Participant recruitment and testing (n ≈ 60)</td>
<td>July–Sept 2025</td>
</tr>
<tr class="odd">
<td>Analysis</td>
<td>Quantitative and qualitative analysis</td>
<td>Oct–Nov 2025</td>
</tr>
<tr class="even">
<td>Output</td>
<td>Final report, dissemination, manuscript prep</td>
<td>Dec 2025</td>
</tr>
</tbody>
</table>
</section>
<section id="expected-contributions" class="level2">
<h2 class="anchored" data-anchor-id="expected-contributions">Expected Contributions</h2>
<p>This study will provide empirical evidence on whether emotionally responsive robot behaviour improves user trust and collaboration, key barriers to automation adoption in industrial environments. Findings will support the design of AI-driven robotic systems that are not only functionally competent but also socially and behaviourally adaptive. These insights have direct relevance for the real-world deployment of collaborative robots in mining, particularly where human-robot teams work under high cognitive or emotional load.</p>
</section>
<section id="budget-note" class="level2">
<h2 class="anchored" data-anchor-id="budget-note">Budget Note</h2>
<p>The Misty II robot was purchased with grant funding from the IAMGOLD President Innovation Fund.</p>
<p>Modelling Human Mental States to Facilitate Trust in real-time, aut:onomous, Human–Robot Collaboration</p>
<p>Autonomous spoken-language interaction</p>
<p>“Spoken language interaction is for many the holy grail in HCI and HRI. It is built upon a collection of technologies, such as Automated Speech Recognition, Dialogue Management, and Text-to-Speech, that are chained together to create a system which allows the user to interact or converse with an artificial system using the most natural interface known to humankind”</p>
<p>Trust in robotic systems</p>
<p>Connecting humans and robots through physiological signals enables “closing-the-loop” in human-robot interaction (HRI) by allowing robots to infer a user’s emotional and cognitive state, such as stress or engagement, in real-time. This allows for adaptive and personalized interactions, where the robot can adjust its behavior to better suit the user’s current state, creating a more natural and responsive experience. The development of wearable sensors and modular software frameworks is making this more feasible for creating responsive system</p>
<p>How responsive robots influence user trust in autonomous-human-robot interaction is an open research question. Trust is a critical factor in HRI, as it influences user acceptance and reliance on robotic systems.</p>
<p>What we did:</p>
<p>We developed an autonomous spoken-language interaction system integrated with a prompted ASR pipeline and the Misty-II robot platform that can engage in natural conversations with users. The system is capable of recognizing speech, managing dialogue, and generating spoken responses.</p>
</section>
<section id="key-components-of-the-system" class="level2">
<h2 class="anchored" data-anchor-id="key-components-of-the-system">Key Components of the System:</h2>
<ol type="1">
<li><p>Misty-II Robot: A programmable robot platform equipped with sensors and actuators for interaction.</p></li>
<li><p>Automated Speech Recognition (ASR): A speech-to-speech pipeline that processes spoken input from users and converts it into text for LLM processing then back to speech for output on the robot.</p>
<ul>
<li>STT: Deepgram API for real-time speech-to-text conversion.</li>
<li>LLM: Gemini API for processing text input and generating contextually relevant responses in JSON format</li>
<li>TTS: Misty-II text-to-speech (TTS) engine on 820 processor.</li>
<li>DistilRoBERTa-base fine-tuned on emotion classification for emotion detection from user utterances</li>
</ul></li>
<li><p>Langchain Dialogue Management: A system that manages the flow of conversation, ensuring coherent and contextually appropriate dialogue within a two-part collaborative task.</p></li>
<li><p>Collaborative-Tasks</p>
<ul>
<li>Task 1: Whodunnit style task where human and robot collaborate to find a missing robot via the human asking Yes/No questions (process of elimination in 6x4 suspect grid) to the robot. Robot knows ground truth but can only answer Yes/No questions about suspect features. Can not directly describe the suspect or name them. (human can choose a random suspect to solve on their own but only 1 in 24 chance of being correct without robot help)</li>
<li>Task 2: Where is Atlas? Robot collaborates with human to find Atlas by deciphering cryptic system and sensor logs. Robot does not know the answer here and can only guide the human usinng its expertise and knowledge of computer systems and basic logical reasoning. (human can solve on their own but very difficult without robot help depending on participants technical background).</li>
</ul></li>
<li><p>Flask-gui dashboard interface: A web-based interface/dashboard that allows participants to interact with the tasks and view task-related information that are pushed to the robot.</p>
<ul>
<li>Task 1 dashboard: Displays the suspect grid and allows the user to select suspects and view their features.</li>
<li>Task 2 dashboard: Displays system logs and allows the user to input their findings.</li>
</ul></li>
<li><p>Pre and post tests:</p>
<ul>
<li>PRE-TEST: Need for Cognition Scale (short); Negative Attitudes to Robots Scale (NARS);</li>
<li>POST-TEST: Trust Perception Scale-HRI; 9 custom questions adapted from Charalambous et al.&nbsp;(2020) on trust in industrial human-robot collaboration;</li>
</ul></li>
</ol>
<p>Study Design:</p>
</section>
</section>
<section id="technical-specifications" class="level1">
<h1>Technical Specifications</h1>
<section id="system-overview" class="level2">
<h2 class="anchored" data-anchor-id="system-overview">System Overview</h2>
<p>This study implements a multi-stage collaborative task system where participants collaborate with the Misty II social robot to solve a who-dunniti type task. The system utilizes an autonomous, mixed-initiative dialogue architecture with affect-responsive capabilities.</p>
</section>
<section id="hardware-platform" class="level2">
<h2 class="anchored" data-anchor-id="hardware-platform">Hardware Platform</h2>
<p><strong>Robot</strong>: Misty II Social Robot (Misty Robotics)</p>
<ul>
<li>Mobile social robot platform with expressive display, arm actuators, and head movement</li>
<li>RGB LED for state indication</li>
<li>RTSP video streaming (1920×1080, 30fps) for audio capture</li>
<li>Custom action scripting for synchronized multimodal expressions</li>
</ul>
</section>
<section id="software-architecture" class="level2">
<h2 class="anchored" data-anchor-id="software-architecture">Software Architecture</h2>
<section id="core-system-components" class="level3">
<h3 class="anchored" data-anchor-id="core-system-components">Core System Components</h3>
<p><strong>Programming Language</strong>: Python 3.10</p>
<p><strong>Primary Dependencies</strong>:</p>
<ul>
<li><code>misty-sdk</code> (Python SDK for Misty Robotics API)</li>
<li><code>ffmpeg-python</code> (0.2.0) - Audio stream processing</li>
<li><code>flask</code> (3.1.2) + <code>flask-socketio</code> (5.5.1) - Web interface for task presentation</li>
<li><code>duckdb</code> (1.4.0) - Experimental data logging database</li>
</ul>
</section>
<section id="large-language-models" class="level3">
<h3 class="anchored" data-anchor-id="large-language-models">Large Language Models</h3>
<p><strong>Primary LLM Provider Options</strong>:</p>
<ol type="1">
<li><p><strong>Google Gemini</strong> (default):</p>
<ul>
<li>Model: <code>gemini-2.5-flash-lite</code> (configurable via environment variable)</li>
<li>Integration: <code>langchain-google-genai</code> with <code>google-generativeai</code> API</li>
<li>Response format: JSON-only output (<code>response_mime_type: "application/json"</code>). This format is required by Misty-II for reliable parsing and for action execution.</li>
</ul></li>
<li><p><strong>OpenAI GPT</strong> (alternative):</p>
<ul>
<li>Model: <code>gpt-4o-mini</code> (configurable)</li>
<li>Integration: <code>langchain-openai</code> (0.3.33)</li>
</ul></li>
</ol>
<p><strong>LLM Configuration</strong>:</p>
<ul>
<li>Temperature: 0.7 (default, configurable)</li>
<li>Memory: Conversation buffer memory with file-based persistence (<code>langchain.memory.ConversationBufferMemory</code>)</li>
<li>Context window: Full conversation history maintained across interaction stages</li>
</ul>
</section>
</section>
<section id="langchain-framework-integration" class="level2">
<h2 class="anchored" data-anchor-id="langchain-framework-integration">LangChain Framework Integration</h2>
<section id="core-langchain-components" class="level3">
<h3 class="anchored" data-anchor-id="core-langchain-components">Core LangChain Components</h3>
<p><strong>Framework Version</strong>: <code>langchain-core</code> with modular provider packages - <code>langchain</code> (meta-package) - <code>langchain-community</code> (0.3.31) - Community integrations - <code>langchain-google-genai</code> - Gemini integration - <code>langchain-openai</code> (0.3.33) - OpenAI integration</p>
</section>
<section id="conversationchain-architecture" class="level3">
<h3 class="anchored" data-anchor-id="conversationchain-architecture">ConversationChain Architecture</h3>
<p><strong>Memory Management</strong> (<code>ConversationChain</code> class in <code>conversation_chain.py</code>):</p>
<ol type="1">
<li><strong>Conversation Buffer Memory</strong>:
<ul>
<li>Implementation: <code>langchain.memory.ConversationBufferMemory</code></li>
<li>Storage: File-based persistent chat history (<code>FileChatMessageHistory</code>)</li>
<li>Format: JSON files in <code>.memory/</code> directory, one per participant session</li>
<li>Memory key: <code>"history"</code></li>
<li>Return format: Message objects (full conversation context)</li>
</ul></li>
<li><strong>Memory Reset Policy</strong>:
<ul>
<li>Default: Reset on each new session launch</li>
<li>Archive previous session: Timestamped archive files stored in <code>.memory/archive/</code></li>
<li>Configuration: <code>RESET_MEMORY</code> and <code>ARCHIVE_MEMORY</code> environment variables</li>
</ul></li>
</ol>
</section>
<section id="prompt-construction" class="level3">
<h3 class="anchored" data-anchor-id="prompt-construction">Prompt Construction</h3>
<p><strong>Message Structure</strong> (LangChain message types): <code>python   [SystemMessage, *history_messages, HumanMessage]</code> System Message Assembly:</p>
<ul>
<li>Core instructions (task framing, role definition)</li>
<li>Personality instructions (mode-specific behavior)</li>
<li>Stage-specific instructions (current task context)</li>
<li>Output format constraints (JSON schema specification)</li>
</ul>
<p><code>Human Message Format:   {     "user": "&lt;transcribed_speech&gt;",     "stage": "&lt;current_stage&gt;",     "detected_emotion": "&lt;emotion_label&gt;",     "frustration_note": "&lt;optional_alert&gt;",     "timer_expired": "&lt;task_id&gt;",     ...   }</code></p>
<ul>
<li>JSON-encoded context variables passed alongside user input</li>
<li>Enables LLM to access environmental state without breaking message history</li>
</ul>
</section>
<section id="llm-invocation" class="level3">
<h3 class="anchored" data-anchor-id="llm-invocation">LLM Invocation</h3>
<p>Provider-Specific Configuration:</p>
<ul>
<li><p>Gemini (ChatGoogleGenerativeAI): model_kwargs={“response_mime_type”: “application/json”}</p>
<ul>
<li>Enforces JSON-only output at API level</li>
<li>Reduces parsing errors from markdown-wrapped responses</li>
</ul></li>
</ul>
<p>Invocation Flow: messages → llm.invoke(messages) → response → memory.save_context()</p>
</section>
<section id="response-processing" class="level3">
<h3 class="anchored" data-anchor-id="response-processing">Response Processing</h3>
<p>JSON Extraction Pipeline: 1. Handle polymorphic response content (string or list) 2. Attempt direct json.loads() (fast path) 3. Fallback: Scan for first balanced {…} object in response 4. Extract required keys: msg, expression, advance_stage (optional) 5. Default fallback: {“msg”: content, “expression”: “thinking”} if parsing fails</p>
</section>
<section id="memory-persistence" class="level3">
<h3 class="anchored" data-anchor-id="memory-persistence">Memory Persistence:</h3>
<ul>
<li>Save after each turn: memory.save_context({“input”: user_text}, {“output”: llm_response})</li>
<li>Maintains conversational coherence across multi-stage interaction</li>
<li>Enables LLM to reference previous exchanges (e.g., “As I mentioned earlier…”)</li>
</ul>
</section>
<section id="prompt-debugging" class="level3">
<h3 class="anchored" data-anchor-id="prompt-debugging">Prompt Debugging</h3>
<p>Logging System (configurable via DEBUG_PROMPTS environment variable):</p>
<ul>
<li>Console output: Formatted display of system prompt, conversation history (truncated), and current human input</li>
<li>File logging: Full prompts saved to .memory/prompts/{session_id}-turn-{number}.txt</li>
<li>Sections: SYSTEM (assembled prompt), HISTORY (all previous messages), HUMAN (current input)</li>
<li>Enables post-hoc analysis of prompt engineering effectiveness</li>
</ul>
</section>
<section id="langchain-design-rationale" class="level3">
<h3 class="anchored" data-anchor-id="langchain-design-rationale">LangChain Design Rationale</h3>
<p>Why LangChain for this application:</p>
<ol type="1">
<li>Memory abstraction: Automatic conversation history management without manual message list handling</li>
<li>Provider flexibility: Easy switching between Gemini and OpenAI without rewriting prompt logic</li>
<li>Message typing: Structured SystemMessage/HumanMessage/AIMessage types maintain role clarity</li>
<li>File persistence: Built-in FileChatMessageHistory enables session recovery and archiving</li>
<li>Future extensibility: Framework supports adding tools, retrieval, or multi-agent patterns if needed</li>
</ol>
<p>Alternatives considered: Direct API calls would reduce dependencies but require reimplementing conversation history management, prompt templating, and cross-provider compatibility layers.</p>
</section>
<section id="langchain-limitations-in-this-context" class="level3">
<h3 class="anchored" data-anchor-id="langchain-limitations-in-this-context">LangChain Limitations in This Context</h3>
<ul>
<li>No chains used: Despite name ConversationChain, this is a direct LLM wrapper (no LangChain Expression Language chains)</li>
<li>No tools/agents: Simple request-response pattern (could extend for future tool-use capabilities)</li>
<li>Custom JSON parsing: LangChain’s built-in output parsers not used; custom extraction handles malformed responses more robustly</li>
</ul>
</section>
<section id="speech-processing" class="level3">
<h3 class="anchored" data-anchor-id="speech-processing">Speech Processing</h3>
<p><strong>Speech-to-Text (STT)</strong>:</p>
<ul>
<li>Provider: Deepgram Nova-2 (<code>deepgram-sdk</code> 4.8.1)</li>
<li>Model: <code>nova-2</code> with US English (<code>en-US</code>)</li>
<li>Smart formatting enabled</li>
<li>Interim results for real-time partial transcription</li>
<li>Voice Activity Detection (VAD) events</li>
<li>Adaptive endpointing: 200ms (conversational stages) / 500ms (log-reading task)</li>
<li>Utterance end timeout: 1000ms (conversational) / 2000ms (log-reading)</li>
<li>Audio processing: RTSP stream from Misty → FFmpeg MP3 encoding → Deepgram WebSocket</li>
</ul>
<p><strong>Text-to-Speech (TTS)</strong> - Three options:</p>
<ol type="1">
<li><p><strong>Misty Onboard TTS</strong> (this is the one we used): Native robot voice via onboard TTS</p></li>
<li><p><strong>OpenAI TTS</strong>:</p>
<ul>
<li>Model: <code>tts-1</code> (low-latency variant)</li>
<li>Voice: <code>sage</code></li>
<li>Format: MP3, served via HTTP (port 8000)</li>
<li>Ultimately chose not to use because we wanted a more robotic, non-human voice</li>
<li>Didn’t want the human voice influencing trust on its own (future research could look at trust in relation to type of voice)</li>
</ul></li>
<li><p><strong>Deepgram Aura</strong>:</p>
<ul>
<li>Model: <code>aura-stella-en</code> (conversational female voice)</li>
<li>Format: MP3, served via HTTP</li>
<li>Ultimately chose not to use because we wanted a more robotic, non-human voice</li>
</ul></li>
</ol>
</section>
<section id="emotion-detection" class="level3">
<h3 class="anchored" data-anchor-id="emotion-detection">Emotion Detection</h3>
<p><strong>Model</strong>: DistilRoBERTa-base fine-tuned on emotion classification</p>
<ul>
<li>HuggingFace identifier: <code>j-hartmann/emotion-english-distilroberta-base</code></li>
<li>Framework: <code>transformers</code> (4.57.1) pipeline</li>
<li>Hardware: CUDA GPU acceleration (automatic fallback to CPU)</li>
<li>Output classes: joy, anger, sadness, fear, disgust, surprise, neutral</li>
<li>Mapped to interaction states: positively engaged, irritated, disappointed, anxious, frustrated, curious, neutral</li>
</ul>
</section>
</section>
<section id="experimental-design-1" class="level2">
<h2 class="anchored" data-anchor-id="experimental-design-1">Experimental Design</h2>
<section id="study-conditions" class="level3">
<h3 class="anchored" data-anchor-id="study-conditions">Study Conditions</h3>
<p><strong>Between-subjects factor</strong>: Robot Interaction Mode</p>
<ol type="1">
<li><strong>RESPONSIVE</strong> (experimental): Warm, emotionally engaged, proactive behavior with emotion-responsive adaptation</li>
<li><strong>CONTROL</strong> (baseline): Neutral, reactive, information-only responses</li>
</ol>
</section>
<section id="task-structure" class="level3">
<h3 class="anchored" data-anchor-id="task-structure">Task Structure</h3>
<p><strong>Five sequential stages</strong>:</p>
<ol type="1">
<li><strong>Greeting</strong> (stage1): Participant introduction and rapport building</li>
<li><strong>Mission Brief</strong> (stage2): Task explanation and scenario framing</li>
<li><strong>Task 1</strong> (stage3): Who dunnit task
<ul>
<li>Identify suspect from 4×6 grid (24 options) by asking robot Yes/No questions based on ground-truth</li>
<li>Ground truth features: red hair, glasses, no hat, long hair, pink hoodie</li>
</ul></li>
<li><strong>Task 2</strong> (stage4): Log analysis task
<ul>
<li>determine missing robot location by collaborating with Misty-II to decipher system, wifi and sensor logs</li>
</ul></li>
<li><strong>Wrap-up</strong> (stage5): Debriefing and conclusion</li>
</ol>
<p><strong>Time constraints</strong>: ~15 minutes total session duration, with configurable per-task timers</p>
</section>
<section id="multimodal-robot-behavior" class="level3">
<h3 class="anchored" data-anchor-id="multimodal-robot-behavior">Multimodal Robot Behavior</h3>
<p><strong>Expression System</strong>: 25 custom action scripts combining:</p>
<ul>
<li>Facial displays (image eye-expression files on screen)</li>
<li>LED color patterns (solid, breathe, blink)</li>
<li>Arm movements (bilateral position control)</li>
<li>Head movements (pitch, yaw, roll control)</li>
</ul>
<p><strong>Nonverbal Backchannel Behaviors</strong> (RESPONSIVE mode only):</p>
<ul>
<li>Real-time listening cues triggered by partial transcripts (disfluencies, hesitation markers)</li>
<li>Emotion-matched expressions (e.g., “concern” for hesitation, “excited” for breakthroughs)</li>
</ul>
<p><strong>LED State Indicators</strong>:</p>
<ul>
<li>Blue (0, 199, 252): Actively listening (microphone open)</li>
<li>Purple (100, 70, 160): Processing/speaking (microphone closed)</li>
</ul>
</section>
</section>
<section id="data-collection" class="level2">
<h2 class="anchored" data-anchor-id="data-collection">Data Collection</h2>
<p><strong>Database</strong>: DuckDB relational database (<code>experiment_data.duckdb</code>)</p>
<p><strong>Logged Data</strong>:</p>
<p>1. <strong>Sessions table</strong>: participant ID (auto-incremented P01, P02…), condition assignment, timestamps, duration</p>
<p>2. <strong>Dialogue turns table</strong>: turn-by-turn user input, LLM response, expression, response latency (ms), behavioral flags</p>
<p>3. <strong>Task responses table</strong>: submitted answers with timestamps and time-on-task</p>
<p>4. <strong>Events table</strong>: stage transitions, silence check-ins, timer expirations, detected emotions</p>
<p><strong>Additional logs</strong>:</p>
<p>- Prompt debugging logs (full LLM prompts with history)</p>
<p>- Audio recordings (MP3, timestamped by utterance)</p>
<p>- Real-time console logs with structured event markers</p>
</section>
<section id="interaction-dynamics" class="level2">
<h2 class="anchored" data-anchor-id="interaction-dynamics">Interaction Dynamics</h2>
<section id="silence-handling" class="level3">
<h3 class="anchored" data-anchor-id="silence-handling">Silence Handling</h3>
<p><strong>Silence detection</strong>: 25-second threshold triggers check-in prompt</p>
<ul>
<li>RESPONSIVE: “Still working on it? No rush - I’m here if you need help!”</li>
<li>CONTROL: “I am ready when you have a question.”</li>
</ul>
</section>
<section id="emotion-responsive-behaviors-responsive-condition-only" class="level3">
<h3 class="anchored" data-anchor-id="emotion-responsive-behaviors-responsive-condition-only">Emotion-Responsive Behaviors (RESPONSIVE condition only)</h3>
<p><strong>Frustration tracking</strong>:</p>
<ul>
<li>Consecutive detection of frustrated/anxious/irritated/disappointed states</li>
<li>Threshold: ≥2 consecutive frustrated turns triggers proactive support</li>
<li>RESPONSIVE adaptation: “This part can be tough. Want me to walk you through it?”</li>
</ul>
<p><strong>Positive emotion matching</strong>:</p>
<ul>
<li>Celebratory language for curious/engaged states</li>
<li>Momentum maintenance: “Yes! Great observation!”</li>
</ul>
</section>
</section>
<section id="system-requirements" class="level2">
<h2 class="anchored" data-anchor-id="system-requirements">System Requirements</h2>
<p><strong>Hardware</strong>:</p>
<ul>
<li>CUDA-capable GPU (optional, for emotion detection acceleration)</li>
<li>Network: Local HTTP server (port 8000 for audio serving, port 5000 for Flask GUI)</li>
<li>Misty II robot on same local network</li>
</ul>
<p><strong>Software</strong>:</p>
<ul>
<li>Python 3.10</li>
<li>PyTorch 2.9.0 with CUDA 12.8 support</li>
<li>FFmpeg system installation</li>
</ul>
</section>
<section id="configuration" class="level2">
<h2 class="anchored" data-anchor-id="configuration">Configuration</h2>
<p><strong>Environment Variables</strong> (<code>.env</code> file):</p>
<ul>
<li><code>GPT_API_KEY</code>: OpenAI API key (required if using GPT or OpenAI TTS)</li>
<li><code>DEEPGRAM_API_KEY</code>: Deepgram API key (required)</li>
<li><code>GEMINI_API_KEY</code> / <code>GOOGLE_API_KEY</code>: Google Gemini API key (required if using Gemini)</li>
<li><code>LLM_PROVIDER</code>: “GEMINI” or “OPENAI” (default: GEMINI)</li>
<li><code>TTS_PROVIDER</code>: “misty”, “openai”, or “deepgram” (default: misty)</li>
<li><code>ENABLE_EMOTION_DETECTION</code>: true/false (default: true)</li>
<li><code>DEBUG_PROMPTS</code>: 0/1 - Enable full prompt logging</li>
<li><code>LLM_TEMPERATURE</code>: 0.0-1.0 (default: 0.7)</li>
<li><code>WRAP_TIMEOUT_SECONDS</code>: Auto-exit timeout (default: 60)</li>
</ul>
<p><strong>Run Mode</strong>: Set programmatically in <code>mistyGPT_emotion.py</code> line 126:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>RUN_MODE <span class="op">=</span> <span class="st">"RESPONSIVE"</span>  <span class="co"># or "CONTROL"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="prompt-engineering" class="level2">
<h2 class="anchored" data-anchor-id="prompt-engineering">Prompt Engineering</h2>
<p>Modular prompt system (PromptLoader class):</p>
<ul>
<li>core_system.md: Task framing, role description, output format schema</li>
<li>role_responsive.md / role_control.md: Condition-specific personality instructions</li>
<li>stage1_greeting.md through stage5_wrap_up.md: Stage-specific task instructions</li>
</ul>
<p>Context injection: Real-time contextual variables passed to LLM:</p>
<ul>
<li>Current stage</li>
<li>Detected emotion (if enabled)</li>
<li>Task submission status</li>
<li>Timer expiration notifications</li>
<li>Silence check-in flags</li>
</ul>
</section>
<section id="inter-process-communication" class="level2">
<h2 class="anchored" data-anchor-id="inter-process-communication">Inter-process Communication</h2>
<p>Flask REST API endpoints:</p>
<ul>
<li>GET /stage_current: Synchronize stage state with facilitator GUI</li>
<li>GET /task_submission_status: Detect participant task submissions</li>
<li>GET /timer_expired_status: Detect timer expirations</li>
<li>POST /stage: Update stage (facilitator override)</li>
<li>POST /reset_timer: Clear timer expiration flags</li>
</ul>
<p>Reliability Features</p>
<ul>
<li>Graceful shutdown handlers (SIGTERM, SIGINT)</li>
<li>Automatic session logging even on interruption</li>
<li>Fallback behaviors: CPU-only emotion detection, Misty TTS if cloud services fail</li>
<li>Deepgram speech_final detection with UtteranceEnd backup handler</li>
</ul>
<hr>
<p>Note: All data collection procedures and experimental protocols were approved by …. .</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>