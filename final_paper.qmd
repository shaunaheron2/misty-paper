---
title: "Responsive Robots"
subtitle: "Preliminary Analysis of Trust and Performance Data"
author: "Shauna Heron"
date: today
format: 
    typst:
        fontsize: 10pt
warning: false
message: false
echo: false
bibliography: references.bib
---

```{r}

library(tidyverse)
library(janitor)
library(scales)
library(corrplot)
library(sjPlot)
library(ggstatsplot)
library(psych)
library(performance)
library(gt)
library(Hmisc)
library(haven)
library(gtsummary)

session_df <- readRDS("full_session_data.rds")
survey_df <- readRDS("survey_data.rds") |>
  arrange(post_date)

set_gtsummary_theme(theme_gtsummary_journal("nejm"))
set_gtsummary_theme(theme_gtsummary_compact())

df_flat_scores_final <- readRDS("full_dataset_with_items.rds") #|>
# filter(!grepl('sprig', email))

df_long_scores_final <- readRDS("full_dataset_long_trust_post.csv")
#saveRDS(df_flat_scores_final_with_items, 'full_dataset_with_items.rds')

```

::: callout-warning
These are preliminary results and analyses. Please do not distribute or cite without permission of the authors.
:::

This study implements a multi-stage collaborative task system where participants collaborate with the Misty II social robot to solve a who-dunnit type task. The system utilizes an autonomous, mixed-initiative dialogue architecture with affect-responsive capabilities.

# Introduction

Trust is a central construct in human–robot interaction (HRI), shaping how people collaborate with, rely on, and accept robotic systems across social, assistive, and task-oriented domains . In collaborative settings, trust influences not only subjective evaluations of the robot but also objective outcomes such as task performance, compliance, and engagement. As a result, a growing body of work has focused on measuring trust following human–robot interaction, including the development of standardized instruments designed to capture users’ perceptions of robot reliability, predictability, and intent.

Despite this growing literature, much of what is currently known about trust in HRI has been derived from interactions conducted under highly controlled or idealized conditions. In many studies, robot behavior is scripted, simulated, or mediated through human control using Wizard-of-Oz (WoZ) paradigms [@maure; @lin2022a]. While such approaches are valuable for early-stage design and hypothesis generation, these approaches alter interaction dynamics by masking sensing failures, response latency, and behavioral inconsistencies that are characteristic of autonomous robotic systems. This gap is especially notable given that autonomy-related challenges—such as speech recognition errors, model hallucinations, delayed responses, and misinterpretations of user intent—are likely to play a critical role in shaping trust during real deployments. From an HRI perspective, understanding trust in the presence of real-world imperfections may be more informative than evaluations conducted under idealized assumptions. Nevertheless, few studies have directly examined trust outcomes following fully autonomous, in-person human–robot interaction.

The present study addresses this gap by evaluating trust following an in-person interaction with a Misty-II robot operating autonomously within predefined behavioral constraints. To this end, participants engaged in solving a mystery 'who-dunnit' style problem with the robot: who took the lab robot 'Atlas' and where is it now? Together the robot and the participant moved through a series of collaborative tasks, the robot managing speech-based interaction, task progression, and affect-responsive behavior, all without human intervention. To this end, two experimental conditions were compared: a control condition in which the robot followed a neutral, non-proactive interaction policy, and a responsive condition in which the robot was prompted to adapt its behavior based on dialogue, detected user affect and the task itself. Importantly, both conditions were subject to the same sensory and interaction limitations inherent to autonomous operation, including speech recognition variability and response timing constraints.

To this end, we developed an autonomous spoken-language interaction system integrated with a prompted ASR pipeline and the Misty-II robot platform that can engage in natural conversations with users. The system is capable of recognizing speech, managing dialogue, and generating spoken responses as well as physical expression and movement of the robot during dialogue. By examining post-interaction trust using established trust measures alongside behavioral and task-level outcomes, this study aims to contribute empirical evidence on how trust might be shaped in fully autonomous HRI scenarios. Rather than seeking to demonstrate optimal performance under ideal conditions, the focus is on understanding trust as it is impacted during realistic human–robot interaction, where uncertainty, interactional breakdowns, and adaptive behavior are unavoidable. As such, this work provides insight into the practical implications of affect-responsive autonomy for trust in human–robot collaboration.

## Task Design and Collaborative Structure

Participants interacted with the robot in solving an immersive puzzle game where the robot served as a diegetic "game guide" and collaborative partner. In the game, participants solve a crime mystery by asking the game guide for information to complete tasks and for hints and advice on how to solve puzzles. The game was composed of two sequential tasks designed to elicit interaction with the robot under differing knowledge and dependency conditions @lin2022a . When @lin2022 et al., utilized a similar task they found that participants who engaged with a robot compared to a human guide had more fun, felt less judged and more connected with the robot while solving tasks compared to a human–though respondents mentioned that it would be helpful if the robot was more proactive in the help it provided. Though they utilized a Woz system, in our case, both tasks were completed autonomously in the presence of a shared task interface that displayed instructions, task materials, and participant inputs. The robot autonomously monitored task progression through the interface and adapted its dialogue accordingly, while all behavioral responses were generated without real-time human intervention.

### Task 1: Robot-dependent collaborative reasoning

The first task required participants to identify a suspect from a 6x4 grid by asking a series of yes/no questions about their features. A grid of potential suspects was displayed on the interface, and participants formulated questions verbally to narrow down the correct individual. In this task, the robot possessed the information necessary to determine whether each question was true or false, making successful task completion dependent on interaction with the robot.

This task was designed to establish an initial forced collaborative dynamic in which the robot served as an essential informational partner. Participants were required to engage verbally with the robot, interpret its responses, and coordinate question strategies to reach a solution within the allotted time (5 minutes). The structured nature of the task ensured that the robot’s role was clear and that collaboration was unavoidable.

### Task 2: Open-ended problem-solving with advisory robot support

The second task involved a more complex problem-solving scenario in which participants examined multiple technical logs presented via the interface to determine the location of the missing 'Atlas' robot. Participants had 3 questions to answer via multiple-choice dropdowns: i) is Atlas still functioning? (yes/no); ii) what building is Atlas is; iii) what floor is Atlas on; iv) what room is Atlas in. Unlike the first task, the robot did not possess ground-truth knowledge about the whereabouts of the robot. The robot’s assistance in this task was limited to general problem-solving support derived from the Gemini language model’s prior training, such as explaining how to interpret log information, suggesting reasoning strategies, or helping participants reflect on inconsistencies across logs. The robot was explicitly constrained such that it was informed only that participants could view several logs, without access to the content of those logs or the correct answers to task-related questions and that its job was to determine Atlas' whereabouts together. The robot could ask the participant questions and vice versa. Importantly, participants could complete the task independently or choose to solicit assistance from the robot.

As a result, the robot functioned as a collaborative reasoning partner rather than an authoritative source. Participants retained full control over decision-making and were free to accept, reject, or ignore the robot’s suggestions. This design allowed collaboration to emerge voluntarily, rather than being enforced by task structure.

Once all answers were submitted, the correct answers were shown to participants, letting them know how they did. At the stage the robot and the participant could briefly debrief on whether they were right or not, and then the task came to the end with the robot thanking them and

### Task difficulty and collaborative intent

The second task was intentionally designed to be sufficiently challenging that completing it within the allotted time was difficult without assistance. This ensured that interaction with the robot represented a meaningful opportunity for collaboration rather than a trivial or purely optional exchange. By contrasting a robot-dependent task with an open-ended advisory task, the study examined trust formation across interaction contexts that varied in both informational asymmetry and reliance on the robot.

Across both tasks, the interface served as a shared workspace facilitating coordination between the participant and the autonomous robot, rather than as a mechanism for remotely controlling robot behavior. At no point during either task did a human operator intervene to guide the robot’s actions or manage task flow.

## System overview and experimental setup

Participants interacted with the Misty-II robot in a shared physical workspace that included both the robot and a computer-based task interface. The interface was visible to participants and used to present brief task instructions, collect responses, and advance between task stages. Importantly, the robot autonomously monitored task progression and participant input through the interface, allowing it to adapt its dialogue and responses without human intervention.The interface served as a communication channel between the participant and the autonomous system rather than as a mechanism for remotely controlling robot behavior (See @fig-setup) .

### Experimental setup and interaction environment

![Experimental setup showing the autonomous robot and participant-facing task interface used during in-person sessions. Participants entered task responses and navigated between task stages using the interface, while the robot autonomously tracked task state and adapted its interaction based on participant input. No real-time human intervention occurred during the interaction.](images/misty-pullback.jpg){#fig-setup fig-align="center" width="482"}

### Robotic system and autonomy pipeline

The robot operated fully autonomously throughout each session, managing speech recognition, dialogue generation, affect detection, and behavioral responses in real time. No human operator intervened during interactions. All behaviors were constrained to a predefined action space designed to ensure safety and task consistency across participants.

The task interface was adapted from prior work with the same robotic platform in which a graphical interface was used to support Wizard-of-Oz control. In the present study, this interface was deliberately repurposed as a shared workspace for human–robot collaboration rather than as a mechanism for remotely controlling robot behavior. Rather than serving as a control surface, the interface functioned as a shared task environment through which both the participant and the robot maintained awareness of task state and progress. Participant inputs were visible to the robot, allowing it to track task transitions and respond contextually, while all behavioral decisions were generated autonomously by the robot.

In the first task, participants worked with the robot to identify a suspect by asking a series of yes/no questions. The robot possessed the ground-truth information necessary to resolve the task but could not explicitly identify the individual directly, making successful completion dependent on effective interaction with the robot.

The second task involved a more complex problem-solving scenario in which participants examined multiple WiFi and other technical logs to determine the location of a missing robot. Unlike the first task, the robot did not possess the correct solution. Participants could choose to work independently or solicit assistance from the robot, which provided guidance, clarification, and affective support but no definitive answers. The second task was intentionally designed to be sufficiently challenging that completing it within the allotted time was difficult without assistance, thereby creating a meaningful opportunity for collaboration rather than a trivial interaction. The robot’s assistance was framed as collaborative support rather than authoritative guidance, and participants were not led to believe that the robot possessed complete or privileged knowledge during the second task.

During the second task, the robot did not possess task-specific knowledge or access to the correct solution. Instead, it provided assistance by helping participants interpret the structure and purpose of the available technical logs, drawing solely on general knowledge and reasoning capabilities acquired during model training. The robot’s responses were conditioned on the interaction context and participant queries, but it did not have access to the log contents beyond what participants explicitly referenced.The robot’s dialogue system was explicitly constrained such that it was informed only that participants could view multiple technical logs, without access to the content of those logs or the correct solution to the task.

As a result, the robot’s role in the second task was that of a collaborative reasoning partner rather than an authoritative source. Participants could choose whether to engage with the robot’s suggestions or pursue independent reasoning strategies.The robot’s responses were framed to emphasize collaborative rather than definitive instruction, reducing the risk of misleading participants when uncertainty was present.

### Interaction conditions

Describe the responsive versus control conditions here briefly. Maybe give explanation of how that was handled with langchain?

# Results

## Participant characteristics and baseline measures

Participants in the control and responsive conditions were comparable with respect to demographic characteristics, academic background, prior experience with robots, and baseline attitudes toward robots. Importantly, Negative Attitudes Towards Robots (NARS) and Need for Cognition scores were similar across groups, indicating that post-interaction differences are unlikely to reflect pre-existing attitudes (see @tbl-pre).

```{r}
#| label: tbl-pre

df_flat_scores_final |>
  mutate(age = as_factor(age)) |>
  data.frame() |>
  select(-post_date) |>
  droplevels() |>
  select(
    group,
    gender,
    age,
    program,
    robot_xp,
    native_english,
    nars_pre,
    #nars_social_influence_robots,
    #nars_emotion_robots,
    #nars_interaction_robots,
    nfc_pre
    #robot_trust_pre,
    #ai_trust_pre,
    #human_trust_pre
    #contains('trust')
    #-mytrust_pre
  ) |>
  tbl_summary(
    by = group,
    type = list(
      nars_pre ~ 'continuous',
      # nars_social_influence_robots ~ 'continuous',
      # nars_emotion_robots ~ 'continuous',
      # nars_interaction_robots ~ 'continuous',
      #human_trust_pre ~ 'continuous',
      #robot_trust_pre ~ 'continuous',

      #ai_trust_pre ~ 'continuous',
      nfc_pre ~ 'continuous'
    ),
    statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} / {N} ({p}%)"
    ),
    label = list(
      gender ~ "Gender",
      age ~ "Age Group",
      program ~ "Program",
      robot_xp ~ "Experience w/Robots",
      native_english ~ 'Native English Speaker',
      nfc_pre ~ 'Need for Cognition',
      nars_pre ~ "NARS Overall"
      # nars_social_influence_robots ~ "NARS: Social Influence",
      #nars_emotion_robots ~ "NARS: Emotions",
      # nars_interaction_robots ~ "NARS: Interaction"
    ),
    missing_text = ('Missing')
  ) |>
  add_n() |>
  add_p() |>
  #add_overall() |>
  bold_p() |>
  bold_labels() |>
  modify_caption(
    "Participant Demographics and Baseline Characteristics by Group"
  )
```

```{r}
#| eval: false
ggplot(
  df_long_scores_final |>
    filter(scale != 'HRI_perception_post'),
  aes(x = robot_trust_post, fill = group)
) +
  geom_density(alpha = 0.5)

ggplot(
  df_long_scores_final |>
    filter(scale == 'HRI_perception_post'),
  aes(x = robot_trust_post, fill = group)
) +
  geom_density(alpha = 0.5)
```

```{r}
#| echo: false

```

```{r}
#| echo: false

# ggbetweenstats(
#   x = group,
#   y = robot_trust_post,
#   #  grouping.var = trust_items,
#   data = df_long_scores_final |>
#     filter(scale != 'HRI_perception_post')
# )

# ggbetweenstats(
#   x = group,
#   y = robot_trust_post,
#   # grouping.var = trust_items,
#   data = df_long_scores_final |>
#     filter(scale == 'HRI_perception_post')
# )
```

## Post-Interaction Trust Differences

Descriptive comparisons of participant-level post-test scores indicated an approximately 10 point difference in post-test Trust Perception Scale-HRI scores (M ≈ 73 vs M ≈ 63) and a 14 point difference in the Trust in Industrial Human-robot Collaboration scale (M ≈ 47 vs M ≈ 61) between conditions, although these differences did not reach conventional significance under a two-sample t-test (p = 0.09 and p = 0.14 respectively), reflecting limited power (n = 29). Although the second trust instrument was administered and scored in its original 5-point Likert format, for ease of interpretation it was linearly rescaled to the first scale's 0–100 metric for interpretability and comparability.

```{r}

ggbetweenstats(
  x = group,
  y = post_trust,
  data = df_flat_scores_final,
  type = "np"
)

ggbetweenstats(
  x = group,
  y = trust_perc_post,
  data = df_flat_scores_final,
  type = "np"
)
```

Hierarchical Bayesian models were next employed to account for item-level structure yielding smaller effect sizes but consistent estimates (≈7–8 points) and a high posterior probability of a positive effect. Notably, treating item-level responses as independent observations (i.e., ignoring non-independance) substantially inflated apparent precision, underscoring that the observed uncertainty is primarily a function of sample size rather than absence of an effect. A larger sample would allow more precise estimation of the effect magnitude.

To explore further, Bayesian hierarchical models indicated higher post-interaction trust scores in the responsive robot condition across both trust-related scales (posterior mean differences ≈ 7–8 points on a 0–100 scale). Although 95% credible intervals overlapped zero, the posterior probability that the responsive condition increased trust was high for both measures (≈0.95), suggesting a robust directional effect alongside substantial individual variability.Sensitivity analyses using substantially wider priors yielded nearly identical posterior estimates for the group effect, indicating that results were not driven by prior specification. In addition to directional effects, the posterior probability that the responsive condition increased trust by at least five points was approximately 0.70, suggesting a moderate likelihood of a practically meaningful effect despite substantial individual variability.

## Trust subscale patterns

Examination of trust subscales suggested that group differences were most pronounced for affective components of trust (e.g., trust feelings), whereas differences in perceived reliability were smaller. This pattern aligns with correlations showing that baseline negative attitudes toward robots were more strongly associated with affective trust than with reliability judgments.

## Interaction dynamics and task performance

```{r}
post_table <- df_flat_scores_final |>
  #drop_na(gender) |>
  select(group, contains('post'), contains('correct'), 63:76) |>
  data.frame() |>
  mutate(across(
    contains('post_trust'),
    ~ scales::rescale(.x, to = c(0, 100), min_old = 1, max_old = 5)
  )) |>
  select(-post_date)

post_table |>
  tbl_summary(
    by = group,
    type = list(
      post_trust_reliability ~ 'continuous',
      post_trust_feelings ~ 'continuous',
      post_trust_perception ~ 'continuous',
      total_correct ~ 'continuous',
      avg_response_ms ~ 'continuous',
      prop_correct ~ 'continuous',
      n_engaged ~ 'continuous',
      n_frust ~ 'continuous',
      n_neg ~ 'continuous'
    ),
    # missing=FALSE,
    statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} / {N} ({p}%)"
    ),
    label = list(
      suspect_correct ~ "Suspect ID Accuracy",
      status_correct ~ "Status Accuracy",
      duration_mins ~ "Avg Task Duration (mins)",

      total_correct ~ "Total Task Accuracy",
      n_turns ~ "Dialogue Turns",
      n_silence ~ "Silent Periods",
      avg_response_ms ~ "Avg Response Time (ms)",
      prop_correct ~ "Overall Task Accuracy",
      n_engaged ~ "Engaged Responses",
      n_frust ~ "Frustrated Responses",
      trust_perc_post ~ "Post-Task Trust Perception"
    )
  ) |>
  add_n() |>
  add_p() |>
  # add_overall() |>
  bold_p() |>
  #bold_labels() |>
  modify_caption("Table 2. Post-Interaction Raw Outcome Measures by Group")
```

### Task performance

Objective task accuracy did not differ between conditions across any task-level measures except suspect accuracy (robot dependendant task), indicating that increased trust was only attributable to improved task success when interaction was necessary to complete accurately.

Despite similar task accuracy, interactions in the responsive condition were characterized by longer durations, slower response times, and a higher number of AI-detected engaged responses. These findings suggest that responsiveness altered the interaction dynamics and affective tone rather than task outcomes.

## Individual differences and correlational patterns

As expected, we find that higher Need for Cognition (NFC) scores are negatively associated with Negative Attitudes Towards Robots (NARS), indicating that individuals who enjoy effortful thinking tend to have more positive attitudes towards robots. This relationship is consistent with prior literature suggesting that cognitive engagement is associated with openness to new technologies. In terms of NARS subscales, NFC was negatively correlated with all three subscales, but significantly so only in the domain of Situations of Interaction with Robots. This suggests that individuals with higher NFC are less likely to hold negative attitudes across various dimensions of robot interaction but especially around direct interaction with robots.

--\> how to talk about post-interaction correlations w/pre-interaction measures Several behavioral and task-level measures were correlated with post-interaction trust, consistent with the interpretation that trust judgments were shaped by interaction quality; these variables were not included as covariates in primary models to avoid conditioning on potential mediators.

Baseline negative attitudes toward robots were negatively correlated with post-interaction trust, with the strongest associations observed for affective trust subscales. In contrast, objective task performance was selectively associated with perceived reliability. Need for cognition was negatively correlated with negative robot attitudes and interaction-level negative affect, suggesting that individual differences contributed to variability in trust responses.

```{r}
#| label: fig-corr
#| fig-height: 7
#| fig-width: 7

df_flat <- df_flat_scores_final |>
  select(
    contains('pre'),
    contains('nars'),
    -human_trust_pre,
    -robot_trust_pre,
    -ai_trust_pre
    # contains('post'),
    # total_correct,
    # n_turns,
    # duration_mins,
    # avg_response_ms,
    # n_silence,
    # n_engaged,
    # n_frust,
    # n_neg,
    # prop_correct,
    # suspect_correct,
    # building_correct,
    # zone_correct,
    # floor_correct
  ) |>
  select(-contains('i_would'))

# df_flat %>%
#   group_by(group) %>%
#   dplyr::summarise(
#     correlation = cor(human_trust, robot_trust, method = "pearson", use = "pairwise.complete.obs"),
#     .groups = "drop"
#   )

res2 <- rcorr(as.matrix(df_flat %>% select(where(is.numeric))))

corr <- cor(
  df_flat %>% select(where(is.numeric)),
  use = "pairwise.complete.obs"
)

testRes = cor.mtest(corr, conf.level = 0.95)
#corrplot(corr, p.mat = testRes$p,  cl.pos = 'n')

tab_corr(
  df_flat %>% select(where(is.numeric)),
  corr.method = "spearman",
  p.numeric = FALSE,
  triangle = 'lower'
)

# corrplot(
#   corr,
#   p.mat = testRes$p,
#   insig = 'label_sig',
#   sig.level = c(0.001, 0.01, 0.05),
#   pch.cex = 0.9,
#   pch.col = 'grey20'
# )

# https://sjdm.org/dmidi/Need_for_Cognition_short.html
# Lins de Holanda Coelho G, H P Hanel P, J Wolf L. The Very Efficient Assessment of Need for
#Cognition: Developing a Six-Item Version. Assessment. 2020 Dec;27(8):1870-1885. doi:
#10.1177/1073191118793208. Epub 2018 Aug 10. PMID: 30095000; PMCID: PMC7545655
# https://rins.st.ryukoku.ac.jp/~nomura/docs/NARS_AAAI06.pdf
# items. hus, the minimum and maximum scores are 6 and
#  https://uhra.herts.ac.uk/id/eprint/13608/1/SyrdalDDautenhahn.pdf <-- discusses problems using standardized scale cross-culturally
```

### Model robustness and predictive checks

Sensitivity analyses using alternative prior specifications yielded substantively similar estimates, and leave-one-out cross-validation indicated comparable predictive performance between models with and without the group effect.

```{r}

```

# Discussion

Descriptive comparisons of post-interaction measures indicated that participants in the responsive condition reported consistently higher trust across all trust measures, with differences ranging from approximately 8 to 16 points on a 0–100 scale, although uncertainty remained high given the small sample. Notably, the responsive condition did not differ from control in objective task accuracy, suggesting that increased trust was not driven by improved task success. Instead, responsive interactions were characterized by longer durations, slower response times, and a higher number of AI-detected engaged responses, indicating a shift in interaction dynamics rather than performance.

Baseline negative attitudes toward robots were most strongly associated with affective components of trust rather than perceptions of reliability, suggesting that pre-existing attitudes primarily shape emotional responses to interaction rather than judgments of system competence. Conversely, objective task performance was selectively associated with perceived reliability, indicating that participants distinguished between affective and functional aspects of trust.

Future work with larger samples could formally test mediation pathways linking robot responsiveness, interaction fluency, affective responses, and trust judgments, as well as moderation by baseline attitudes toward robots and need for cognition.

Participants in the responsive condition also exhibited higher levels of AI-detected engagement during interaction, as indexed by a greater number of responses classified as positive affect (t-test result). This suggests that responsive behaviors altered the affective tone of the interaction itself.